<!-- Generated by pkgdown: do not edit by hand -->
<!DOCTYPE html>
<html lang="en">
  <head>
  <meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<title>Fit and validate Generalized Additive Models based on Ensemble of Small of Model approach — esm_max • flexsdm</title>


<!-- jquery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>
<!-- Bootstrap -->
<link href="https://cdnjs.cloudflare.com/ajax/libs/bootswatch/3.4.0/united/bootstrap.min.css" rel="stylesheet" crossorigin="anonymous" />


<script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/js/bootstrap.min.js" integrity="sha256-nuL8/2cJ5NDSSwnKD8VqreErSWHtnEP9E7AySL+1ev4=" crossorigin="anonymous"></script>

<!-- bootstrap-toc -->
<link rel="stylesheet" href="../bootstrap-toc.css">
<script src="../bootstrap-toc.js"></script>

<!-- Font Awesome icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous" />

<!-- clipboard.js -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script>

<!-- headroom.js -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script>

<!-- pkgdown -->
<link href="../pkgdown.css" rel="stylesheet">
<script src="../pkgdown.js"></script>




<meta property="og:title" content="Fit and validate Generalized Additive Models based on Ensemble of Small of Model approach — esm_max" />
<meta property="og:description" content="This function constructs Generalized Additive Models using the
Ensemble of Small Model (ESM) approach (Breiner et al., 2015, 2018)." />




<!-- mathjax -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script>

<!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->



  </head>

  <body data-spy="scroll" data-target="#toc">
    <div class="container template-reference-topic">
      <header>
      <div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="../index.html">flexsdm</a>
        <span class="version label label-default" data-toggle="tooltip" data-placement="bottom" title="Released version">0.0.1</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="../reference/index.html">Functions</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Articles
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="../articles/01_pre_modeling.html">Overview of Pre-modeling functions</a>
    </li>
    <li>
      <a href="../articles/04_Red_fir_example.html">An example with Red Fir</a>
    </li>
  </ul>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
      
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

      

      </header>

<div class="row">
  <div class="col-md-9 contents">
    <div class="page-header">
    <h1>Fit and validate Generalized Additive Models based on Ensemble of Small of Model approach</h1>
    
    <div class="hidden name"><code>esm_max.Rd</code></div>
    </div>

    <div class="ref-description">
    <p>This function constructs Generalized Additive Models using the
Ensemble of Small Model (ESM) approach (Breiner et al., 2015, 2018).</p>
    </div>

    <pre class="usage"><span class='fu'>esm_max</span><span class='op'>(</span>
  <span class='va'>data</span>,
  <span class='va'>response</span>,
  <span class='va'>predictors</span>,
  <span class='va'>partition</span>,
  thr <span class='op'>=</span> <span class='cn'>NULL</span>,
  background <span class='op'>=</span> <span class='cn'>NULL</span>,
  clamp <span class='op'>=</span> <span class='cn'>TRUE</span>,
  classes <span class='op'>=</span> <span class='st'>"default"</span>,
  pred_type <span class='op'>=</span> <span class='st'>"cloglog"</span>,
  regmult <span class='op'>=</span> <span class='fl'>2.5</span>
<span class='op'>)</span></pre>

    <h2 class="hasAnchor" id="arguments"><a class="anchor" href="#arguments"></a>Arguments</h2>
    <table class="ref-arguments">
    <colgroup><col class="name" /><col class="desc" /></colgroup>
    <tr>
      <th>data</th>
      <td><p>data.frame. Database with the response (0,1) and predictors values.</p></td>
    </tr>
    <tr>
      <th>response</th>
      <td><p>character. Column name with species absence-presence data (0,1)</p></td>
    </tr>
    <tr>
      <th>predictors</th>
      <td><p>character. Vector with the column names of quantitative
predictor variables (i.e. continuous variables). This function does not allow categorical variables
Usage predictors = c("aet", "cwd", "tmin"). This function only can construct models with
continuous variables.</p></td>
    </tr>
    <tr>
      <th>partition</th>
      <td><p>character. Column name with training and validation partition groups.</p></td>
    </tr>
    <tr>
      <th>thr</th>
      <td><p>character. Threshold used to get binary suitability values (i.e. 0,1). It is useful for threshold-dependent performance metrics. It is possible to use more than one threshold type. It is necessary to provide a vector for this argument. The next threshold area available:</p><ul>
<li><p>equal_sens_spec: Threshold at which the sensitivity and specificity are equal.</p></li>
<li><p>max_sens_spec: Threshold at which the sum of the sensitivity and specificity is the highest (aka threshold that maximizes the TSS).</p></li>
<li><p>max_jaccard: The threshold at which Jaccard is the highest.</p></li>
<li><p>max_sorensen: The threshold at which Sorensen is highest.</p></li>
<li><p>max_fpb: The threshold at which FPB is highest.</p></li>
<li><p>sensitivity: Threshold based on a specified sensitivity value.
  Usage thr = c('sensitivity', sens='0.6') or thr = c('sensitivity'). 'sens' refers to sensitivity value. If it is not specified a sensitivity values, the function will use by default 0.9</p></li>
</ul><p>If the user wants to include more than one threshold type, it is necessary concatenate threshold types, e.g., thr=c('max_sens_spec', 'max_jaccard'), or thr=c('max_sens_spec', 'sensitivity', sens='0.8'), or thr=c('max_sens_spec', 'sensitivity'). Function will use all thresholds if no threshold is specified</p></td>
    </tr>
    <tr>
      <th>background</th>
      <td><p>data.frame. Database with response column only with 0 and predictors variables. All
column names must be consistent with data. Default NULL</p></td>
    </tr>
    <tr>
      <th>clamp</th>
      <td><p>logical. It is set with TRUE, predictors and features are restricted to the range seen during model training.</p></td>
    </tr>
    <tr>
      <th>classes</th>
      <td><p>character. A single feature of any combinations of them. Features are symbolized by letters: l (linear), q (quadratic), h (hinge), p (product), and t (threshold). Usage classes = "lpq". Default "default" (see details).</p></td>
    </tr>
    <tr>
      <th>pred_type</th>
      <td><p>character. Type of response required available "link", "exponential", "cloglog" and "logistic". Default "cloglog"</p></td>
    </tr>
    <tr>
      <th>regmult</th>
      <td><p>numeric. A constant to adjust regularization. Because ESM are used
for modeling species with few records default value is 2.5</p></td>
    </tr>
    </table>

    <h2 class="hasAnchor" id="value"><a class="anchor" href="#value"></a>Value</h2>

    <p>A list object with:</p><ul>
<li><p>esm_model: A list with "Gam" class object for each bivariate model. This object can be used
for predicting ensemble of small model with <code><a href='sdm_predict.html'>sdm_predict</a></code> function.</p></li>
<li><p>predictors: A tibble with variables use for modeling.</p></li>
<li><p>performance: Performance metric (see <code><a href='sdm_eval.html'>sdm_eval</a></code>).
Those threshold dependent metric are calculated based on the threshold specified in thr argument.</p></li>
</ul>

    <h2 class="hasAnchor" id="details"><a class="anchor" href="#details"></a>Details</h2>

    <p>This method consists of creating bivariate models with all the pair-wise combinations
of predictors and perform an ensemble based on the average of suitability weighted by
Somers'D metric (D = 2 x (AUC -0.5)). ESM is recommended for modeling species with few occurrences.
This function does not allow categorical variables because the use of these types of variables
could be problematic when using with few occurrences. Further detail see
Breiner et al. (2015, 2018). This function use a default regularization multiplier
equal to 2.5 (see details in Breinter et al., 2018)</p>
    <h2 class="hasAnchor" id="references"><a class="anchor" href="#references"></a>References</h2>

    
<ul>
<li><p>Breiner, F. T., Guisan, A., Bergamini, A., &amp; Nobis, M. P. (2015). Overcoming limitations of modelling rare species by using ensembles of small models. Methods in Ecology and Evolution, 6(10), 1210-218. https://doi.org/10.1111/2041-210X.12403</p></li>
<li><p>Breiner, F. T., Nobis, M. P., Bergamini, A., &amp; Guisan, A. (2018). Optimizing ensembles of small models for predicting the distribution of species with few occurrences. Methods in Ecology and Evolution, 9(4), 802-808. https://doi.org/10.1111/2041-210X.12957</p></li>
</ul>

    <h2 class="hasAnchor" id="see-also"><a class="anchor" href="#see-also"></a>See also</h2>

    <div class='dont-index'><p><code><a href='esm_gam.html'>esm_gam</a></code>, <code><a href='esm_gau.html'>esm_gau</a></code>, <code><a href='esm_gbm.html'>esm_gbm</a></code>,
<code><a href='esm_glm.html'>esm_glm</a></code>, <code><a href='esm_net.html'>esm_net</a></code>, and <code><a href='esm_svm.html'>esm_svm</a></code>.</p></div>

    <h2 class="hasAnchor" id="examples"><a class="anchor" href="#examples"></a>Examples</h2>
    <pre class="examples"><div class='input'><span class='co'># \dontrun{</span>
<span class='fu'><a href='https://rdrr.io/r/utils/data.html'>data</a></span><span class='op'>(</span><span class='st'>"abies"</span><span class='op'>)</span>
<span class='fu'><a href='https://rdrr.io/r/utils/data.html'>data</a></span><span class='op'>(</span><span class='st'>"backg"</span><span class='op'>)</span>
<span class='kw'><a href='https://rdrr.io/r/base/library.html'>require</a></span><span class='op'>(</span><span class='va'><a href='https://dplyr.tidyverse.org'>dplyr</a></span><span class='op'>)</span>

<span class='co'># Using k-fold partition method</span>
<span class='fu'><a href='https://rdrr.io/r/base/Random.html'>set.seed</a></span><span class='op'>(</span><span class='fl'>10</span><span class='op'>)</span>
<span class='va'>abies2</span> <span class='op'>&lt;-</span> <span class='va'>abies</span> <span class='op'>%&gt;%</span>
  <span class='fu'><a href='https://rdrr.io/pkg/terra/man/na.omit.html'>na.omit</a></span><span class='op'>(</span><span class='op'>)</span> <span class='op'>%&gt;%</span>
  <span class='fu'><a href='https://dplyr.tidyverse.org/reference/group_by.html'>group_by</a></span><span class='op'>(</span><span class='va'>pr_ab</span><span class='op'>)</span> <span class='op'>%&gt;%</span>
  <span class='fu'>dplyr</span><span class='fu'>::</span><span class='fu'><a href='https://dplyr.tidyverse.org/reference/slice.html'>slice_sample</a></span><span class='op'>(</span>n <span class='op'>=</span> <span class='fl'>10</span><span class='op'>)</span> <span class='op'>%&gt;%</span>
  <span class='fu'><a href='https://dplyr.tidyverse.org/reference/group_by.html'>group_by</a></span><span class='op'>(</span><span class='op'>)</span>

<span class='va'>abies2</span> <span class='op'>&lt;-</span> <span class='fu'><a href='part_random.html'>part_random</a></span><span class='op'>(</span>
  data <span class='op'>=</span> <span class='va'>abies2</span>,
  pr_ab <span class='op'>=</span> <span class='st'>"pr_ab"</span>,
  method <span class='op'>=</span> <span class='fu'><a href='https://rdrr.io/pkg/terra/man/c.html'>c</a></span><span class='op'>(</span>method <span class='op'>=</span> <span class='st'>"rep_kfold"</span>, folds <span class='op'>=</span> <span class='fl'>5</span>, replicates <span class='op'>=</span> <span class='fl'>5</span><span class='op'>)</span>
<span class='op'>)</span>
<span class='va'>abies2</span>
</div><div class='output co'>#&gt; <span style='color: #949494;'># A tibble: 20 x 18</span>
#&gt;       id pr_ab        x        y   aet   cwd   tmin ppt_djf ppt_jja    pH    awc
#&gt;    <span style='color: #949494; font-style: italic;'>&lt;int&gt;</span> <span style='color: #949494; font-style: italic;'>&lt;dbl&gt;</span>    <span style='color: #949494; font-style: italic;'>&lt;dbl&gt;</span>    <span style='color: #949494; font-style: italic;'>&lt;dbl&gt;</span> <span style='color: #949494; font-style: italic;'>&lt;dbl&gt;</span> <span style='color: #949494; font-style: italic;'>&lt;dbl&gt;</span>  <span style='color: #949494; font-style: italic;'>&lt;dbl&gt;</span>   <span style='color: #949494; font-style: italic;'>&lt;dbl&gt;</span>   <span style='color: #949494; font-style: italic;'>&lt;dbl&gt;</span> <span style='color: #949494; font-style: italic;'>&lt;dbl&gt;</span>  <span style='color: #949494; font-style: italic;'>&lt;dbl&gt;</span>
#&gt; <span style='color: #BCBCBC;'> 1</span> <span style='text-decoration: underline;'>12</span>040     0 -<span style='color: #BB0000; text-decoration: underline;'>308</span><span style='color: #BB0000;'>909.</span>  <span style='text-decoration: underline;'>384</span>248.  573.  332.  4.84     521.   48.8   5.63 0.108 
#&gt; <span style='color: #BCBCBC;'> 2</span> <span style='text-decoration: underline;'>10</span>361     0 -<span style='color: #BB0000; text-decoration: underline;'>254</span><span style='color: #BB0000;'>286.</span>  <span style='text-decoration: underline;'>417</span>158.  260.  469.  2.93     151.   15.1   6.20 0.095<span style='text-decoration: underline;'>0</span>
#&gt; <span style='color: #BCBCBC;'> 3</span>  <span style='text-decoration: underline;'>9</span>402     0 -<span style='color: #BB0000; text-decoration: underline;'>286</span><span style='color: #BB0000;'>979.</span>  <span style='text-decoration: underline;'>386</span>206.  587.  376.  6.45     333.   15.7   5.5  0.160 
#&gt; <span style='color: #BCBCBC;'> 4</span>  <span style='text-decoration: underline;'>9</span>815     0 -<span style='color: #BB0000; text-decoration: underline;'>291</span><span style='color: #BB0000;'>849.</span>  <span style='text-decoration: underline;'>445</span>595.  443.  455.  4.39     332.   19.1   6    0.070<span style='text-decoration: underline;'>0</span>
#&gt; <span style='color: #BCBCBC;'> 5</span> <span style='text-decoration: underline;'>10</span>524     0 -<span style='color: #BB0000; text-decoration: underline;'>256</span><span style='color: #BB0000;'>658.</span>  <span style='text-decoration: underline;'>184</span>438.  355.  568.  5.87     303.   10.6   5.20 0.080<span style='text-decoration: underline;'>0</span>
#&gt; <span style='color: #BCBCBC;'> 6</span>  <span style='text-decoration: underline;'>8</span>860     0  <span style='text-decoration: underline;'>121</span>343. -<span style='color: #BB0000; text-decoration: underline;'>164</span><span style='color: #BB0000;'>170.</span>  354.  733.  3.97     182.    9.83  0    0     
#&gt; <span style='color: #BCBCBC;'> 7</span>  <span style='text-decoration: underline;'>6</span>431     0  <span style='text-decoration: underline;'>107</span>903. -<span style='color: #BB0000; text-decoration: underline;'>122</span><span style='color: #BB0000;'>968.</span>  461.  578.  4.87     161.    7.66  5.90 0.090<span style='text-decoration: underline;'>0</span>
#&gt; <span style='color: #BCBCBC;'> 8</span> <span style='text-decoration: underline;'>11</span>730     0 -<span style='color: #BB0000; text-decoration: underline;'>333</span><span style='color: #BB0000;'>903.</span>  <span style='text-decoration: underline;'>431</span>238.  561.  364.  6.73     387.   25.2   5.80 0.130 
#&gt; <span style='color: #BCBCBC;'> 9</span>   808     0 -<span style='color: #BB0000; text-decoration: underline;'>150</span><span style='color: #BB0000;'>163.</span>  <span style='text-decoration: underline;'>357</span>180.  339.  564.  2.64     220.   15.3   6.40 0.100 
#&gt; <span style='color: #BCBCBC;'>10</span> <span style='text-decoration: underline;'>11</span>054     0 -<span style='color: #BB0000; text-decoration: underline;'>293</span><span style='color: #BB0000;'>663.</span>  <span style='text-decoration: underline;'>340</span>981.  477.  396.  3.89     332.   26.4   4.60 0.063<span style='text-decoration: underline;'>4</span>
#&gt; <span style='color: #BCBCBC;'>11</span>  <span style='text-decoration: underline;'>2</span>960     1  -<span style='color: #BB0000; text-decoration: underline;'>49</span><span style='color: #BB0000;'>273.</span>  <span style='text-decoration: underline;'>181</span>752.  512.  275.  0.920    319.   17.3   5.92 0.090<span style='text-decoration: underline;'>0</span>
#&gt; <span style='color: #BCBCBC;'>12</span>  <span style='text-decoration: underline;'>3</span>065     1  <span style='text-decoration: underline;'>126</span>907. -<span style='color: #BB0000; text-decoration: underline;'>198</span><span style='color: #BB0000;'>892.</span>  322.  544.  0.700    203.   10.6   5.60 0.110 
#&gt; <span style='color: #BCBCBC;'>13</span>  <span style='text-decoration: underline;'>5</span>527     1  <span style='text-decoration: underline;'>116</span>751. -<span style='color: #BB0000; text-decoration: underline;'>181</span><span style='color: #BB0000;'>089.</span>  261.  537.  0.363    178.    7.43  0    0     
#&gt; <span style='color: #BCBCBC;'>14</span>  <span style='text-decoration: underline;'>4</span>035     1  -<span style='color: #BB0000; text-decoration: underline;'>31</span><span style='color: #BB0000;'>777.</span>  <span style='text-decoration: underline;'>115</span>940.  394.  440.  2.07     298.   11.2   6.01 0.076<span style='text-decoration: underline;'>9</span>
#&gt; <span style='color: #BCBCBC;'>15</span>  <span style='text-decoration: underline;'>4</span>081     1   -<span style='color: #BB0000; text-decoration: underline;'>5</span><span style='color: #BB0000;'>158.</span>   <span style='text-decoration: underline;'>90</span>159.  301.  502.  0.703    203.   14.6   6.11 0.063<span style='text-decoration: underline;'>3</span>
#&gt; <span style='color: #BCBCBC;'>16</span>  <span style='text-decoration: underline;'>3</span>087     1  <span style='text-decoration: underline;'>102</span>151. -<span style='color: #BB0000; text-decoration: underline;'>143</span><span style='color: #BB0000;'>976.</span>  299.  425. -<span style='color: #BB0000;'>2.08</span>     205.   13.4   3.88 0.110 
#&gt; <span style='color: #BCBCBC;'>17</span>  <span style='text-decoration: underline;'>3</span>495     1  -<span style='color: #BB0000; text-decoration: underline;'>19</span><span style='color: #BB0000;'>586.</span>   <span style='text-decoration: underline;'>89</span>803.  438.  419.  2.13     189.   15.2   6.19 0.095<span style='text-decoration: underline;'>9</span>
#&gt; <span style='color: #BCBCBC;'>18</span>  <span style='text-decoration: underline;'>4</span>441     1   <span style='text-decoration: underline;'>49</span>405.  -<span style='color: #BB0000; text-decoration: underline;'>60</span><span style='color: #BB0000;'>502.</span>  362.  582.  2.42     218.    7.84  5.64 0.078<span style='text-decoration: underline;'>6</span>
#&gt; <span style='color: #BCBCBC;'>19</span>   301     1 -<span style='color: #BB0000; text-decoration: underline;'>132</span><span style='color: #BB0000;'>516.</span>  <span style='text-decoration: underline;'>270</span>845.  367.  196. -<span style='color: #BB0000;'>2.56</span>     422.   26.3   6.70 0.030<span style='text-decoration: underline;'>0</span>
#&gt; <span style='color: #BCBCBC;'>20</span>  <span style='text-decoration: underline;'>3</span>162     1   <span style='text-decoration: underline;'>59</span>905.  -<span style='color: #BB0000; text-decoration: underline;'>53</span><span style='color: #BB0000;'>634.</span>  319.  626.  1.99     212.    4.50  4.51 0.039<span style='text-decoration: underline;'>6</span>
#&gt; <span style='color: #949494;'># ... with 7 more variables: depth &lt;dbl&gt;, landform &lt;fct&gt;, .part1 &lt;int&gt;,</span>
#&gt; <span style='color: #949494;'>#   .part2 &lt;int&gt;, .part3 &lt;int&gt;, .part4 &lt;int&gt;, .part5 &lt;int&gt;</span></div><div class='input'>
<span class='fu'><a href='https://rdrr.io/r/base/Random.html'>set.seed</a></span><span class='op'>(</span><span class='fl'>10</span><span class='op'>)</span>
<span class='va'>backg2</span> <span class='op'>&lt;-</span> <span class='va'>backg</span> <span class='op'>%&gt;%</span>
  <span class='fu'><a href='https://rdrr.io/pkg/terra/man/na.omit.html'>na.omit</a></span><span class='op'>(</span><span class='op'>)</span> <span class='op'>%&gt;%</span>
  <span class='fu'><a href='https://dplyr.tidyverse.org/reference/group_by.html'>group_by</a></span><span class='op'>(</span><span class='va'>pr_ab</span><span class='op'>)</span> <span class='op'>%&gt;%</span>
  <span class='fu'>dplyr</span><span class='fu'>::</span><span class='fu'><a href='https://dplyr.tidyverse.org/reference/slice.html'>slice_sample</a></span><span class='op'>(</span>n <span class='op'>=</span> <span class='fl'>100</span><span class='op'>)</span> <span class='op'>%&gt;%</span>
  <span class='fu'><a href='https://dplyr.tidyverse.org/reference/group_by.html'>group_by</a></span><span class='op'>(</span><span class='op'>)</span>

<span class='va'>backg2</span> <span class='op'>&lt;-</span> <span class='fu'><a href='part_random.html'>part_random</a></span><span class='op'>(</span>
  data <span class='op'>=</span> <span class='va'>backg2</span>,
  pr_ab <span class='op'>=</span> <span class='st'>"pr_ab"</span>,
  method <span class='op'>=</span> <span class='fu'><a href='https://rdrr.io/pkg/terra/man/c.html'>c</a></span><span class='op'>(</span>method <span class='op'>=</span> <span class='st'>"rep_kfold"</span>, folds <span class='op'>=</span> <span class='fl'>5</span>, replicates <span class='op'>=</span> <span class='fl'>5</span><span class='op'>)</span>
<span class='op'>)</span>
<span class='va'>backg2</span>
</div><div class='output co'>#&gt; <span style='color: #949494;'># A tibble: 100 x 18</span>
#&gt;    pr_ab        x        y   aet   cwd   tmin ppt_djf ppt_jja     pH      awc
#&gt;    <span style='color: #949494; font-style: italic;'>&lt;dbl&gt;</span>    <span style='color: #949494; font-style: italic;'>&lt;dbl&gt;</span>    <span style='color: #949494; font-style: italic;'>&lt;dbl&gt;</span> <span style='color: #949494; font-style: italic;'>&lt;dbl&gt;</span> <span style='color: #949494; font-style: italic;'>&lt;dbl&gt;</span>  <span style='color: #949494; font-style: italic;'>&lt;dbl&gt;</span>   <span style='color: #949494; font-style: italic;'>&lt;dbl&gt;</span>   <span style='color: #949494; font-style: italic;'>&lt;dbl&gt;</span>  <span style='color: #949494; font-style: italic;'>&lt;dbl&gt;</span>    <span style='color: #949494; font-style: italic;'>&lt;dbl&gt;</span>
#&gt; <span style='color: #BCBCBC;'> 1</span>     0  -<span style='color: #BB0000; text-decoration: underline;'>23</span><span style='color: #BB0000;'>361.</span>  <span style='text-decoration: underline;'>129</span>722.  448.  257.  0.683   285.   19.6   0.023<span style='text-decoration: underline;'>0</span> 0.000<span style='text-decoration: underline;'>356</span>
#&gt; <span style='color: #BCBCBC;'> 2</span>     0   <span style='text-decoration: underline;'>54</span>669. -<span style='color: #BB0000; text-decoration: underline;'>311</span><span style='color: #BB0000;'>728.</span>  149. <span style='text-decoration: underline;'>1</span>328. 10.9      24.7   0.710 8.20   0.090<span style='text-decoration: underline;'>0</span>  
#&gt; <span style='color: #BCBCBC;'> 3</span>     0 -<span style='color: #BB0000; text-decoration: underline;'>162</span><span style='color: #BB0000;'>141.</span>  -<span style='color: #BB0000; text-decoration: underline;'>59</span><span style='color: #BB0000;'>278.</span>  332.  863.  9.56    110.    1.92  6.34   0.138   
#&gt; <span style='color: #BCBCBC;'> 4</span>     0  <span style='text-decoration: underline;'>185</span>349. -<span style='color: #BB0000; text-decoration: underline;'>426</span><span style='color: #BB0000;'>208.</span>  380. <span style='text-decoration: underline;'>1</span>105. 11.6     105.    3.04  0.588  0.009<span style='text-decoration: underline;'>95</span> 
#&gt; <span style='color: #BCBCBC;'> 5</span>     0  <span style='text-decoration: underline;'>125</span>949. -<span style='color: #BB0000; text-decoration: underline;'>180</span><span style='color: #BB0000;'>238.</span>  220.  303. -<span style='color: #BB0000;'>4.75</span>    216.    9.95  0      0       
#&gt; <span style='color: #BCBCBC;'> 6</span>     0  -<span style='color: #BB0000; text-decoration: underline;'>27</span><span style='color: #BB0000;'>411.</span> -<span style='color: #BB0000; text-decoration: underline;'>378</span><span style='color: #BB0000;'>148.</span>  322. <span style='text-decoration: underline;'>1</span>047.  8.13     87.8   0.717 7.73   0.141   
#&gt; <span style='color: #BCBCBC;'> 7</span>     0   <span style='text-decoration: underline;'>17</span>409.  -<span style='color: #BB0000; text-decoration: underline;'>99</span><span style='color: #BB0000;'>508.</span>  340. <span style='text-decoration: underline;'>1</span>003.  8.15     93.7   1.49  6.5    0.070<span style='text-decoration: underline;'>8</span>  
#&gt; <span style='color: #BCBCBC;'> 8</span>     0 -<span style='color: #BB0000; text-decoration: underline;'>216</span><span style='color: #BB0000;'>411.</span>  <span style='text-decoration: underline;'>432</span>662.  259.  657.  4.44    110.   16.3   5.37   0.070<span style='text-decoration: underline;'>4</span>  
#&gt; <span style='color: #BCBCBC;'> 9</span>     0  <span style='text-decoration: underline;'>311</span>709. -<span style='color: #BB0000; text-decoration: underline;'>456</span><span style='color: #BB0000;'>718.</span>  271.  877.  6.34     83.0   8.45  0      0       
#&gt; <span style='color: #BCBCBC;'>10</span>     0 -<span style='color: #BB0000; text-decoration: underline;'>211</span><span style='color: #BB0000;'>821.</span>  <span style='text-decoration: underline;'>419</span>972.  369.  633.  3.36     73.2  16.6   6.12   0.158   
#&gt; <span style='color: #949494;'># ... with 90 more rows, and 8 more variables: depth &lt;dbl&gt;, percent_clay &lt;dbl&gt;,</span>
#&gt; <span style='color: #949494;'>#   landform &lt;fct&gt;, .part1 &lt;int&gt;, .part2 &lt;int&gt;, .part3 &lt;int&gt;, .part4 &lt;int&gt;,</span>
#&gt; <span style='color: #949494;'>#   .part5 &lt;int&gt;</span></div><div class='input'>
<span class='co'># Without threshold specification and with kfold</span>
<span class='va'>esm_max_t1</span> <span class='op'>&lt;-</span> <span class='fu'>esm_max</span><span class='op'>(</span>
  data <span class='op'>=</span> <span class='va'>abies2</span>,
  response <span class='op'>=</span> <span class='st'>"pr_ab"</span>,
  predictors <span class='op'>=</span> <span class='fu'><a href='https://rdrr.io/pkg/terra/man/c.html'>c</a></span><span class='op'>(</span><span class='st'>"aet"</span>, <span class='st'>"cwd"</span>, <span class='st'>"tmin"</span>, <span class='st'>"ppt_djf"</span>, <span class='st'>"ppt_jja"</span>, <span class='st'>"pH"</span>, <span class='st'>"awc"</span>, <span class='st'>"depth"</span><span class='op'>)</span>,
  partition <span class='op'>=</span> <span class='st'>".part"</span>,
  thr <span class='op'>=</span> <span class='cn'>NULL</span>,
  background <span class='op'>=</span> <span class='va'>backg2</span>,
  clamp <span class='op'>=</span> <span class='cn'>TRUE</span>,
  classes <span class='op'>=</span> <span class='st'>"default"</span>,
  pred_type <span class='op'>=</span> <span class='st'>"cloglog"</span>,
  regmult <span class='op'>=</span> <span class='fl'>1</span>
<span class='op'>)</span>
</div><div class='output co'>#&gt;   |                                                                              |                                                                      |   0%  |                                                                              |==                                                                    |   4%  |                                                                              |=====                                                                 |   7%  |                                                                              |========                                                              |  11%  |                                                                              |==========                                                            |  14%  |                                                                              |============                                                          |  18%  |                                                                              |===============                                                       |  21%  |                                                                              |==================                                                    |  25%  |                                                                              |====================                                                  |  29%  |                                                                              |======================                                                |  32%  |                                                                              |=========================                                             |  36%  |                                                                              |============================                                          |  39%  |                                                                              |==============================                                        |  43%  |                                                                              |================================                                      |  46%  |                                                                              |===================================                                   |  50%  |                                                                              |======================================                                |  54%  |                                                                              |========================================                              |  57%  |                                                                              |==========================================                            |  61%  |                                                                              |=============================================                         |  64%  |                                                                              |================================================                      |  68%  |                                                                              |==================================================                    |  71%  |                                                                              |====================================================                  |  75%  |                                                                              |=======================================================               |  79%  |                                                                              |==========================================================            |  82%  |                                                                              |============================================================          |  86%  |                                                                              |==============================================================        |  89%  |                                                                              |=================================================================     |  93%  |                                                                              |====================================================================  |  96%  |                                                                              |======================================================================| 100%</div><div class='input'>
<span class='va'>esm_max_t1</span><span class='op'>$</span><span class='va'>esm_model</span> <span class='co'># bivariate model</span>
</div><div class='output co'>#&gt; $`-0.04`
#&gt; 
#&gt; Call:  glmnet::glmnet(x = mm, y = as.factor(p), family = "binomial",      weights = weights, lambda = 10^(seq(4, 0, length.out = 200)) *          sum(reg)/length(reg) * sum(p)/sum(weights), standardize = F,      penalty.factor = reg) 
#&gt; 
#&gt;     Df  %Dev  Lambda
#&gt; 1    0  0.00 1067000
#&gt; 2    0  0.00 1019000
#&gt; 3    0  0.00  972700
#&gt; 4    0  0.00  928700
#&gt; 5    0  0.00  886700
#&gt; 6    0  0.00  846600
#&gt; 7    0  0.00  808300
#&gt; 8    0  0.00  771700
#&gt; 9    0  0.00  736800
#&gt; 10   0  0.00  703500
#&gt; 11   0  0.00  671700
#&gt; 12   0  0.00  641300
#&gt; 13   0  0.00  612300
#&gt; 14   0  0.00  584600
#&gt; 15   0  0.00  558200
#&gt; 16   0  0.00  532900
#&gt; 17   0  0.00  508800
#&gt; 18   0  0.00  485800
#&gt; 19   0  0.00  463800
#&gt; 20   0  0.00  442900
#&gt; 21   0  0.00  422800
#&gt; 22   0  0.00  403700
#&gt; 23   0  0.00  385400
#&gt; 24   0  0.00  368000
#&gt; 25   0  0.00  351400
#&gt; 26   0  0.00  335500
#&gt; 27   0  0.00  320300
#&gt; 28   0  0.00  305800
#&gt; 29   0  0.00  292000
#&gt; 30   0  0.00  278800
#&gt; 31   0  0.00  266200
#&gt; 32   0  0.00  254100
#&gt; 33   0  0.00  242600
#&gt; 34   0  0.00  231700
#&gt; 35   0  0.00  221200
#&gt; 36   0  0.00  211200
#&gt; 37   0  0.00  201600
#&gt; 38   0  0.00  192500
#&gt; 39   0  0.00  183800
#&gt; 40   0  0.00  175500
#&gt; 41   0  0.00  167600
#&gt; 42   0  0.00  160000
#&gt; 43   0  0.00  152700
#&gt; 44   0  0.00  145800
#&gt; 45   0  0.00  139200
#&gt; 46   0  0.00  132900
#&gt; 47   0  0.00  126900
#&gt; 48   0  0.00  121200
#&gt; 49   0  0.00  115700
#&gt; 50   0  0.00  110500
#&gt; 51   0  0.00  105500
#&gt; 52   0  0.00  100700
#&gt; 53   0  0.00   96150
#&gt; 54   0  0.00   91800
#&gt; 55   0  0.00   87650
#&gt; 56   0  0.00   83680
#&gt; 57   0  0.00   79900
#&gt; 58   0  0.00   76290
#&gt; 59   0  0.00   72840
#&gt; 60   0  0.00   69540
#&gt; 61   0  0.00   66400
#&gt; 62   0  0.00   63390
#&gt; 63   0  0.00   60530
#&gt; 64   0  0.00   57790
#&gt; 65   0  0.00   55170
#&gt; 66   0  0.00   52680
#&gt; 67   0  0.00   50300
#&gt; 68   0  0.00   48020
#&gt; 69   0  0.00   45850
#&gt; 70   0  0.00   43780
#&gt; 71   0  0.00   41800
#&gt; 72   0  0.00   39910
#&gt; 73   0  0.00   38100
#&gt; 74   0  0.00   36380
#&gt; 75   0  0.00   34730
#&gt; 76   0  0.00   33160
#&gt; 77   0  0.00   31660
#&gt; 78   0  0.00   30230
#&gt; 79   0  0.00   28860
#&gt; 80   0  0.00   27560
#&gt; 81   0  0.00   26310
#&gt; 82   0  0.00   25120
#&gt; 83   0  0.00   23980
#&gt; 84   0  0.00   22900
#&gt; 85   0  0.00   21860
#&gt; 86   0  0.00   20880
#&gt; 87   0  0.00   19930
#&gt; 88   0  0.00   19030
#&gt; 89   0  0.00   18170
#&gt; 90   0  0.00   17350
#&gt; 91   0  0.00   16560
#&gt; 92   0  0.00   15810
#&gt; 93   0  0.00   15100
#&gt; 94   0  0.00   14420
#&gt; 95   0  0.00   13760
#&gt; 96   0  0.00   13140
#&gt; 97   0  0.00   12550
#&gt; 98   0  0.00   11980
#&gt; 99   0  0.00   11440
#&gt; 100  0  0.00   10920
#&gt; 101  0  0.00   10430
#&gt; 102  0  0.00    9955
#&gt; 103  0  0.00    9504
#&gt; 104  0  0.00    9074
#&gt; 105  0  0.00    8664
#&gt; 106  0  0.00    8272
#&gt; 107  0  0.00    7898
#&gt; 108  0  0.00    7541
#&gt; 109  0  0.00    7200
#&gt; 110  0  0.00    6874
#&gt; 111  0  0.00    6563
#&gt; 112  0  0.00    6266
#&gt; 113  0  0.00    5983
#&gt; 114  0  0.00    5712
#&gt; 115  0  0.00    5454
#&gt; 116  0  0.00    5207
#&gt; 117  0  0.00    4972
#&gt; 118  0  0.00    4747
#&gt; 119  0  0.00    4532
#&gt; 120  0  0.00    4327
#&gt; 121  0  0.00    4132
#&gt; 122  0  0.00    3945
#&gt; 123  0  0.00    3766
#&gt; 124  0  0.00    3596
#&gt; 125  0  0.00    3433
#&gt; 126  0  0.00    3278
#&gt; 127  0  0.00    3130
#&gt; 128  0  0.00    2988
#&gt; 129  0  0.00    2853
#&gt; 130  0  0.00    2724
#&gt; 131  0  0.00    2601
#&gt; 132  0  0.00    2483
#&gt; 133  0  0.00    2371
#&gt; 134  0  0.00    2264
#&gt; 135  0  0.00    2161
#&gt; 136  0  0.00    2063
#&gt; 137  0  0.00    1970
#&gt; 138  0  0.00    1881
#&gt; 139  0  0.00    1796
#&gt; 140  0  0.00    1715
#&gt; 141  0  0.00    1637
#&gt; 142  0  0.00    1563
#&gt; 143  0  0.00    1492
#&gt; 144  0  0.00    1425
#&gt; 145  0  0.00    1361
#&gt; 146  0  0.00    1299
#&gt; 147  0  0.00    1240
#&gt; 148  0  0.00    1184
#&gt; 149  0  0.00    1131
#&gt; 150  0  0.00    1079
#&gt; 151  0  0.00    1031
#&gt; 152  0  0.00     984
#&gt; 153  0  0.00     940
#&gt; 154  0  0.00     897
#&gt; 155  0  0.00     856
#&gt; 156  0  0.00     818
#&gt; 157  0  0.00     781
#&gt; 158  0  0.00     745
#&gt; 159  0  0.00     712
#&gt; 160  0  0.00     680
#&gt; 161  0  0.00     649
#&gt; 162  0  0.00     619
#&gt; 163  0  0.00     591
#&gt; 164  0  0.00     565
#&gt; 165  0  0.00     539
#&gt; 166  1  0.17     515
#&gt; 167  1  0.63     492
#&gt; 168  1  1.06     469
#&gt; 169  1  1.45     448
#&gt; 170  1  1.82     428
#&gt; 171  1  2.15     408
#&gt; 172  1  2.46     390
#&gt; 173  1  2.74     372
#&gt; 174  1  3.01     356
#&gt; 175  1  3.25     339
#&gt; 176  1  3.47     324
#&gt; 177  1  3.68     309
#&gt; 178  1  3.87     295
#&gt; 179  1  4.05     282
#&gt; 180  1  4.21     269
#&gt; 181  1  4.37     257
#&gt; 182  1  4.51     246
#&gt; 183  1  4.64     234
#&gt; 184  1  4.76     224
#&gt; 185  2  5.17     214
#&gt; 186  2  5.68     204
#&gt; 187  2  6.14     195
#&gt; 188  2  6.56     186
#&gt; 189  2  6.95     178
#&gt; 190  2  7.31     170
#&gt; 191  2  7.64     162
#&gt; 192  2  7.95     154
#&gt; 193  2  8.93     148
#&gt; 194  2  9.32     141
#&gt; 195  2  9.69     134
#&gt; 196  2 10.03     128
#&gt; 197  2 10.35     123
#&gt; 198  2 10.64     117
#&gt; 199  2 10.90     112
#&gt; 200  2 11.15     107
#&gt; 
#&gt; $`0.84`
#&gt; 
#&gt; Call:  glmnet::glmnet(x = mm, y = as.factor(p), family = "binomial",      weights = weights, lambda = 10^(seq(4, 0, length.out = 200)) *          sum(reg)/length(reg) * sum(p)/sum(weights), standardize = F,      penalty.factor = reg) 
#&gt; 
#&gt;     Df  %Dev Lambda
#&gt; 1    0  0.00 365000
#&gt; 2    0  0.00 348500
#&gt; 3    0  0.00 332700
#&gt; 4    0  0.00 317700
#&gt; 5    0  0.00 303300
#&gt; 6    0  0.00 289600
#&gt; 7    0  0.00 276500
#&gt; 8    0  0.00 264000
#&gt; 9    0  0.00 252100
#&gt; 10   0  0.00 240700
#&gt; 11   0  0.00 229800
#&gt; 12   0  0.00 219400
#&gt; 13   0  0.00 209500
#&gt; 14   0  0.00 200000
#&gt; 15   0  0.00 190900
#&gt; 16   0  0.00 182300
#&gt; 17   0  0.00 174100
#&gt; 18   0  0.00 166200
#&gt; 19   0  0.00 158700
#&gt; 20   0  0.00 151500
#&gt; 21   0  0.00 144600
#&gt; 22   0  0.00 138100
#&gt; 23   0  0.00 131900
#&gt; 24   0  0.00 125900
#&gt; 25   0  0.00 120200
#&gt; 26   0  0.00 114800
#&gt; 27   0  0.00 109600
#&gt; 28   0  0.00 104600
#&gt; 29   0  0.00  99880
#&gt; 30   0  0.00  95360
#&gt; 31   0  0.00  91050
#&gt; 32   0  0.00  86930
#&gt; 33   0  0.00  83000
#&gt; 34   0  0.00  79250
#&gt; 35   0  0.00  75660
#&gt; 36   0  0.00  72240
#&gt; 37   0  0.00  68970
#&gt; 38   0  0.00  65850
#&gt; 39   0  0.00  62880
#&gt; 40   0  0.00  60030
#&gt; 41   0  0.00  57320
#&gt; 42   0  0.00  54720
#&gt; 43   0  0.00  52250
#&gt; 44   0  0.00  49890
#&gt; 45   0  0.00  47630
#&gt; 46   0  0.00  45480
#&gt; 47   0  0.00  43420
#&gt; 48   0  0.00  41450
#&gt; 49   0  0.00  39580
#&gt; 50   0  0.00  37790
#&gt; 51   0  0.00  36080
#&gt; 52   0  0.00  34450
#&gt; 53   0  0.00  32890
#&gt; 54   0  0.00  31400
#&gt; 55   0  0.00  29980
#&gt; 56   0  0.00  28630
#&gt; 57   0  0.00  27330
#&gt; 58   0  0.00  26100
#&gt; 59   0  0.00  24920
#&gt; 60   0  0.00  23790
#&gt; 61   0  0.00  22710
#&gt; 62   0  0.00  21690
#&gt; 63   0  0.00  20700
#&gt; 64   0  0.00  19770
#&gt; 65   0  0.00  18870
#&gt; 66   0  0.00  18020
#&gt; 67   0  0.00  17210
#&gt; 68   0  0.00  16430
#&gt; 69   0  0.00  15680
#&gt; 70   0  0.00  14970
#&gt; 71   0  0.00  14300
#&gt; 72   0  0.00  13650
#&gt; 73   0  0.00  13030
#&gt; 74   0  0.00  12440
#&gt; 75   0  0.00  11880
#&gt; 76   0  0.00  11340
#&gt; 77   0  0.00  10830
#&gt; 78   0  0.00  10340
#&gt; 79   0  0.00   9873
#&gt; 80   0  0.00   9427
#&gt; 81   0  0.00   9000
#&gt; 82   0  0.00   8593
#&gt; 83   0  0.00   8205
#&gt; 84   0  0.00   7834
#&gt; 85   0  0.00   7479
#&gt; 86   0  0.00   7141
#&gt; 87   0  0.00   6818
#&gt; 88   0  0.00   6510
#&gt; 89   0  0.00   6215
#&gt; 90   0  0.00   5934
#&gt; 91   0  0.00   5666
#&gt; 92   0  0.00   5409
#&gt; 93   0  0.00   5165
#&gt; 94   0  0.00   4931
#&gt; 95   0  0.00   4708
#&gt; 96   0  0.00   4495
#&gt; 97   0  0.00   4292
#&gt; 98   0  0.00   4098
#&gt; 99   0  0.00   3912
#&gt; 100  0  0.00   3735
#&gt; 101  0  0.00   3567
#&gt; 102  0  0.00   3405
#&gt; 103  0  0.00   3251
#&gt; 104  0  0.00   3104
#&gt; 105  0  0.00   2964
#&gt; 106  0  0.00   2830
#&gt; 107  0  0.00   2702
#&gt; 108  0  0.00   2580
#&gt; 109  0  0.00   2463
#&gt; 110  0  0.00   2351
#&gt; 111  0  0.00   2245
#&gt; 112  0  0.00   2144
#&gt; 113  0  0.00   2047
#&gt; 114  0  0.00   1954
#&gt; 115  0  0.00   1866
#&gt; 116  0  0.00   1781
#&gt; 117  0  0.00   1701
#&gt; 118  0  0.00   1624
#&gt; 119  0  0.00   1550
#&gt; 120  0  0.00   1480
#&gt; 121  0  0.00   1413
#&gt; 122  0  0.00   1349
#&gt; 123  0  0.00   1288
#&gt; 124  0  0.00   1230
#&gt; 125  1  0.81   1174
#&gt; 126  1  2.82   1121
#&gt; 127  1  4.73   1071
#&gt; 128  1  6.55   1022
#&gt; 129  1  8.30    976
#&gt; 130  1  9.98    932
#&gt; 131  1 11.61    890
#&gt; 132  1 13.20    849
#&gt; 133  1 14.74    811
#&gt; 134  1 16.25    774
#&gt; 135  1 17.74    739
#&gt; 136  1 19.21    706
#&gt; 137  1 20.65    674
#&gt; 138  1 22.08    644
#&gt; 139  1 23.50    614
#&gt; 140  1 24.91    587
#&gt; 141  1 26.31    560
#&gt; 142  1 27.70    535
#&gt; 143  1 29.08    510
#&gt; 144  1 30.46    487
#&gt; 145  1 31.83    465
#&gt; 146  1 33.19    444
#&gt; 147  1 34.54    424
#&gt; 148  1 35.89    405
#&gt; 149  1 37.22    387
#&gt; 150  1 38.54    369
#&gt; 151  1 39.86    353
#&gt; 152  1 41.16    337
#&gt; 153  1 42.47    321
#&gt; 154  1 43.75    307
#&gt; 155  1 45.04    293
#&gt; 156  1 46.31    280
#&gt; 157  1 47.57    267
#&gt; 158  1 48.82    255
#&gt; 159  1 50.06    244
#&gt; 160  1 51.28    232
#&gt; 161  1 52.48    222
#&gt; 162  1 53.65    212
#&gt; 163  1 54.80    202
#&gt; 164  1 55.91    193
#&gt; 165  1 56.99    184
#&gt; 166  1 58.03    176
#&gt; 167  1 59.04    168
#&gt; 168  1 60.02    160
#&gt; 169  1 60.95    153
#&gt; 170  1 61.85    146
#&gt; 171  1 62.72    140
#&gt; 172  1 63.55    133
#&gt; 173  1 64.36    127
#&gt; 174  1 65.14    122
#&gt; 175  1 65.90    116
#&gt; 176  1 66.63    111
#&gt; 177  1 67.34    106
#&gt; 178  1 68.04    101
#&gt; 179  2 69.24     96
#&gt; 180  2 70.55     92
#&gt; 181  2 71.70     88
#&gt; 182  2 72.73     84
#&gt; 183  2 73.67     80
#&gt; 184  2 74.55     77
#&gt; 185  2 75.37     73
#&gt; 186  2 76.15     70
#&gt; 187  2 76.89     67
#&gt; 188  2 77.61     64
#&gt; 189  2 78.31     61
#&gt; 190  2 78.98     58
#&gt; 191  2 79.64     55
#&gt; 192  2 80.28     53
#&gt; 193  2 80.91     50
#&gt; 194  2 81.53     48
#&gt; 195  2 82.14     46
#&gt; 196  2 82.73     44
#&gt; 197  2 83.31     42
#&gt; 198  2 83.87     40
#&gt; 199  2 84.42     38
#&gt; 200  2 84.95     36
#&gt; 
#&gt; $`-0.24`
#&gt; 
#&gt; Call:  glmnet::glmnet(x = mm, y = as.factor(p), family = "binomial",      weights = weights, lambda = 10^(seq(4, 0, length.out = 200)) *          sum(reg)/length(reg) * sum(p)/sum(weights), standardize = F,      penalty.factor = reg) 
#&gt; 
#&gt;     Df %Dev Lambda
#&gt; 1    0 0.00 653600
#&gt; 2    0 0.00 624000
#&gt; 3    0 0.00 595800
#&gt; 4    0 0.00 568900
#&gt; 5    0 0.00 543100
#&gt; 6    0 0.00 518600
#&gt; 7    0 0.00 495100
#&gt; 8    0 0.00 472700
#&gt; 9    0 0.00 451300
#&gt; 10   0 0.00 430900
#&gt; 11   0 0.00 411400
#&gt; 12   0 0.00 392800
#&gt; 13   0 0.00 375100
#&gt; 14   0 0.00 358100
#&gt; 15   0 0.00 341900
#&gt; 16   0 0.00 326400
#&gt; 17   0 0.00 311700
#&gt; 18   0 0.00 297600
#&gt; 19   0 0.00 284100
#&gt; 20   0 0.00 271300
#&gt; 21   0 0.00 259000
#&gt; 22   0 0.00 247300
#&gt; 23   0 0.00 236100
#&gt; 24   0 0.00 225400
#&gt; 25   0 0.00 215200
#&gt; 26   0 0.00 205500
#&gt; 27   0 0.00 196200
#&gt; 28   0 0.00 187300
#&gt; 29   0 0.00 178900
#&gt; 30   0 0.00 170800
#&gt; 31   0 0.00 163000
#&gt; 32   0 0.00 155700
#&gt; 33   0 0.00 148600
#&gt; 34   0 0.00 141900
#&gt; 35   0 0.00 135500
#&gt; 36   0 0.00 129400
#&gt; 37   0 0.00 123500
#&gt; 38   0 0.00 117900
#&gt; 39   0 0.00 112600
#&gt; 40   0 0.00 107500
#&gt; 41   0 0.00 102600
#&gt; 42   0 0.00  97990
#&gt; 43   0 0.00  93560
#&gt; 44   0 0.00  89330
#&gt; 45   0 0.00  85290
#&gt; 46   0 0.00  81430
#&gt; 47   0 0.00  77750
#&gt; 48   0 0.00  74230
#&gt; 49   0 0.00  70870
#&gt; 50   0 0.00  67670
#&gt; 51   0 0.00  64610
#&gt; 52   0 0.00  61690
#&gt; 53   0 0.00  58900
#&gt; 54   0 0.00  56230
#&gt; 55   0 0.00  53690
#&gt; 56   0 0.00  51260
#&gt; 57   0 0.00  48940
#&gt; 58   0 0.00  46730
#&gt; 59   0 0.00  44610
#&gt; 60   0 0.00  42600
#&gt; 61   0 0.00  40670
#&gt; 62   0 0.00  38830
#&gt; 63   0 0.00  37070
#&gt; 64   0 0.00  35400
#&gt; 65   0 0.00  33800
#&gt; 66   0 0.00  32270
#&gt; 67   0 0.00  30810
#&gt; 68   0 0.00  29420
#&gt; 69   0 0.00  28080
#&gt; 70   0 0.00  26810
#&gt; 71   0 0.00  25600
#&gt; 72   0 0.00  24440
#&gt; 73   0 0.00  23340
#&gt; 74   0 0.00  22280
#&gt; 75   0 0.00  21270
#&gt; 76   0 0.00  20310
#&gt; 77   0 0.00  19390
#&gt; 78   0 0.00  18520
#&gt; 79   0 0.00  17680
#&gt; 80   0 0.00  16880
#&gt; 81   0 0.00  16120
#&gt; 82   0 0.00  15390
#&gt; 83   0 0.00  14690
#&gt; 84   0 0.00  14030
#&gt; 85   0 0.00  13390
#&gt; 86   0 0.00  12790
#&gt; 87   0 0.00  12210
#&gt; 88   0 0.00  11660
#&gt; 89   0 0.00  11130
#&gt; 90   0 0.00  10630
#&gt; 91   0 0.00  10150
#&gt; 92   0 0.00   9686
#&gt; 93   0 0.00   9248
#&gt; 94   0 0.00   8830
#&gt; 95   0 0.00   8431
#&gt; 96   0 0.00   8049
#&gt; 97   0 0.00   7685
#&gt; 98   0 0.00   7338
#&gt; 99   0 0.00   7006
#&gt; 100  0 0.00   6689
#&gt; 101  0 0.00   6386
#&gt; 102  0 0.00   6098
#&gt; 103  0 0.00   5822
#&gt; 104  0 0.00   5558
#&gt; 105  0 0.00   5307
#&gt; 106  0 0.00   5067
#&gt; 107  0 0.00   4838
#&gt; 108  0 0.00   4619
#&gt; 109  0 0.00   4410
#&gt; 110  0 0.00   4211
#&gt; 111  0 0.00   4020
#&gt; 112  0 0.00   3838
#&gt; 113  0 0.00   3665
#&gt; 114  0 0.00   3499
#&gt; 115  0 0.00   3341
#&gt; 116  0 0.00   3190
#&gt; 117  0 0.00   3045
#&gt; 118  0 0.00   2908
#&gt; 119  0 0.00   2776
#&gt; 120  0 0.00   2651
#&gt; 121  0 0.00   2531
#&gt; 122  0 0.00   2416
#&gt; 123  0 0.00   2307
#&gt; 124  0 0.00   2203
#&gt; 125  0 0.00   2103
#&gt; 126  0 0.00   2008
#&gt; 127  0 0.00   1917
#&gt; 128  0 0.00   1830
#&gt; 129  0 0.00   1748
#&gt; 130  0 0.00   1669
#&gt; 131  0 0.00   1593
#&gt; 132  0 0.00   1521
#&gt; 133  0 0.00   1452
#&gt; 134  0 0.00   1387
#&gt; 135  0 0.00   1324
#&gt; 136  0 0.00   1264
#&gt; 137  0 0.00   1207
#&gt; 138  0 0.00   1152
#&gt; 139  0 0.00   1100
#&gt; 140  0 0.00   1050
#&gt; 141  0 0.00   1003
#&gt; 142  0 0.00    958
#&gt; 143  0 0.00    914
#&gt; 144  0 0.00    873
#&gt; 145  0 0.00    833
#&gt; 146  0 0.00    796
#&gt; 147  0 0.00    760
#&gt; 148  0 0.00    725
#&gt; 149  0 0.00    692
#&gt; 150  0 0.00    661
#&gt; 151  0 0.00    631
#&gt; 152  0 0.00    603
#&gt; 153  0 0.00    576
#&gt; 154  0 0.00    550
#&gt; 155  0 0.00    525
#&gt; 156  0 0.00    501
#&gt; 157  0 0.00    478
#&gt; 158  0 0.00    457
#&gt; 159  0 0.00    436
#&gt; 160  0 0.00    416
#&gt; 161  0 0.00    397
#&gt; 162  0 0.00    379
#&gt; 163  0 0.00    362
#&gt; 164  0 0.00    346
#&gt; 165  0 0.00    330
#&gt; 166  1 0.17    315
#&gt; 167  1 0.63    301
#&gt; 168  1 1.06    287
#&gt; 169  1 1.45    274
#&gt; 170  1 1.82    262
#&gt; 171  1 2.15    250
#&gt; 172  1 2.46    239
#&gt; 173  1 2.74    228
#&gt; 174  1 3.01    218
#&gt; 175  1 3.25    208
#&gt; 176  1 3.47    198
#&gt; 177  1 3.68    190
#&gt; 178  1 3.87    181
#&gt; 179  1 4.05    173
#&gt; 180  1 4.21    165
#&gt; 181  1 4.37    158
#&gt; 182  1 4.51    150
#&gt; 183  1 4.64    144
#&gt; 184  1 4.76    137
#&gt; 185  1 4.87    131
#&gt; 186  1 4.97    125
#&gt; 187  1 5.07    119
#&gt; 188  1 5.15    114
#&gt; 189  1 5.24    109
#&gt; 190  1 5.31    104
#&gt; 191  1 5.38     99
#&gt; 192  1 5.44     95
#&gt; 193  1 5.50     90
#&gt; 194  1 5.56     86
#&gt; 195  1 5.61     82
#&gt; 196  1 5.65     79
#&gt; 197  1 5.70     75
#&gt; 198  1 5.74     72
#&gt; 199  1 5.77     68
#&gt; 200  1 5.81     65
#&gt; 
#&gt; $`-0.39`
#&gt; 
#&gt; Call:  glmnet::glmnet(x = mm, y = as.factor(p), family = "binomial",      weights = weights, lambda = 10^(seq(4, 0, length.out = 200)) *          sum(reg)/length(reg) * sum(p)/sum(weights), standardize = F,      penalty.factor = reg) 
#&gt; 
#&gt;     Df %Dev Lambda
#&gt; 1    0 0.00 366200
#&gt; 2    0 0.00 349700
#&gt; 3    0 0.00 333900
#&gt; 4    0 0.00 318800
#&gt; 5    0 0.00 304300
#&gt; 6    0 0.00 290600
#&gt; 7    0 0.00 277400
#&gt; 8    0 0.00 264900
#&gt; 9    0 0.00 252900
#&gt; 10   0 0.00 241500
#&gt; 11   0 0.00 230500
#&gt; 12   0 0.00 220100
#&gt; 13   0 0.00 210200
#&gt; 14   0 0.00 200700
#&gt; 15   0 0.00 191600
#&gt; 16   0 0.00 182900
#&gt; 17   0 0.00 174600
#&gt; 18   0 0.00 166700
#&gt; 19   0 0.00 159200
#&gt; 20   0 0.00 152000
#&gt; 21   0 0.00 145100
#&gt; 22   0 0.00 138600
#&gt; 23   0 0.00 132300
#&gt; 24   0 0.00 126300
#&gt; 25   0 0.00 120600
#&gt; 26   0 0.00 115100
#&gt; 27   0 0.00 109900
#&gt; 28   0 0.00 105000
#&gt; 29   0 0.00 100200
#&gt; 30   0 0.00  95690
#&gt; 31   0 0.00  91360
#&gt; 32   0 0.00  87230
#&gt; 33   0 0.00  83280
#&gt; 34   0 0.00  79510
#&gt; 35   0 0.00  75920
#&gt; 36   0 0.00  72480
#&gt; 37   0 0.00  69210
#&gt; 38   0 0.00  66080
#&gt; 39   0 0.00  63090
#&gt; 40   0 0.00  60230
#&gt; 41   0 0.00  57510
#&gt; 42   0 0.00  54910
#&gt; 43   0 0.00  52420
#&gt; 44   0 0.00  50050
#&gt; 45   0 0.00  47790
#&gt; 46   0 0.00  45630
#&gt; 47   0 0.00  43560
#&gt; 48   0 0.00  41590
#&gt; 49   0 0.00  39710
#&gt; 50   0 0.00  37920
#&gt; 51   0 0.00  36200
#&gt; 52   0 0.00  34560
#&gt; 53   0 0.00  33000
#&gt; 54   0 0.00  31510
#&gt; 55   0 0.00  30080
#&gt; 56   0 0.00  28720
#&gt; 57   0 0.00  27420
#&gt; 58   0 0.00  26180
#&gt; 59   0 0.00  25000
#&gt; 60   0 0.00  23870
#&gt; 61   0 0.00  22790
#&gt; 62   0 0.00  21760
#&gt; 63   0 0.00  20770
#&gt; 64   0 0.00  19830
#&gt; 65   0 0.00  18940
#&gt; 66   0 0.00  18080
#&gt; 67   0 0.00  17260
#&gt; 68   0 0.00  16480
#&gt; 69   0 0.00  15740
#&gt; 70   0 0.00  15030
#&gt; 71   0 0.00  14350
#&gt; 72   0 0.00  13700
#&gt; 73   0 0.00  13080
#&gt; 74   0 0.00  12490
#&gt; 75   0 0.00  11920
#&gt; 76   0 0.00  11380
#&gt; 77   0 0.00  10870
#&gt; 78   0 0.00  10380
#&gt; 79   0 0.00   9906
#&gt; 80   0 0.00   9458
#&gt; 81   0 0.00   9031
#&gt; 82   0 0.00   8622
#&gt; 83   0 0.00   8232
#&gt; 84   0 0.00   7860
#&gt; 85   0 0.00   7504
#&gt; 86   0 0.00   7165
#&gt; 87   0 0.00   6841
#&gt; 88   0 0.00   6532
#&gt; 89   0 0.00   6236
#&gt; 90   0 0.00   5954
#&gt; 91   0 0.00   5685
#&gt; 92   0 0.00   5428
#&gt; 93   0 0.00   5182
#&gt; 94   0 0.00   4948
#&gt; 95   0 0.00   4724
#&gt; 96   0 0.00   4510
#&gt; 97   0 0.00   4306
#&gt; 98   0 0.00   4112
#&gt; 99   0 0.00   3926
#&gt; 100  0 0.00   3748
#&gt; 101  0 0.00   3579
#&gt; 102  0 0.00   3417
#&gt; 103  0 0.00   3262
#&gt; 104  0 0.00   3115
#&gt; 105  0 0.00   2974
#&gt; 106  0 0.00   2839
#&gt; 107  0 0.00   2711
#&gt; 108  0 0.00   2588
#&gt; 109  0 0.00   2471
#&gt; 110  0 0.00   2359
#&gt; 111  0 0.00   2253
#&gt; 112  0 0.00   2151
#&gt; 113  0 0.00   2054
#&gt; 114  0 0.00   1961
#&gt; 115  0 0.00   1872
#&gt; 116  0 0.00   1787
#&gt; 117  0 0.00   1706
#&gt; 118  0 0.00   1629
#&gt; 119  0 0.00   1556
#&gt; 120  0 0.00   1485
#&gt; 121  0 0.00   1418
#&gt; 122  0 0.00   1354
#&gt; 123  0 0.00   1293
#&gt; 124  0 0.00   1234
#&gt; 125  0 0.00   1178
#&gt; 126  0 0.00   1125
#&gt; 127  0 0.00   1074
#&gt; 128  0 0.00   1026
#&gt; 129  0 0.00    979
#&gt; 130  0 0.00    935
#&gt; 131  0 0.00    893
#&gt; 132  0 0.00    852
#&gt; 133  0 0.00    814
#&gt; 134  0 0.00    777
#&gt; 135  0 0.00    742
#&gt; 136  0 0.00    708
#&gt; 137  0 0.00    676
#&gt; 138  0 0.00    646
#&gt; 139  0 0.00    616
#&gt; 140  0 0.00    589
#&gt; 141  0 0.00    562
#&gt; 142  0 0.00    536
#&gt; 143  0 0.00    512
#&gt; 144  0 0.00    489
#&gt; 145  0 0.00    467
#&gt; 146  0 0.00    446
#&gt; 147  0 0.00    426
#&gt; 148  0 0.00    406
#&gt; 149  0 0.00    388
#&gt; 150  0 0.00    370
#&gt; 151  0 0.00    354
#&gt; 152  0 0.00    338
#&gt; 153  0 0.00    322
#&gt; 154  0 0.00    308
#&gt; 155  0 0.00    294
#&gt; 156  0 0.00    281
#&gt; 157  0 0.00    268
#&gt; 158  0 0.00    256
#&gt; 159  0 0.00    244
#&gt; 160  0 0.00    233
#&gt; 161  1 0.05    223
#&gt; 162  1 0.22    213
#&gt; 163  1 0.38    203
#&gt; 164  1 0.54    194
#&gt; 165  1 0.69    185
#&gt; 166  1 0.83    177
#&gt; 167  1 0.97    169
#&gt; 168  2 1.16    161
#&gt; 169  2 1.56    154
#&gt; 170  2 1.93    147
#&gt; 171  2 2.28    140
#&gt; 172  2 2.59    134
#&gt; 173  2 2.89    128
#&gt; 174  2 3.16    122
#&gt; 175  2 3.41    116
#&gt; 176  2 3.64    111
#&gt; 177  2 3.86    106
#&gt; 178  2 4.06    101
#&gt; 179  2 4.25     97
#&gt; 180  2 4.42     92
#&gt; 181  2 4.59     88
#&gt; 182  2 4.74     84
#&gt; 183  2 4.88     80
#&gt; 184  2 5.01     77
#&gt; 185  2 5.13     73
#&gt; 186  2 5.24     70
#&gt; 187  2 5.35     67
#&gt; 188  2 5.45     64
#&gt; 189  2 5.54     61
#&gt; 190  2 5.63     58
#&gt; 191  2 5.71     56
#&gt; 192  2 5.78     53
#&gt; 193  2 5.85     51
#&gt; 194  2 5.92     48
#&gt; 195  3 5.98     46
#&gt; 196  3 6.11     44
#&gt; 197  3 6.25     42
#&gt; 198  3 6.38     40
#&gt; 199  3 6.50     38
#&gt; 200  2 6.59     37
#&gt; 
#&gt; $`-0.0600000000000001`
#&gt; 
#&gt; Call:  glmnet::glmnet(x = mm, y = as.factor(p), family = "binomial",      weights = weights, lambda = 10^(seq(4, 0, length.out = 200)) *          sum(reg)/length(reg) * sum(p)/sum(weights), standardize = F,      penalty.factor = reg) 
#&gt; 
#&gt;     Df %Dev Lambda
#&gt; 1    0 0.00 365100
#&gt; 2    0 0.00 348600
#&gt; 3    0 0.00 332800
#&gt; 4    0 0.00 317700
#&gt; 5    0 0.00 303400
#&gt; 6    0 0.00 289700
#&gt; 7    0 0.00 276600
#&gt; 8    0 0.00 264000
#&gt; 9    0 0.00 252100
#&gt; 10   0 0.00 240700
#&gt; 11   0 0.00 229800
#&gt; 12   0 0.00 219400
#&gt; 13   0 0.00 209500
#&gt; 14   0 0.00 200000
#&gt; 15   0 0.00 191000
#&gt; 16   0 0.00 182300
#&gt; 17   0 0.00 174100
#&gt; 18   0 0.00 166200
#&gt; 19   0 0.00 158700
#&gt; 20   0 0.00 151500
#&gt; 21   0 0.00 144700
#&gt; 22   0 0.00 138100
#&gt; 23   0 0.00 131900
#&gt; 24   0 0.00 125900
#&gt; 25   0 0.00 120200
#&gt; 26   0 0.00 114800
#&gt; 27   0 0.00 109600
#&gt; 28   0 0.00 104600
#&gt; 29   0 0.00  99900
#&gt; 30   0 0.00  95380
#&gt; 31   0 0.00  91070
#&gt; 32   0 0.00  86950
#&gt; 33   0 0.00  83020
#&gt; 34   0 0.00  79260
#&gt; 35   0 0.00  75680
#&gt; 36   0 0.00  72250
#&gt; 37   0 0.00  68990
#&gt; 38   0 0.00  65870
#&gt; 39   0 0.00  62890
#&gt; 40   0 0.00  60040
#&gt; 41   0 0.00  57330
#&gt; 42   0 0.00  54730
#&gt; 43   0 0.00  52260
#&gt; 44   0 0.00  49900
#&gt; 45   0 0.00  47640
#&gt; 46   0 0.00  45480
#&gt; 47   0 0.00  43430
#&gt; 48   0 0.00  41460
#&gt; 49   0 0.00  39590
#&gt; 50   0 0.00  37800
#&gt; 51   0 0.00  36090
#&gt; 52   0 0.00  34460
#&gt; 53   0 0.00  32900
#&gt; 54   0 0.00  31410
#&gt; 55   0 0.00  29990
#&gt; 56   0 0.00  28630
#&gt; 57   0 0.00  27340
#&gt; 58   0 0.00  26100
#&gt; 59   0 0.00  24920
#&gt; 60   0 0.00  23790
#&gt; 61   0 0.00  22720
#&gt; 62   0 0.00  21690
#&gt; 63   0 0.00  20710
#&gt; 64   0 0.00  19770
#&gt; 65   0 0.00  18880
#&gt; 66   0 0.00  18020
#&gt; 67   0 0.00  17210
#&gt; 68   0 0.00  16430
#&gt; 69   0 0.00  15690
#&gt; 70   0 0.00  14980
#&gt; 71   0 0.00  14300
#&gt; 72   0 0.00  13650
#&gt; 73   0 0.00  13040
#&gt; 74   0 0.00  12450
#&gt; 75   0 0.00  11880
#&gt; 76   0 0.00  11350
#&gt; 77   0 0.00  10830
#&gt; 78   0 0.00  10340
#&gt; 79   0 0.00   9875
#&gt; 80   0 0.00   9428
#&gt; 81   0 0.00   9002
#&gt; 82   0 0.00   8595
#&gt; 83   0 0.00   8206
#&gt; 84   0 0.00   7835
#&gt; 85   0 0.00   7481
#&gt; 86   0 0.00   7142
#&gt; 87   0 0.00   6819
#&gt; 88   0 0.00   6511
#&gt; 89   0 0.00   6216
#&gt; 90   0 0.00   5935
#&gt; 91   0 0.00   5667
#&gt; 92   0 0.00   5410
#&gt; 93   0 0.00   5166
#&gt; 94   0 0.00   4932
#&gt; 95   0 0.00   4709
#&gt; 96   0 0.00   4496
#&gt; 97   0 0.00   4293
#&gt; 98   0 0.00   4099
#&gt; 99   0 0.00   3913
#&gt; 100  0 0.00   3736
#&gt; 101  0 0.00   3567
#&gt; 102  0 0.00   3406
#&gt; 103  0 0.00   3252
#&gt; 104  0 0.00   3105
#&gt; 105  0 0.00   2964
#&gt; 106  0 0.00   2830
#&gt; 107  0 0.00   2702
#&gt; 108  0 0.00   2580
#&gt; 109  0 0.00   2463
#&gt; 110  0 0.00   2352
#&gt; 111  0 0.00   2246
#&gt; 112  0 0.00   2144
#&gt; 113  0 0.00   2047
#&gt; 114  0 0.00   1954
#&gt; 115  0 0.00   1866
#&gt; 116  0 0.00   1782
#&gt; 117  0 0.00   1701
#&gt; 118  0 0.00   1624
#&gt; 119  0 0.00   1551
#&gt; 120  0 0.00   1481
#&gt; 121  0 0.00   1414
#&gt; 122  0 0.00   1350
#&gt; 123  0 0.00   1289
#&gt; 124  0 0.00   1230
#&gt; 125  0 0.00   1175
#&gt; 126  0 0.00   1122
#&gt; 127  0 0.00   1071
#&gt; 128  0 0.00   1022
#&gt; 129  0 0.00    976
#&gt; 130  0 0.00    932
#&gt; 131  0 0.00    890
#&gt; 132  0 0.00    850
#&gt; 133  0 0.00    811
#&gt; 134  0 0.00    774
#&gt; 135  0 0.00    740
#&gt; 136  0 0.00    706
#&gt; 137  0 0.00    674
#&gt; 138  0 0.00    644
#&gt; 139  0 0.00    614
#&gt; 140  0 0.00    587
#&gt; 141  0 0.00    560
#&gt; 142  0 0.00    535
#&gt; 143  0 0.00    511
#&gt; 144  0 0.00    488
#&gt; 145  0 0.00    466
#&gt; 146  0 0.00    444
#&gt; 147  0 0.00    424
#&gt; 148  0 0.00    405
#&gt; 149  0 0.00    387
#&gt; 150  0 0.00    369
#&gt; 151  0 0.00    353
#&gt; 152  0 0.00    337
#&gt; 153  0 0.00    321
#&gt; 154  0 0.00    307
#&gt; 155  0 0.00    293
#&gt; 156  0 0.00    280
#&gt; 157  0 0.00    267
#&gt; 158  0 0.00    255
#&gt; 159  0 0.00    244
#&gt; 160  0 0.00    232
#&gt; 161  0 0.00    222
#&gt; 162  0 0.00    212
#&gt; 163  0 0.00    202
#&gt; 164  0 0.00    193
#&gt; 165  0 0.00    184
#&gt; 166  1 0.17    176
#&gt; 167  1 0.63    168
#&gt; 168  1 1.06    160
#&gt; 169  1 1.45    153
#&gt; 170  1 1.82    146
#&gt; 171  1 2.15    140
#&gt; 172  1 2.46    133
#&gt; 173  1 2.74    127
#&gt; 174  1 3.01    122
#&gt; 175  1 3.25    116
#&gt; 176  1 3.47    111
#&gt; 177  1 3.68    106
#&gt; 178  1 3.87    101
#&gt; 179  1 4.05     96
#&gt; 180  1 4.21     92
#&gt; 181  1 4.37     88
#&gt; 182  1 4.51     84
#&gt; 183  1 4.64     80
#&gt; 184  1 4.76     77
#&gt; 185  1 4.87     73
#&gt; 186  1 4.97     70
#&gt; 187  1 5.07     67
#&gt; 188  1 5.15     64
#&gt; 189  1 5.24     61
#&gt; 190  1 5.31     58
#&gt; 191  1 5.38     55
#&gt; 192  1 5.44     53
#&gt; 193  1 5.50     50
#&gt; 194  1 5.56     48
#&gt; 195  1 5.61     46
#&gt; 196  1 5.65     44
#&gt; 197  1 5.70     42
#&gt; 198  1 5.74     40
#&gt; 199  1 5.77     38
#&gt; 200  1 5.81     37
#&gt; 
#&gt; $`0.25`
#&gt; 
#&gt; Call:  glmnet::glmnet(x = mm, y = as.factor(p), family = "binomial",      weights = weights, lambda = 10^(seq(4, 0, length.out = 200)) *          sum(reg)/length(reg) * sum(p)/sum(weights), standardize = F,      penalty.factor = reg) 
#&gt; 
#&gt;     Df %Dev Lambda
#&gt; 1    0 0.00 365000
#&gt; 2    0 0.00 348500
#&gt; 3    0 0.00 332700
#&gt; 4    0 0.00 317700
#&gt; 5    0 0.00 303300
#&gt; 6    0 0.00 289600
#&gt; 7    0 0.00 276500
#&gt; 8    0 0.00 264000
#&gt; 9    0 0.00 252000
#&gt; 10   0 0.00 240600
#&gt; 11   0 0.00 229800
#&gt; 12   0 0.00 219400
#&gt; 13   0 0.00 209400
#&gt; 14   0 0.00 200000
#&gt; 15   0 0.00 190900
#&gt; 16   0 0.00 182300
#&gt; 17   0 0.00 174000
#&gt; 18   0 0.00 166200
#&gt; 19   0 0.00 158700
#&gt; 20   0 0.00 151500
#&gt; 21   0 0.00 144600
#&gt; 22   0 0.00 138100
#&gt; 23   0 0.00 131800
#&gt; 24   0 0.00 125900
#&gt; 25   0 0.00 120200
#&gt; 26   0 0.00 114800
#&gt; 27   0 0.00 109600
#&gt; 28   0 0.00 104600
#&gt; 29   0 0.00  99870
#&gt; 30   0 0.00  95360
#&gt; 31   0 0.00  91040
#&gt; 32   0 0.00  86930
#&gt; 33   0 0.00  82990
#&gt; 34   0 0.00  79240
#&gt; 35   0 0.00  75660
#&gt; 36   0 0.00  72240
#&gt; 37   0 0.00  68970
#&gt; 38   0 0.00  65850
#&gt; 39   0 0.00  62870
#&gt; 40   0 0.00  60030
#&gt; 41   0 0.00  57310
#&gt; 42   0 0.00  54720
#&gt; 43   0 0.00  52250
#&gt; 44   0 0.00  49880
#&gt; 45   0 0.00  47630
#&gt; 46   0 0.00  45470
#&gt; 47   0 0.00  43420
#&gt; 48   0 0.00  41450
#&gt; 49   0 0.00  39580
#&gt; 50   0 0.00  37790
#&gt; 51   0 0.00  36080
#&gt; 52   0 0.00  34450
#&gt; 53   0 0.00  32890
#&gt; 54   0 0.00  31400
#&gt; 55   0 0.00  29980
#&gt; 56   0 0.00  28620
#&gt; 57   0 0.00  27330
#&gt; 58   0 0.00  26090
#&gt; 59   0 0.00  24910
#&gt; 60   0 0.00  23790
#&gt; 61   0 0.00  22710
#&gt; 62   0 0.00  21680
#&gt; 63   0 0.00  20700
#&gt; 64   0 0.00  19770
#&gt; 65   0 0.00  18870
#&gt; 66   0 0.00  18020
#&gt; 67   0 0.00  17200
#&gt; 68   0 0.00  16430
#&gt; 69   0 0.00  15680
#&gt; 70   0 0.00  14970
#&gt; 71   0 0.00  14300
#&gt; 72   0 0.00  13650
#&gt; 73   0 0.00  13030
#&gt; 74   0 0.00  12440
#&gt; 75   0 0.00  11880
#&gt; 76   0 0.00  11340
#&gt; 77   0 0.00  10830
#&gt; 78   0 0.00  10340
#&gt; 79   0 0.00   9873
#&gt; 80   0 0.00   9426
#&gt; 81   0 0.00   9000
#&gt; 82   0 0.00   8593
#&gt; 83   0 0.00   8204
#&gt; 84   0 0.00   7833
#&gt; 85   0 0.00   7479
#&gt; 86   0 0.00   7140
#&gt; 87   0 0.00   6817
#&gt; 88   0 0.00   6509
#&gt; 89   0 0.00   6215
#&gt; 90   0 0.00   5934
#&gt; 91   0 0.00   5665
#&gt; 92   0 0.00   5409
#&gt; 93   0 0.00   5164
#&gt; 94   0 0.00   4931
#&gt; 95   0 0.00   4708
#&gt; 96   0 0.00   4495
#&gt; 97   0 0.00   4292
#&gt; 98   0 0.00   4097
#&gt; 99   0 0.00   3912
#&gt; 100  0 0.00   3735
#&gt; 101  0 0.00   3566
#&gt; 102  0 0.00   3405
#&gt; 103  0 0.00   3251
#&gt; 104  0 0.00   3104
#&gt; 105  0 0.00   2964
#&gt; 106  0 0.00   2830
#&gt; 107  0 0.00   2702
#&gt; 108  0 0.00   2579
#&gt; 109  0 0.00   2463
#&gt; 110  0 0.00   2351
#&gt; 111  0 0.00   2245
#&gt; 112  0 0.00   2143
#&gt; 113  0 0.00   2046
#&gt; 114  0 0.00   1954
#&gt; 115  0 0.00   1866
#&gt; 116  0 0.00   1781
#&gt; 117  0 0.00   1701
#&gt; 118  0 0.00   1624
#&gt; 119  0 0.00   1550
#&gt; 120  0 0.00   1480
#&gt; 121  0 0.00   1413
#&gt; 122  0 0.00   1349
#&gt; 123  0 0.00   1288
#&gt; 124  0 0.00   1230
#&gt; 125  0 0.00   1174
#&gt; 126  0 0.00   1121
#&gt; 127  0 0.00   1071
#&gt; 128  0 0.00   1022
#&gt; 129  0 0.00    976
#&gt; 130  0 0.00    932
#&gt; 131  0 0.00    890
#&gt; 132  0 0.00    849
#&gt; 133  0 0.00    811
#&gt; 134  0 0.00    774
#&gt; 135  0 0.00    739
#&gt; 136  0 0.00    706
#&gt; 137  0 0.00    674
#&gt; 138  0 0.00    643
#&gt; 139  0 0.00    614
#&gt; 140  0 0.00    586
#&gt; 141  0 0.00    560
#&gt; 142  0 0.00    535
#&gt; 143  0 0.00    510
#&gt; 144  0 0.00    487
#&gt; 145  0 0.00    465
#&gt; 146  0 0.00    444
#&gt; 147  0 0.00    424
#&gt; 148  0 0.00    405
#&gt; 149  0 0.00    387
#&gt; 150  0 0.00    369
#&gt; 151  0 0.00    352
#&gt; 152  0 0.00    337
#&gt; 153  0 0.00    321
#&gt; 154  0 0.00    307
#&gt; 155  0 0.00    293
#&gt; 156  0 0.00    280
#&gt; 157  0 0.00    267
#&gt; 158  0 0.00    255
#&gt; 159  0 0.00    243
#&gt; 160  0 0.00    232
#&gt; 161  0 0.00    222
#&gt; 162  0 0.00    212
#&gt; 163  0 0.00    202
#&gt; 164  0 0.00    193
#&gt; 165  0 0.00    184
#&gt; 166  1 0.17    176
#&gt; 167  1 0.63    168
#&gt; 168  1 1.06    160
#&gt; 169  1 1.45    153
#&gt; 170  1 1.82    146
#&gt; 171  1 2.15    140
#&gt; 172  1 2.46    133
#&gt; 173  1 2.74    127
#&gt; 174  1 3.01    122
#&gt; 175  1 3.25    116
#&gt; 176  1 3.47    111
#&gt; 177  1 3.68    106
#&gt; 178  1 3.87    101
#&gt; 179  1 4.05     96
#&gt; 180  1 4.21     92
#&gt; 181  1 4.37     88
#&gt; 182  1 4.51     84
#&gt; 183  1 4.64     80
#&gt; 184  1 4.76     77
#&gt; 185  1 4.87     73
#&gt; 186  1 4.97     70
#&gt; 187  1 5.07     67
#&gt; 188  1 5.15     64
#&gt; 189  1 5.24     61
#&gt; 190  1 5.31     58
#&gt; 191  1 5.38     55
#&gt; 192  2 5.46     53
#&gt; 193  2 5.56     50
#&gt; 194  2 5.66     48
#&gt; 195  2 5.75     46
#&gt; 196  2 5.83     44
#&gt; 197  2 5.90     42
#&gt; 198  2 5.98     40
#&gt; 199  2 6.05     38
#&gt; 200  2 6.11     36
#&gt; 
#&gt; $`0.64`
#&gt; 
#&gt; Call:  glmnet::glmnet(x = mm, y = as.factor(p), family = "binomial",      weights = weights, lambda = 10^(seq(4, 0, length.out = 200)) *          sum(reg)/length(reg) * sum(p)/sum(weights), standardize = F,      penalty.factor = reg) 
#&gt; 
#&gt;     Df  %Dev Lambda
#&gt; 1    0  0.00 459800
#&gt; 2    0  0.00 439000
#&gt; 3    0  0.00 419100
#&gt; 4    0  0.00 400200
#&gt; 5    0  0.00 382100
#&gt; 6    0  0.00 364800
#&gt; 7    0  0.00 348300
#&gt; 8    0  0.00 332500
#&gt; 9    0  0.00 317500
#&gt; 10   0  0.00 303100
#&gt; 11   0  0.00 289400
#&gt; 12   0  0.00 276300
#&gt; 13   0  0.00 263800
#&gt; 14   0  0.00 251900
#&gt; 15   0  0.00 240500
#&gt; 16   0  0.00 229600
#&gt; 17   0  0.00 219300
#&gt; 18   0  0.00 209300
#&gt; 19   0  0.00 199900
#&gt; 20   0  0.00 190800
#&gt; 21   0  0.00 182200
#&gt; 22   0  0.00 174000
#&gt; 23   0  0.00 166100
#&gt; 24   0  0.00 158600
#&gt; 25   0  0.00 151400
#&gt; 26   0  0.00 144600
#&gt; 27   0  0.00 138000
#&gt; 28   0  0.00 131800
#&gt; 29   0  0.00 125800
#&gt; 30   0  0.00 120100
#&gt; 31   0  0.00 114700
#&gt; 32   0  0.00 109500
#&gt; 33   0  0.00 104600
#&gt; 34   0  0.00  99820
#&gt; 35   0  0.00  95310
#&gt; 36   0  0.00  91000
#&gt; 37   0  0.00  86880
#&gt; 38   0  0.00  82950
#&gt; 39   0  0.00  79200
#&gt; 40   0  0.00  75620
#&gt; 41   0  0.00  72200
#&gt; 42   0  0.00  68930
#&gt; 43   0  0.00  65820
#&gt; 44   0  0.00  62840
#&gt; 45   0  0.00  60000
#&gt; 46   0  0.00  57280
#&gt; 47   0  0.00  54690
#&gt; 48   0  0.00  52220
#&gt; 49   0  0.00  49860
#&gt; 50   0  0.00  47600
#&gt; 51   0  0.00  45450
#&gt; 52   0  0.00  43390
#&gt; 53   0  0.00  41430
#&gt; 54   0  0.00  39560
#&gt; 55   0  0.00  37770
#&gt; 56   0  0.00  36060
#&gt; 57   0  0.00  34430
#&gt; 58   0  0.00  32870
#&gt; 59   0  0.00  31380
#&gt; 60   0  0.00  29970
#&gt; 61   0  0.00  28610
#&gt; 62   0  0.00  27320
#&gt; 63   0  0.00  26080
#&gt; 64   0  0.00  24900
#&gt; 65   0  0.00  23770
#&gt; 66   0  0.00  22700
#&gt; 67   0  0.00  21670
#&gt; 68   0  0.00  20690
#&gt; 69   0  0.00  19760
#&gt; 70   0  0.00  18860
#&gt; 71   0  0.00  18010
#&gt; 72   0  0.00  17200
#&gt; 73   0  0.00  16420
#&gt; 74   0  0.00  15680
#&gt; 75   0  0.00  14970
#&gt; 76   0  0.00  14290
#&gt; 77   0  0.00  13640
#&gt; 78   0  0.00  13030
#&gt; 79   0  0.00  12440
#&gt; 80   0  0.00  11870
#&gt; 81   0  0.00  11340
#&gt; 82   0  0.00  10820
#&gt; 83   0  0.00  10330
#&gt; 84   0  0.00   9868
#&gt; 85   0  0.00   9421
#&gt; 86   0  0.00   8995
#&gt; 87   0  0.00   8588
#&gt; 88   0  0.00   8200
#&gt; 89   0  0.00   7829
#&gt; 90   0  0.00   7475
#&gt; 91   0  0.00   7137
#&gt; 92   0  0.00   6814
#&gt; 93   0  0.00   6506
#&gt; 94   0  0.00   6212
#&gt; 95   0  0.00   5931
#&gt; 96   0  0.00   5662
#&gt; 97   0  0.00   5406
#&gt; 98   0  0.00   5162
#&gt; 99   0  0.00   4928
#&gt; 100  0  0.00   4705
#&gt; 101  0  0.00   4493
#&gt; 102  0  0.00   4289
#&gt; 103  0  0.00   4095
#&gt; 104  0  0.00   3910
#&gt; 105  0  0.00   3733
#&gt; 106  0  0.00   3564
#&gt; 107  0  0.00   3403
#&gt; 108  0  0.00   3249
#&gt; 109  0  0.00   3102
#&gt; 110  0  0.00   2962
#&gt; 111  0  0.00   2828
#&gt; 112  0  0.00   2700
#&gt; 113  0  0.00   2578
#&gt; 114  0  0.00   2461
#&gt; 115  0  0.00   2350
#&gt; 116  0  0.00   2244
#&gt; 117  0  0.00   2142
#&gt; 118  0  0.00   2045
#&gt; 119  0  0.00   1953
#&gt; 120  0  0.00   1865
#&gt; 121  0  0.00   1780
#&gt; 122  0  0.00   1700
#&gt; 123  0  0.00   1623
#&gt; 124  0  0.00   1549
#&gt; 125  0  0.00   1479
#&gt; 126  0  0.00   1412
#&gt; 127  0  0.00   1349
#&gt; 128  0  0.00   1288
#&gt; 129  0  0.00   1229
#&gt; 130  0  0.00   1174
#&gt; 131  0  0.00   1121
#&gt; 132  0  0.00   1070
#&gt; 133  0  0.00   1022
#&gt; 134  0  0.00    975
#&gt; 135  0  0.00    931
#&gt; 136  0  0.00    889
#&gt; 137  0  0.00    849
#&gt; 138  0  0.00    811
#&gt; 139  0  0.00    774
#&gt; 140  0  0.00    739
#&gt; 141  0  0.00    706
#&gt; 142  0  0.00    674
#&gt; 143  0  0.00    643
#&gt; 144  0  0.00    614
#&gt; 145  0  0.00    586
#&gt; 146  0  0.00    560
#&gt; 147  0  0.00    534
#&gt; 148  0  0.00    510
#&gt; 149  0  0.00    487
#&gt; 150  0  0.00    465
#&gt; 151  0  0.00    444
#&gt; 152  0  0.00    424
#&gt; 153  0  0.00    405
#&gt; 154  0  0.00    386
#&gt; 155  0  0.00    369
#&gt; 156  0  0.00    352
#&gt; 157  0  0.00    336
#&gt; 158  0  0.00    321
#&gt; 159  0  0.00    307
#&gt; 160  0  0.00    293
#&gt; 161  0  0.00    280
#&gt; 162  0  0.00    267
#&gt; 163  0  0.00    255
#&gt; 164  0  0.00    243
#&gt; 165  0  0.00    232
#&gt; 166  1  0.17    222
#&gt; 167  1  0.63    212
#&gt; 168  2  2.29    202
#&gt; 169  2  8.82    193
#&gt; 170  2 14.29    184
#&gt; 171  2 18.84    176
#&gt; 172  2 22.62    168
#&gt; 173  2 25.75    160
#&gt; 174  2 28.38    153
#&gt; 175  2 30.61    146
#&gt; 176  2 32.53    140
#&gt; 177  2 34.21    133
#&gt; 178  2 35.69    127
#&gt; 179  2 37.02    122
#&gt; 180  2 38.21    116
#&gt; 181  2 39.29    111
#&gt; 182  2 40.29    106
#&gt; 183  2 41.20    101
#&gt; 184  2 42.05     96
#&gt; 185  2 42.84     92
#&gt; 186  2 43.57     88
#&gt; 187  2 44.26     84
#&gt; 188  2 44.91     80
#&gt; 189  2 45.53     76
#&gt; 190  2 46.10     73
#&gt; 191  2 46.65     70
#&gt; 192  2 47.17     67
#&gt; 193  2 47.67     64
#&gt; 194  2 48.14     61
#&gt; 195  2 48.59     58
#&gt; 196  2 49.02     55
#&gt; 197  2 49.43     53
#&gt; 198  2 49.83     50
#&gt; 199  2 50.20     48
#&gt; 200  2 50.57     46
#&gt; 
#&gt; $`0.9`
#&gt; 
#&gt; Call:  glmnet::glmnet(x = mm, y = as.factor(p), family = "binomial",      weights = weights, lambda = 10^(seq(4, 0, length.out = 200)) *          sum(reg)/length(reg) * sum(p)/sum(weights), standardize = F,      penalty.factor = reg) 
#&gt; 
#&gt;     Df  %Dev Lambda
#&gt; 1    0  0.00 702100
#&gt; 2    0  0.00 670300
#&gt; 3    0  0.00 640000
#&gt; 4    0  0.00 611000
#&gt; 5    0  0.00 583400
#&gt; 6    0  0.00 557000
#&gt; 7    0  0.00 531800
#&gt; 8    0  0.00 507800
#&gt; 9    0  0.00 484800
#&gt; 10   0  0.00 462900
#&gt; 11   0  0.00 441900
#&gt; 12   0  0.00 422000
#&gt; 13   0  0.00 402900
#&gt; 14   0  0.00 384700
#&gt; 15   0  0.00 367300
#&gt; 16   0  0.00 350600
#&gt; 17   0  0.00 334800
#&gt; 18   0  0.00 319600
#&gt; 19   0  0.00 305200
#&gt; 20   0  0.00 291400
#&gt; 21   0  0.00 278200
#&gt; 22   0  0.00 265600
#&gt; 23   0  0.00 253600
#&gt; 24   0  0.00 242100
#&gt; 25   0  0.00 231200
#&gt; 26   0  0.00 220700
#&gt; 27   0  0.00 210700
#&gt; 28   0  0.00 201200
#&gt; 29   0  0.00 192100
#&gt; 30   0  0.00 183400
#&gt; 31   0  0.00 175100
#&gt; 32   0  0.00 167200
#&gt; 33   0  0.00 159600
#&gt; 34   0  0.00 152400
#&gt; 35   0  0.00 145500
#&gt; 36   0  0.00 139000
#&gt; 37   0  0.00 132700
#&gt; 38   0  0.00 126700
#&gt; 39   0  0.00 120900
#&gt; 40   0  0.00 115500
#&gt; 41   0  0.00 110200
#&gt; 42   0  0.00 105300
#&gt; 43   0  0.00 100500
#&gt; 44   0  0.00  95950
#&gt; 45   0  0.00  91610
#&gt; 46   0  0.00  87470
#&gt; 47   0  0.00  83510
#&gt; 48   0  0.00  79740
#&gt; 49   0  0.00  76130
#&gt; 50   0  0.00  72690
#&gt; 51   0  0.00  69400
#&gt; 52   0  0.00  66260
#&gt; 53   0  0.00  63260
#&gt; 54   0  0.00  60400
#&gt; 55   0  0.00  57670
#&gt; 56   0  0.00  55060
#&gt; 57   0  0.00  52570
#&gt; 58   0  0.00  50190
#&gt; 59   0  0.00  47920
#&gt; 60   0  0.00  45760
#&gt; 61   0  0.00  43690
#&gt; 62   0  0.00  41710
#&gt; 63   0  0.00  39820
#&gt; 64   0  0.00  38020
#&gt; 65   0  0.00  36300
#&gt; 66   0  0.00  34660
#&gt; 67   0  0.00  33090
#&gt; 68   0  0.00  31600
#&gt; 69   0  0.00  30170
#&gt; 70   0  0.00  28800
#&gt; 71   0  0.00  27500
#&gt; 72   0  0.00  26260
#&gt; 73   0  0.00  25070
#&gt; 74   0  0.00  23940
#&gt; 75   0  0.00  22850
#&gt; 76   0  0.00  21820
#&gt; 77   0  0.00  20830
#&gt; 78   0  0.00  19890
#&gt; 79   0  0.00  18990
#&gt; 80   0  0.00  18130
#&gt; 81   0  0.00  17310
#&gt; 82   0  0.00  16530
#&gt; 83   0  0.00  15780
#&gt; 84   0  0.00  15070
#&gt; 85   0  0.00  14390
#&gt; 86   0  0.00  13740
#&gt; 87   0  0.00  13110
#&gt; 88   0  0.00  12520
#&gt; 89   0  0.00  11950
#&gt; 90   0  0.00  11410
#&gt; 91   0  0.00  10900
#&gt; 92   0  0.00  10400
#&gt; 93   0  0.00   9934
#&gt; 94   0  0.00   9485
#&gt; 95   0  0.00   9056
#&gt; 96   0  0.00   8646
#&gt; 97   0  0.00   8255
#&gt; 98   0  0.00   7882
#&gt; 99   0  0.00   7525
#&gt; 100  0  0.00   7185
#&gt; 101  0  0.00   6860
#&gt; 102  0  0.00   6550
#&gt; 103  0  0.00   6254
#&gt; 104  0  0.00   5971
#&gt; 105  0  0.00   5701
#&gt; 106  0  0.00   5443
#&gt; 107  0  0.00   5197
#&gt; 108  0  0.00   4962
#&gt; 109  0  0.00   4737
#&gt; 110  0  0.00   4523
#&gt; 111  0  0.00   4318
#&gt; 112  0  0.00   4123
#&gt; 113  0  0.00   3937
#&gt; 114  0  0.00   3759
#&gt; 115  0  0.00   3589
#&gt; 116  0  0.00   3426
#&gt; 117  0  0.00   3271
#&gt; 118  0  0.00   3123
#&gt; 119  0  0.00   2982
#&gt; 120  0  0.00   2847
#&gt; 121  0  0.00   2718
#&gt; 122  0  0.00   2595
#&gt; 123  0  0.00   2478
#&gt; 124  0  0.00   2366
#&gt; 125  1  0.81   2259
#&gt; 126  1  2.82   2157
#&gt; 127  1  4.73   2059
#&gt; 128  1  6.55   1966
#&gt; 129  1  8.30   1877
#&gt; 130  1  9.98   1792
#&gt; 131  1 11.61   1711
#&gt; 132  1 13.20   1634
#&gt; 133  1 14.74   1560
#&gt; 134  1 16.25   1489
#&gt; 135  1 17.74   1422
#&gt; 136  1 19.21   1358
#&gt; 137  1 20.65   1296
#&gt; 138  1 22.08   1238
#&gt; 139  1 23.50   1182
#&gt; 140  1 24.91   1128
#&gt; 141  1 26.31   1077
#&gt; 142  1 27.70   1029
#&gt; 143  1 29.08    982
#&gt; 144  1 30.46    938
#&gt; 145  1 31.83    895
#&gt; 146  1 33.19    855
#&gt; 147  1 34.54    816
#&gt; 148  1 35.89    779
#&gt; 149  1 37.22    744
#&gt; 150  1 38.54    710
#&gt; 151  1 39.86    678
#&gt; 152  1 41.16    647
#&gt; 153  1 42.47    618
#&gt; 154  1 43.75    590
#&gt; 155  1 45.04    564
#&gt; 156  1 46.31    538
#&gt; 157  1 47.57    514
#&gt; 158  1 48.82    490
#&gt; 159  1 50.06    468
#&gt; 160  1 51.28    447
#&gt; 161  1 52.48    427
#&gt; 162  1 53.65    408
#&gt; 163  1 54.80    389
#&gt; 164  1 55.91    372
#&gt; 165  1 56.99    355
#&gt; 166  1 58.03    339
#&gt; 167  1 59.04    323
#&gt; 168  1 60.02    309
#&gt; 169  1 60.95    295
#&gt; 170  1 61.85    281
#&gt; 171  1 62.72    269
#&gt; 172  1 63.55    257
#&gt; 173  1 64.36    245
#&gt; 174  1 65.14    234
#&gt; 175  1 65.90    223
#&gt; 176  1 66.63    213
#&gt; 177  1 67.34    204
#&gt; 178  1 68.04    194
#&gt; 179  2 69.24    186
#&gt; 180  2 70.55    177
#&gt; 181  2 71.70    169
#&gt; 182  2 72.73    162
#&gt; 183  2 73.67    154
#&gt; 184  2 74.55    147
#&gt; 185  2 75.37    141
#&gt; 186  2 76.15    134
#&gt; 187  2 76.89    128
#&gt; 188  2 77.61    122
#&gt; 189  2 78.31    117
#&gt; 190  2 78.98    112
#&gt; 191  2 79.64    106
#&gt; 192  2 80.28    102
#&gt; 193  2 80.91     97
#&gt; 194  2 81.53     93
#&gt; 195  2 82.14     88
#&gt; 196  2 82.73     84
#&gt; 197  2 83.31     81
#&gt; 198  2 83.87     77
#&gt; 199  2 84.42     74
#&gt; 200  2 84.95     70
#&gt; 
#&gt; $`-0.02`
#&gt; 
#&gt; Call:  glmnet::glmnet(x = mm, y = as.factor(p), family = "binomial",      weights = weights, lambda = 10^(seq(4, 0, length.out = 200)) *          sum(reg)/length(reg) * sum(p)/sum(weights), standardize = F,      penalty.factor = reg) 
#&gt; 
#&gt;     Df %Dev Lambda
#&gt; 1    0 0.00 990700
#&gt; 2    0 0.00 945800
#&gt; 3    0 0.00 903100
#&gt; 4    0 0.00 862200
#&gt; 5    0 0.00 823200
#&gt; 6    0 0.00 786000
#&gt; 7    0 0.00 750400
#&gt; 8    0 0.00 716500
#&gt; 9    0 0.00 684100
#&gt; 10   0 0.00 653200
#&gt; 11   0 0.00 623600
#&gt; 12   0 0.00 595400
#&gt; 13   0 0.00 568500
#&gt; 14   0 0.00 542800
#&gt; 15   0 0.00 518200
#&gt; 16   0 0.00 494800
#&gt; 17   0 0.00 472400
#&gt; 18   0 0.00 451000
#&gt; 19   0 0.00 430600
#&gt; 20   0 0.00 411200
#&gt; 21   0 0.00 392600
#&gt; 22   0 0.00 374800
#&gt; 23   0 0.00 357900
#&gt; 24   0 0.00 341700
#&gt; 25   0 0.00 326200
#&gt; 26   0 0.00 311500
#&gt; 27   0 0.00 297400
#&gt; 28   0 0.00 283900
#&gt; 29   0 0.00 271100
#&gt; 30   0 0.00 258800
#&gt; 31   0 0.00 247100
#&gt; 32   0 0.00 235900
#&gt; 33   0 0.00 225300
#&gt; 34   0 0.00 215100
#&gt; 35   0 0.00 205400
#&gt; 36   0 0.00 196100
#&gt; 37   0 0.00 187200
#&gt; 38   0 0.00 178700
#&gt; 39   0 0.00 170600
#&gt; 40   0 0.00 162900
#&gt; 41   0 0.00 155600
#&gt; 42   0 0.00 148500
#&gt; 43   0 0.00 141800
#&gt; 44   0 0.00 135400
#&gt; 45   0 0.00 129300
#&gt; 46   0 0.00 123400
#&gt; 47   0 0.00 117800
#&gt; 48   0 0.00 112500
#&gt; 49   0 0.00 107400
#&gt; 50   0 0.00 102600
#&gt; 51   0 0.00  97930
#&gt; 52   0 0.00  93500
#&gt; 53   0 0.00  89270
#&gt; 54   0 0.00  85230
#&gt; 55   0 0.00  81380
#&gt; 56   0 0.00  77690
#&gt; 57   0 0.00  74180
#&gt; 58   0 0.00  70830
#&gt; 59   0 0.00  67620
#&gt; 60   0 0.00  64560
#&gt; 61   0 0.00  61640
#&gt; 62   0 0.00  58860
#&gt; 63   0 0.00  56190
#&gt; 64   0 0.00  53650
#&gt; 65   0 0.00  51230
#&gt; 66   0 0.00  48910
#&gt; 67   0 0.00  46700
#&gt; 68   0 0.00  44580
#&gt; 69   0 0.00  42570
#&gt; 70   0 0.00  40640
#&gt; 71   0 0.00  38800
#&gt; 72   0 0.00  37050
#&gt; 73   0 0.00  35370
#&gt; 74   0 0.00  33770
#&gt; 75   0 0.00  32250
#&gt; 76   0 0.00  30790
#&gt; 77   0 0.00  29400
#&gt; 78   0 0.00  28070
#&gt; 79   0 0.00  26800
#&gt; 80   0 0.00  25580
#&gt; 81   0 0.00  24430
#&gt; 82   0 0.00  23320
#&gt; 83   0 0.00  22270
#&gt; 84   0 0.00  21260
#&gt; 85   0 0.00  20300
#&gt; 86   0 0.00  19380
#&gt; 87   0 0.00  18500
#&gt; 88   0 0.00  17670
#&gt; 89   0 0.00  16870
#&gt; 90   0 0.00  16110
#&gt; 91   0 0.00  15380
#&gt; 92   0 0.00  14680
#&gt; 93   0 0.00  14020
#&gt; 94   0 0.00  13380
#&gt; 95   0 0.00  12780
#&gt; 96   0 0.00  12200
#&gt; 97   0 0.00  11650
#&gt; 98   0 0.00  11120
#&gt; 99   0 0.00  10620
#&gt; 100  0 0.00  10140
#&gt; 101  0 0.00   9680
#&gt; 102  0 0.00   9242
#&gt; 103  0 0.00   8824
#&gt; 104  0 0.00   8425
#&gt; 105  0 0.00   8044
#&gt; 106  0 0.00   7680
#&gt; 107  0 0.00   7333
#&gt; 108  0 0.00   7001
#&gt; 109  0 0.00   6684
#&gt; 110  0 0.00   6382
#&gt; 111  0 0.00   6093
#&gt; 112  0 0.00   5818
#&gt; 113  0 0.00   5555
#&gt; 114  0 0.00   5304
#&gt; 115  0 0.00   5064
#&gt; 116  0 0.00   4835
#&gt; 117  0 0.00   4616
#&gt; 118  0 0.00   4407
#&gt; 119  0 0.00   4208
#&gt; 120  0 0.00   4018
#&gt; 121  0 0.00   3836
#&gt; 122  0 0.00   3662
#&gt; 123  0 0.00   3497
#&gt; 124  0 0.00   3339
#&gt; 125  0 0.00   3188
#&gt; 126  0 0.00   3043
#&gt; 127  0 0.00   2906
#&gt; 128  0 0.00   2774
#&gt; 129  0 0.00   2649
#&gt; 130  0 0.00   2529
#&gt; 131  0 0.00   2415
#&gt; 132  0 0.00   2305
#&gt; 133  0 0.00   2201
#&gt; 134  0 0.00   2102
#&gt; 135  0 0.00   2007
#&gt; 136  0 0.00   1916
#&gt; 137  0 0.00   1829
#&gt; 138  0 0.00   1746
#&gt; 139  0 0.00   1667
#&gt; 140  0 0.00   1592
#&gt; 141  0 0.00   1520
#&gt; 142  0 0.00   1451
#&gt; 143  0 0.00   1386
#&gt; 144  0 0.00   1323
#&gt; 145  0 0.00   1263
#&gt; 146  0 0.00   1206
#&gt; 147  0 0.00   1151
#&gt; 148  0 0.00   1099
#&gt; 149  0 0.00   1050
#&gt; 150  0 0.00   1002
#&gt; 151  0 0.00    957
#&gt; 152  0 0.00    914
#&gt; 153  0 0.00    872
#&gt; 154  0 0.00    833
#&gt; 155  0 0.00    795
#&gt; 156  0 0.00    759
#&gt; 157  0 0.00    725
#&gt; 158  0 0.00    692
#&gt; 159  0 0.00    661
#&gt; 160  0 0.00    631
#&gt; 161  0 0.00    602
#&gt; 162  0 0.00    575
#&gt; 163  0 0.00    549
#&gt; 164  0 0.00    524
#&gt; 165  0 0.00    500
#&gt; 166  0 0.00    478
#&gt; 167  0 0.00    456
#&gt; 168  0 0.00    436
#&gt; 169  0 0.00    416
#&gt; 170  0 0.00    397
#&gt; 171  0 0.00    379
#&gt; 172  0 0.00    362
#&gt; 173  0 0.00    346
#&gt; 174  0 0.00    330
#&gt; 175  0 0.00    315
#&gt; 176  0 0.00    301
#&gt; 177  0 0.00    287
#&gt; 178  0 0.00    274
#&gt; 179  1 0.12    262
#&gt; 180  1 0.28    250
#&gt; 181  1 0.42    239
#&gt; 182  1 0.55    228
#&gt; 183  1 0.68    218
#&gt; 184  1 0.79    208
#&gt; 185  1 0.90    198
#&gt; 186  1 1.00    189
#&gt; 187  1 1.10    181
#&gt; 188  1 1.18    173
#&gt; 189  1 1.26    165
#&gt; 190  1 1.34    157
#&gt; 191  1 1.41    150
#&gt; 192  2 1.77    144
#&gt; 193  2 2.18    137
#&gt; 194  2 2.55    131
#&gt; 195  2 2.90    125
#&gt; 196  2 3.22    119
#&gt; 197  2 3.52    114
#&gt; 198  3 4.19    109
#&gt; 199  2 4.70    104
#&gt; 200  2 5.06     99
#&gt; 
#&gt; $`-0.1`
#&gt; 
#&gt; Call:  glmnet::glmnet(x = mm, y = as.factor(p), family = "binomial",      weights = weights, lambda = 10^(seq(4, 0, length.out = 200)) *          sum(reg)/length(reg) * sum(p)/sum(weights), standardize = F,      penalty.factor = reg) 
#&gt; 
#&gt;     Df  %Dev Lambda
#&gt; 1    0  0.00 703300
#&gt; 2    0  0.00 671500
#&gt; 3    0  0.00 641100
#&gt; 4    0  0.00 612100
#&gt; 5    0  0.00 584400
#&gt; 6    0  0.00 558000
#&gt; 7    0  0.00 532800
#&gt; 8    0  0.00 508700
#&gt; 9    0  0.00 485700
#&gt; 10   0  0.00 463700
#&gt; 11   0  0.00 442700
#&gt; 12   0  0.00 422700
#&gt; 13   0  0.00 403600
#&gt; 14   0  0.00 385300
#&gt; 15   0  0.00 367900
#&gt; 16   0  0.00 351300
#&gt; 17   0  0.00 335400
#&gt; 18   0  0.00 320200
#&gt; 19   0  0.00 305700
#&gt; 20   0  0.00 291900
#&gt; 21   0  0.00 278700
#&gt; 22   0  0.00 266100
#&gt; 23   0  0.00 254100
#&gt; 24   0  0.00 242600
#&gt; 25   0  0.00 231600
#&gt; 26   0  0.00 221100
#&gt; 27   0  0.00 211100
#&gt; 28   0  0.00 201600
#&gt; 29   0  0.00 192500
#&gt; 30   0  0.00 183700
#&gt; 31   0  0.00 175400
#&gt; 32   0  0.00 167500
#&gt; 33   0  0.00 159900
#&gt; 34   0  0.00 152700
#&gt; 35   0  0.00 145800
#&gt; 36   0  0.00 139200
#&gt; 37   0  0.00 132900
#&gt; 38   0  0.00 126900
#&gt; 39   0  0.00 121100
#&gt; 40   0  0.00 115700
#&gt; 41   0  0.00 110400
#&gt; 42   0  0.00 105400
#&gt; 43   0  0.00 100700
#&gt; 44   0  0.00  96120
#&gt; 45   0  0.00  91770
#&gt; 46   0  0.00  87620
#&gt; 47   0  0.00  83660
#&gt; 48   0  0.00  79880
#&gt; 49   0  0.00  76260
#&gt; 50   0  0.00  72810
#&gt; 51   0  0.00  69520
#&gt; 52   0  0.00  66380
#&gt; 53   0  0.00  63370
#&gt; 54   0  0.00  60510
#&gt; 55   0  0.00  57770
#&gt; 56   0  0.00  55160
#&gt; 57   0  0.00  52660
#&gt; 58   0  0.00  50280
#&gt; 59   0  0.00  48010
#&gt; 60   0  0.00  45840
#&gt; 61   0  0.00  43760
#&gt; 62   0  0.00  41780
#&gt; 63   0  0.00  39890
#&gt; 64   0  0.00  38090
#&gt; 65   0  0.00  36370
#&gt; 66   0  0.00  34720
#&gt; 67   0  0.00  33150
#&gt; 68   0  0.00  31650
#&gt; 69   0  0.00  30220
#&gt; 70   0  0.00  28850
#&gt; 71   0  0.00  27550
#&gt; 72   0  0.00  26300
#&gt; 73   0  0.00  25110
#&gt; 74   0  0.00  23980
#&gt; 75   0  0.00  22890
#&gt; 76   0  0.00  21860
#&gt; 77   0  0.00  20870
#&gt; 78   0  0.00  19920
#&gt; 79   0  0.00  19020
#&gt; 80   0  0.00  18160
#&gt; 81   0  0.00  17340
#&gt; 82   0  0.00  16560
#&gt; 83   0  0.00  15810
#&gt; 84   0  0.00  15090
#&gt; 85   0  0.00  14410
#&gt; 86   0  0.00  13760
#&gt; 87   0  0.00  13140
#&gt; 88   0  0.00  12540
#&gt; 89   0  0.00  11980
#&gt; 90   0  0.00  11430
#&gt; 91   0  0.00  10920
#&gt; 92   0  0.00  10420
#&gt; 93   0  0.00   9952
#&gt; 94   0  0.00   9501
#&gt; 95   0  0.00   9072
#&gt; 96   0  0.00   8661
#&gt; 97   0  0.00   8270
#&gt; 98   0  0.00   7896
#&gt; 99   0  0.00   7539
#&gt; 100  0  0.00   7198
#&gt; 101  0  0.00   6872
#&gt; 102  0  0.00   6561
#&gt; 103  0  0.00   6264
#&gt; 104  0  0.00   5981
#&gt; 105  0  0.00   5711
#&gt; 106  0  0.00   5452
#&gt; 107  0  0.00   5206
#&gt; 108  0  0.00   4970
#&gt; 109  0  0.00   4746
#&gt; 110  0  0.00   4531
#&gt; 111  0  0.00   4326
#&gt; 112  0  0.00   4130
#&gt; 113  0  0.00   3943
#&gt; 114  0  0.00   3765
#&gt; 115  0  0.00   3595
#&gt; 116  0  0.00   3432
#&gt; 117  0  0.00   3277
#&gt; 118  0  0.00   3129
#&gt; 119  0  0.00   2987
#&gt; 120  0  0.00   2852
#&gt; 121  0  0.00   2723
#&gt; 122  0  0.00   2600
#&gt; 123  0  0.00   2482
#&gt; 124  0  0.00   2370
#&gt; 125  0  0.00   2263
#&gt; 126  0  0.00   2161
#&gt; 127  0  0.00   2063
#&gt; 128  0  0.00   1970
#&gt; 129  0  0.00   1880
#&gt; 130  0  0.00   1795
#&gt; 131  0  0.00   1714
#&gt; 132  0  0.00   1637
#&gt; 133  0  0.00   1563
#&gt; 134  0  0.00   1492
#&gt; 135  0  0.00   1425
#&gt; 136  0  0.00   1360
#&gt; 137  0  0.00   1299
#&gt; 138  0  0.00   1240
#&gt; 139  0  0.00   1184
#&gt; 140  0  0.00   1130
#&gt; 141  0  0.00   1079
#&gt; 142  0  0.00   1030
#&gt; 143  0  0.00    984
#&gt; 144  0  0.00    939
#&gt; 145  0  0.00    897
#&gt; 146  0  0.00    856
#&gt; 147  0  0.00    818
#&gt; 148  0  0.00    780
#&gt; 149  0  0.00    745
#&gt; 150  0  0.00    712
#&gt; 151  0  0.00    679
#&gt; 152  0  0.00    649
#&gt; 153  0  0.00    619
#&gt; 154  0  0.00    591
#&gt; 155  0  0.00    564
#&gt; 156  0  0.00    539
#&gt; 157  0  0.00    515
#&gt; 158  0  0.00    491
#&gt; 159  0  0.00    469
#&gt; 160  0  0.00    448
#&gt; 161  1  0.05    428
#&gt; 162  1  0.22    408
#&gt; 163  1  0.38    390
#&gt; 164  1  0.54    372
#&gt; 165  1  0.69    355
#&gt; 166  1  0.83    339
#&gt; 167  1  0.97    324
#&gt; 168  1  1.10    309
#&gt; 169  1  1.23    295
#&gt; 170  1  1.35    282
#&gt; 171  1  1.47    269
#&gt; 172  1  1.58    257
#&gt; 173  1  1.69    245
#&gt; 174  1  1.80    234
#&gt; 175  1  1.90    224
#&gt; 176  1  1.99    214
#&gt; 177  1  2.09    204
#&gt; 178  1  2.18    195
#&gt; 179  1  2.26    186
#&gt; 180  1  2.35    178
#&gt; 181  1  2.43    170
#&gt; 182  1  2.50    162
#&gt; 183  1  2.58    154
#&gt; 184  1  2.65    148
#&gt; 185  1  2.72    141
#&gt; 186  1  2.78    134
#&gt; 187  2  3.06    128
#&gt; 188  2  3.50    123
#&gt; 189  2  3.92    117
#&gt; 190  3  4.51    112
#&gt; 191  3  5.57    107
#&gt; 192  3  6.55    102
#&gt; 193  3  7.47     97
#&gt; 194  2  8.17     93
#&gt; 195  3  9.37     89
#&gt; 196  2 10.42     85
#&gt; 197  2 11.12     81
#&gt; 198  2 11.78     77
#&gt; 199  2 12.40     74
#&gt; 200  2 13.00     70
#&gt; 
#&gt; $`-0.14`
#&gt; 
#&gt; Call:  glmnet::glmnet(x = mm, y = as.factor(p), family = "binomial",      weights = weights, lambda = 10^(seq(4, 0, length.out = 200)) *          sum(reg)/length(reg) * sum(p)/sum(weights), standardize = F,      penalty.factor = reg) 
#&gt; 
#&gt;     Df %Dev Lambda
#&gt; 1    0    0 702100
#&gt; 2    0    0 670400
#&gt; 3    0    0 640100
#&gt; 4    0    0 611100
#&gt; 5    0    0 583500
#&gt; 6    0    0 557100
#&gt; 7    0    0 531900
#&gt; 8    0    0 507800
#&gt; 9    0    0 484900
#&gt; 10   0    0 462900
#&gt; 11   0    0 442000
#&gt; 12   0    0 422000
#&gt; 13   0    0 402900
#&gt; 14   0    0 384700
#&gt; 15   0    0 367300
#&gt; 16   0    0 350700
#&gt; 17   0    0 334800
#&gt; 18   0    0 319700
#&gt; 19   0    0 305200
#&gt; 20   0    0 291400
#&gt; 21   0    0 278200
#&gt; 22   0    0 265600
#&gt; 23   0    0 253600
#&gt; 24   0    0 242200
#&gt; 25   0    0 231200
#&gt; 26   0    0 220800
#&gt; 27   0    0 210800
#&gt; 28   0    0 201200
#&gt; 29   0    0 192100
#&gt; 30   0    0 183400
#&gt; 31   0    0 175100
#&gt; 32   0    0 167200
#&gt; 33   0    0 159700
#&gt; 34   0    0 152400
#&gt; 35   0    0 145500
#&gt; 36   0    0 139000
#&gt; 37   0    0 132700
#&gt; 38   0    0 126700
#&gt; 39   0    0 120900
#&gt; 40   0    0 115500
#&gt; 41   0    0 110300
#&gt; 42   0    0 105300
#&gt; 43   0    0 100500
#&gt; 44   0    0  95960
#&gt; 45   0    0  91620
#&gt; 46   0    0  87480
#&gt; 47   0    0  83520
#&gt; 48   0    0  79740
#&gt; 49   0    0  76140
#&gt; 50   0    0  72690
#&gt; 51   0    0  69410
#&gt; 52   0    0  66270
#&gt; 53   0    0  63270
#&gt; 54   0    0  60410
#&gt; 55   0    0  57680
#&gt; 56   0    0  55070
#&gt; 57   0    0  52580
#&gt; 58   0    0  50200
#&gt; 59   0    0  47930
#&gt; 60   0    0  45760
#&gt; 61   0    0  43690
#&gt; 62   0    0  41710
#&gt; 63   0    0  39830
#&gt; 64   0    0  38030
#&gt; 65   0    0  36310
#&gt; 66   0    0  34660
#&gt; 67   0    0  33100
#&gt; 68   0    0  31600
#&gt; 69   0    0  30170
#&gt; 70   0    0  28810
#&gt; 71   0    0  27500
#&gt; 72   0    0  26260
#&gt; 73   0    0  25070
#&gt; 74   0    0  23940
#&gt; 75   0    0  22860
#&gt; 76   0    0  21820
#&gt; 77   0    0  20830
#&gt; 78   0    0  19890
#&gt; 79   0    0  18990
#&gt; 80   0    0  18130
#&gt; 81   0    0  17310
#&gt; 82   0    0  16530
#&gt; 83   0    0  15780
#&gt; 84   0    0  15070
#&gt; 85   0    0  14390
#&gt; 86   0    0  13740
#&gt; 87   0    0  13120
#&gt; 88   0    0  12520
#&gt; 89   0    0  11960
#&gt; 90   0    0  11410
#&gt; 91   0    0  10900
#&gt; 92   0    0  10410
#&gt; 93   0    0   9935
#&gt; 94   0    0   9486
#&gt; 95   0    0   9057
#&gt; 96   0    0   8647
#&gt; 97   0    0   8256
#&gt; 98   0    0   7883
#&gt; 99   0    0   7526
#&gt; 100  0    0   7186
#&gt; 101  0    0   6861
#&gt; 102  0    0   6550
#&gt; 103  0    0   6254
#&gt; 104  0    0   5971
#&gt; 105  0    0   5701
#&gt; 106  0    0   5443
#&gt; 107  0    0   5197
#&gt; 108  0    0   4962
#&gt; 109  0    0   4738
#&gt; 110  0    0   4523
#&gt; 111  0    0   4319
#&gt; 112  0    0   4123
#&gt; 113  0    0   3937
#&gt; 114  0    0   3759
#&gt; 115  0    0   3589
#&gt; 116  0    0   3427
#&gt; 117  0    0   3272
#&gt; 118  0    0   3124
#&gt; 119  0    0   2982
#&gt; 120  0    0   2847
#&gt; 121  0    0   2719
#&gt; 122  0    0   2596
#&gt; 123  0    0   2478
#&gt; 124  0    0   2366
#&gt; 125  0    0   2259
#&gt; 126  0    0   2157
#&gt; 127  0    0   2059
#&gt; 128  0    0   1966
#&gt; 129  0    0   1877
#&gt; 130  0    0   1792
#&gt; 131  0    0   1711
#&gt; 132  0    0   1634
#&gt; 133  0    0   1560
#&gt; 134  0    0   1490
#&gt; 135  0    0   1422
#&gt; 136  0    0   1358
#&gt; 137  0    0   1296
#&gt; 138  0    0   1238
#&gt; 139  0    0   1182
#&gt; 140  0    0   1128
#&gt; 141  0    0   1077
#&gt; 142  0    0   1029
#&gt; 143  0    0    982
#&gt; 144  0    0    938
#&gt; 145  0    0    895
#&gt; 146  0    0    855
#&gt; 147  0    0    816
#&gt; 148  0    0    779
#&gt; 149  0    0    744
#&gt; 150  0    0    710
#&gt; 151  0    0    678
#&gt; 152  0    0    648
#&gt; 153  0    0    618
#&gt; 154  0    0    590
#&gt; 155  0    0    564
#&gt; 156  0    0    538
#&gt; 157  0    0    514
#&gt; 158  0    0    490
#&gt; 159  0    0    468
#&gt; 160  0    0    447
#&gt; 161  0    0    427
#&gt; 162  0    0    408
#&gt; 163  0    0    389
#&gt; 164  0    0    372
#&gt; 165  0    0    355
#&gt; 166  0    0    339
#&gt; 167  0    0    323
#&gt; 168  0    0    309
#&gt; 169  0    0    295
#&gt; 170  0    0    282
#&gt; 171  0    0    269
#&gt; 172  0    0    257
#&gt; 173  0    0    245
#&gt; 174  0    0    234
#&gt; 175  0    0    223
#&gt; 176  0    0    213
#&gt; 177  0    0    204
#&gt; 178  0    0    194
#&gt; 179  0    0    186
#&gt; 180  0    0    177
#&gt; 181  0    0    169
#&gt; 182  0    0    162
#&gt; 183  0    0    154
#&gt; 184  0    0    147
#&gt; 185  0    0    141
#&gt; 186  0    0    134
#&gt; 187  0    0    128
#&gt; 188  0    0    122
#&gt; 189  0    0    117
#&gt; 190  0    0    112
#&gt; 191  0    0    106
#&gt; 192  0    0    102
#&gt; 193  0    0     97
#&gt; 194  0    0     93
#&gt; 195  0    0     88
#&gt; 196  0    0     84
#&gt; 197  0    0     81
#&gt; 198  0    0     77
#&gt; 199  0    0     74
#&gt; 200  0    0     70
#&gt; 
#&gt; $`0.1`
#&gt; 
#&gt; Call:  glmnet::glmnet(x = mm, y = as.factor(p), family = "binomial",      weights = weights, lambda = 10^(seq(4, 0, length.out = 200)) *          sum(reg)/length(reg) * sum(p)/sum(weights), standardize = F,      penalty.factor = reg) 
#&gt; 
#&gt;     Df %Dev Lambda
#&gt; 1    0 0.00 702000
#&gt; 2    0 0.00 670300
#&gt; 3    0 0.00 640000
#&gt; 4    0 0.00 611000
#&gt; 5    0 0.00 583400
#&gt; 6    0 0.00 557000
#&gt; 7    0 0.00 531800
#&gt; 8    0 0.00 507800
#&gt; 9    0 0.00 484800
#&gt; 10   0 0.00 462900
#&gt; 11   0 0.00 441900
#&gt; 12   0 0.00 421900
#&gt; 13   0 0.00 402900
#&gt; 14   0 0.00 384600
#&gt; 15   0 0.00 367200
#&gt; 16   0 0.00 350600
#&gt; 17   0 0.00 334800
#&gt; 18   0 0.00 319600
#&gt; 19   0 0.00 305200
#&gt; 20   0 0.00 291400
#&gt; 21   0 0.00 278200
#&gt; 22   0 0.00 265600
#&gt; 23   0 0.00 253600
#&gt; 24   0 0.00 242100
#&gt; 25   0 0.00 231200
#&gt; 26   0 0.00 220700
#&gt; 27   0 0.00 210700
#&gt; 28   0 0.00 201200
#&gt; 29   0 0.00 192100
#&gt; 30   0 0.00 183400
#&gt; 31   0 0.00 175100
#&gt; 32   0 0.00 167200
#&gt; 33   0 0.00 159600
#&gt; 34   0 0.00 152400
#&gt; 35   0 0.00 145500
#&gt; 36   0 0.00 138900
#&gt; 37   0 0.00 132700
#&gt; 38   0 0.00 126700
#&gt; 39   0 0.00 120900
#&gt; 40   0 0.00 115500
#&gt; 41   0 0.00 110200
#&gt; 42   0 0.00 105300
#&gt; 43   0 0.00 100500
#&gt; 44   0 0.00  95950
#&gt; 45   0 0.00  91610
#&gt; 46   0 0.00  87470
#&gt; 47   0 0.00  83510
#&gt; 48   0 0.00  79730
#&gt; 49   0 0.00  76130
#&gt; 50   0 0.00  72680
#&gt; 51   0 0.00  69400
#&gt; 52   0 0.00  66260
#&gt; 53   0 0.00  63260
#&gt; 54   0 0.00  60400
#&gt; 55   0 0.00  57670
#&gt; 56   0 0.00  55060
#&gt; 57   0 0.00  52570
#&gt; 58   0 0.00  50190
#&gt; 59   0 0.00  47920
#&gt; 60   0 0.00  45750
#&gt; 61   0 0.00  43680
#&gt; 62   0 0.00  41710
#&gt; 63   0 0.00  39820
#&gt; 64   0 0.00  38020
#&gt; 65   0 0.00  36300
#&gt; 66   0 0.00  34660
#&gt; 67   0 0.00  33090
#&gt; 68   0 0.00  31600
#&gt; 69   0 0.00  30170
#&gt; 70   0 0.00  28800
#&gt; 71   0 0.00  27500
#&gt; 72   0 0.00  26260
#&gt; 73   0 0.00  25070
#&gt; 74   0 0.00  23930
#&gt; 75   0 0.00  22850
#&gt; 76   0 0.00  21820
#&gt; 77   0 0.00  20830
#&gt; 78   0 0.00  19890
#&gt; 79   0 0.00  18990
#&gt; 80   0 0.00  18130
#&gt; 81   0 0.00  17310
#&gt; 82   0 0.00  16530
#&gt; 83   0 0.00  15780
#&gt; 84   0 0.00  15070
#&gt; 85   0 0.00  14390
#&gt; 86   0 0.00  13730
#&gt; 87   0 0.00  13110
#&gt; 88   0 0.00  12520
#&gt; 89   0 0.00  11950
#&gt; 90   0 0.00  11410
#&gt; 91   0 0.00  10900
#&gt; 92   0 0.00  10400
#&gt; 93   0 0.00   9934
#&gt; 94   0 0.00   9484
#&gt; 95   0 0.00   9056
#&gt; 96   0 0.00   8646
#&gt; 97   0 0.00   8255
#&gt; 98   0 0.00   7882
#&gt; 99   0 0.00   7525
#&gt; 100  0 0.00   7185
#&gt; 101  0 0.00   6860
#&gt; 102  0 0.00   6550
#&gt; 103  0 0.00   6253
#&gt; 104  0 0.00   5970
#&gt; 105  0 0.00   5700
#&gt; 106  0 0.00   5443
#&gt; 107  0 0.00   5196
#&gt; 108  0 0.00   4961
#&gt; 109  0 0.00   4737
#&gt; 110  0 0.00   4523
#&gt; 111  0 0.00   4318
#&gt; 112  0 0.00   4123
#&gt; 113  0 0.00   3936
#&gt; 114  0 0.00   3758
#&gt; 115  0 0.00   3588
#&gt; 116  0 0.00   3426
#&gt; 117  0 0.00   3271
#&gt; 118  0 0.00   3123
#&gt; 119  0 0.00   2982
#&gt; 120  0 0.00   2847
#&gt; 121  0 0.00   2718
#&gt; 122  0 0.00   2595
#&gt; 123  0 0.00   2478
#&gt; 124  0 0.00   2366
#&gt; 125  0 0.00   2259
#&gt; 126  0 0.00   2157
#&gt; 127  0 0.00   2059
#&gt; 128  0 0.00   1966
#&gt; 129  0 0.00   1877
#&gt; 130  0 0.00   1792
#&gt; 131  0 0.00   1711
#&gt; 132  0 0.00   1634
#&gt; 133  0 0.00   1560
#&gt; 134  0 0.00   1489
#&gt; 135  0 0.00   1422
#&gt; 136  0 0.00   1358
#&gt; 137  0 0.00   1296
#&gt; 138  0 0.00   1238
#&gt; 139  0 0.00   1182
#&gt; 140  0 0.00   1128
#&gt; 141  0 0.00   1077
#&gt; 142  0 0.00   1028
#&gt; 143  0 0.00    982
#&gt; 144  0 0.00    938
#&gt; 145  0 0.00    895
#&gt; 146  0 0.00    855
#&gt; 147  0 0.00    816
#&gt; 148  0 0.00    779
#&gt; 149  0 0.00    744
#&gt; 150  0 0.00    710
#&gt; 151  0 0.00    678
#&gt; 152  0 0.00    647
#&gt; 153  0 0.00    618
#&gt; 154  0 0.00    590
#&gt; 155  0 0.00    564
#&gt; 156  0 0.00    538
#&gt; 157  0 0.00    514
#&gt; 158  0 0.00    490
#&gt; 159  0 0.00    468
#&gt; 160  0 0.00    447
#&gt; 161  0 0.00    427
#&gt; 162  0 0.00    408
#&gt; 163  0 0.00    389
#&gt; 164  0 0.00    372
#&gt; 165  0 0.00    355
#&gt; 166  0 0.00    339
#&gt; 167  0 0.00    323
#&gt; 168  0 0.00    309
#&gt; 169  0 0.00    295
#&gt; 170  0 0.00    281
#&gt; 171  0 0.00    269
#&gt; 172  0 0.00    257
#&gt; 173  0 0.00    245
#&gt; 174  0 0.00    234
#&gt; 175  1 0.12    223
#&gt; 176  1 0.34    213
#&gt; 177  1 0.54    204
#&gt; 178  1 0.73    194
#&gt; 179  1 0.91    186
#&gt; 180  1 1.07    177
#&gt; 181  1 1.22    169
#&gt; 182  1 1.37    162
#&gt; 183  1 1.50    154
#&gt; 184  1 1.62    147
#&gt; 185  1 1.74    141
#&gt; 186  1 1.85    134
#&gt; 187  1 1.94    128
#&gt; 188  1 2.04    122
#&gt; 189  1 2.12    117
#&gt; 190  2 2.51    112
#&gt; 191  2 2.92    106
#&gt; 192  2 3.29    102
#&gt; 193  2 3.63     97
#&gt; 194  2 3.94     93
#&gt; 195  2 4.22     88
#&gt; 196  2 4.48     84
#&gt; 197  2 4.71     81
#&gt; 198  2 4.93     77
#&gt; 199  2 5.12     74
#&gt; 200  2 5.30     70
#&gt; 
#&gt; $`0`
#&gt; 
#&gt; Call:  glmnet::glmnet(x = mm, y = as.factor(p), family = "binomial",      weights = weights, lambda = 10^(seq(4, 0, length.out = 200)) *          sum(reg)/length(reg) * sum(p)/sum(weights), standardize = F,      penalty.factor = reg) 
#&gt; 
#&gt;     Df  %Dev Lambda
#&gt; 1    0  0.00 796800
#&gt; 2    0  0.00 760800
#&gt; 3    0  0.00 726400
#&gt; 4    0  0.00 693500
#&gt; 5    0  0.00 662200
#&gt; 6    0  0.00 632200
#&gt; 7    0  0.00 603600
#&gt; 8    0  0.00 576300
#&gt; 9    0  0.00 550300
#&gt; 10   0  0.00 525400
#&gt; 11   0  0.00 501600
#&gt; 12   0  0.00 478900
#&gt; 13   0  0.00 457300
#&gt; 14   0  0.00 436600
#&gt; 15   0  0.00 416800
#&gt; 16   0  0.00 398000
#&gt; 17   0  0.00 380000
#&gt; 18   0  0.00 362800
#&gt; 19   0  0.00 346400
#&gt; 20   0  0.00 330700
#&gt; 21   0  0.00 315800
#&gt; 22   0  0.00 301500
#&gt; 23   0  0.00 287800
#&gt; 24   0  0.00 274800
#&gt; 25   0  0.00 262400
#&gt; 26   0  0.00 250500
#&gt; 27   0  0.00 239200
#&gt; 28   0  0.00 228400
#&gt; 29   0  0.00 218100
#&gt; 30   0  0.00 208200
#&gt; 31   0  0.00 198800
#&gt; 32   0  0.00 189800
#&gt; 33   0  0.00 181200
#&gt; 34   0  0.00 173000
#&gt; 35   0  0.00 165200
#&gt; 36   0  0.00 157700
#&gt; 37   0  0.00 150600
#&gt; 38   0  0.00 143800
#&gt; 39   0  0.00 137300
#&gt; 40   0  0.00 131100
#&gt; 41   0  0.00 125100
#&gt; 42   0  0.00 119500
#&gt; 43   0  0.00 114100
#&gt; 44   0  0.00 108900
#&gt; 45   0  0.00 104000
#&gt; 46   0  0.00  99280
#&gt; 47   0  0.00  94790
#&gt; 48   0  0.00  90500
#&gt; 49   0  0.00  86410
#&gt; 50   0  0.00  82500
#&gt; 51   0  0.00  78770
#&gt; 52   0  0.00  75200
#&gt; 53   0  0.00  71800
#&gt; 54   0  0.00  68560
#&gt; 55   0  0.00  65460
#&gt; 56   0  0.00  62490
#&gt; 57   0  0.00  59670
#&gt; 58   0  0.00  56970
#&gt; 59   0  0.00  54390
#&gt; 60   0  0.00  51930
#&gt; 61   0  0.00  49580
#&gt; 62   0  0.00  47340
#&gt; 63   0  0.00  45200
#&gt; 64   0  0.00  43160
#&gt; 65   0  0.00  41200
#&gt; 66   0  0.00  39340
#&gt; 67   0  0.00  37560
#&gt; 68   0  0.00  35860
#&gt; 69   0  0.00  34240
#&gt; 70   0  0.00  32690
#&gt; 71   0  0.00  31210
#&gt; 72   0  0.00  29800
#&gt; 73   0  0.00  28450
#&gt; 74   0  0.00  27170
#&gt; 75   0  0.00  25940
#&gt; 76   0  0.00  24760
#&gt; 77   0  0.00  23640
#&gt; 78   0  0.00  22580
#&gt; 79   0  0.00  21550
#&gt; 80   0  0.00  20580
#&gt; 81   0  0.00  19650
#&gt; 82   0  0.00  18760
#&gt; 83   0  0.00  17910
#&gt; 84   0  0.00  17100
#&gt; 85   0  0.00  16330
#&gt; 86   0  0.00  15590
#&gt; 87   0  0.00  14880
#&gt; 88   0  0.00  14210
#&gt; 89   0  0.00  13570
#&gt; 90   0  0.00  12950
#&gt; 91   0  0.00  12370
#&gt; 92   0  0.00  11810
#&gt; 93   0  0.00  11280
#&gt; 94   0  0.00  10770
#&gt; 95   0  0.00  10280
#&gt; 96   0  0.00   9813
#&gt; 97   0  0.00   9370
#&gt; 98   0  0.00   8946
#&gt; 99   0  0.00   8541
#&gt; 100  0  0.00   8155
#&gt; 101  0  0.00   7786
#&gt; 102  0  0.00   7434
#&gt; 103  0  0.00   7098
#&gt; 104  0  0.00   6777
#&gt; 105  0  0.00   6470
#&gt; 106  0  0.00   6178
#&gt; 107  0  0.00   5898
#&gt; 108  0  0.00   5631
#&gt; 109  0  0.00   5377
#&gt; 110  0  0.00   5134
#&gt; 111  0  0.00   4901
#&gt; 112  0  0.00   4680
#&gt; 113  0  0.00   4468
#&gt; 114  0  0.00   4266
#&gt; 115  0  0.00   4073
#&gt; 116  0  0.00   3889
#&gt; 117  0  0.00   3713
#&gt; 118  0  0.00   3545
#&gt; 119  0  0.00   3385
#&gt; 120  0  0.00   3232
#&gt; 121  0  0.00   3085
#&gt; 122  0  0.00   2946
#&gt; 123  0  0.00   2813
#&gt; 124  0  0.00   2685
#&gt; 125  0  0.00   2564
#&gt; 126  0  0.00   2448
#&gt; 127  0  0.00   2337
#&gt; 128  0  0.00   2232
#&gt; 129  0  0.00   2131
#&gt; 130  0  0.00   2034
#&gt; 131  0  0.00   1942
#&gt; 132  0  0.00   1854
#&gt; 133  0  0.00   1771
#&gt; 134  0  0.00   1690
#&gt; 135  0  0.00   1614
#&gt; 136  0  0.00   1541
#&gt; 137  0  0.00   1471
#&gt; 138  0  0.00   1405
#&gt; 139  0  0.00   1341
#&gt; 140  0  0.00   1281
#&gt; 141  0  0.00   1223
#&gt; 142  0  0.00   1167
#&gt; 143  0  0.00   1115
#&gt; 144  0  0.00   1064
#&gt; 145  0  0.00   1016
#&gt; 146  0  0.00    970
#&gt; 147  0  0.00    926
#&gt; 148  0  0.00    884
#&gt; 149  0  0.00    844
#&gt; 150  0  0.00    806
#&gt; 151  0  0.00    770
#&gt; 152  0  0.00    735
#&gt; 153  0  0.00    702
#&gt; 154  0  0.00    670
#&gt; 155  0  0.00    640
#&gt; 156  0  0.00    611
#&gt; 157  0  0.00    583
#&gt; 158  0  0.00    557
#&gt; 159  0  0.00    532
#&gt; 160  0  0.00    507
#&gt; 161  0  0.00    484
#&gt; 162  0  0.00    463
#&gt; 163  0  0.00    442
#&gt; 164  0  0.00    422
#&gt; 165  0  0.00    403
#&gt; 166  0  0.00    384
#&gt; 167  0  0.00    367
#&gt; 168  0  0.00    350
#&gt; 169  1  2.01    335
#&gt; 170  1  4.27    319
#&gt; 171  1  6.12    305
#&gt; 172  1  7.65    291
#&gt; 173  1  8.95    278
#&gt; 174  1 10.07    266
#&gt; 175  1 11.04    253
#&gt; 176  1 11.88    242
#&gt; 177  1 12.63    231
#&gt; 178  1 13.28    221
#&gt; 179  1 13.87    211
#&gt; 180  1 14.39    201
#&gt; 181  1 14.86    192
#&gt; 182  1 15.28    183
#&gt; 183  1 15.66    175
#&gt; 184  1 16.00    167
#&gt; 185  1 16.31    160
#&gt; 186  1 16.59    152
#&gt; 187  1 16.85    145
#&gt; 188  1 17.08    139
#&gt; 189  1 17.29    133
#&gt; 190  1 17.48    127
#&gt; 191  1 17.66    121
#&gt; 192  1 17.82    115
#&gt; 193  1 17.97    110
#&gt; 194  1 18.10    105
#&gt; 195  1 18.22    100
#&gt; 196  2 18.51     96
#&gt; 197  2 19.03     92
#&gt; 198  2 19.52     87
#&gt; 199  2 19.97     83
#&gt; 200  2 20.39     80
#&gt; 
#&gt; $`0.84`
#&gt; 
#&gt; Call:  glmnet::glmnet(x = mm, y = as.factor(p), family = "binomial",      weights = weights, lambda = 10^(seq(4, 0, length.out = 200)) *          sum(reg)/length(reg) * sum(p)/sum(weights), standardize = F,      penalty.factor = reg) 
#&gt; 
#&gt;     Df  %Dev Lambda
#&gt; 1    0  0.00 288600
#&gt; 2    0  0.00 275600
#&gt; 3    0  0.00 263100
#&gt; 4    0  0.00 251200
#&gt; 5    0  0.00 239900
#&gt; 6    0  0.00 229000
#&gt; 7    0  0.00 218700
#&gt; 8    0  0.00 208800
#&gt; 9    0  0.00 199300
#&gt; 10   0  0.00 190300
#&gt; 11   0  0.00 181700
#&gt; 12   0  0.00 173500
#&gt; 13   0  0.00 165600
#&gt; 14   0  0.00 158100
#&gt; 15   0  0.00 151000
#&gt; 16   0  0.00 144200
#&gt; 17   0  0.00 137600
#&gt; 18   0  0.00 131400
#&gt; 19   0  0.00 125500
#&gt; 20   0  0.00 119800
#&gt; 21   0  0.00 114400
#&gt; 22   0  0.00 109200
#&gt; 23   0  0.00 104300
#&gt; 24   0  0.00  99550
#&gt; 25   0  0.00  95050
#&gt; 26   0  0.00  90750
#&gt; 27   0  0.00  86640
#&gt; 28   0  0.00  82730
#&gt; 29   0  0.00  78980
#&gt; 30   0  0.00  75410
#&gt; 31   0  0.00  72000
#&gt; 32   0  0.00  68740
#&gt; 33   0  0.00  65640
#&gt; 34   0  0.00  62670
#&gt; 35   0  0.00  59830
#&gt; 36   0  0.00  57130
#&gt; 37   0  0.00  54540
#&gt; 38   0  0.00  52080
#&gt; 39   0  0.00  49720
#&gt; 40   0  0.00  47470
#&gt; 41   0  0.00  45320
#&gt; 42   0  0.00  43270
#&gt; 43   0  0.00  41320
#&gt; 44   0  0.00  39450
#&gt; 45   0  0.00  37660
#&gt; 46   0  0.00  35960
#&gt; 47   0  0.00  34330
#&gt; 48   0  0.00  32780
#&gt; 49   0  0.00  31300
#&gt; 50   0  0.00  29880
#&gt; 51   0  0.00  28530
#&gt; 52   0  0.00  27240
#&gt; 53   0  0.00  26010
#&gt; 54   0  0.00  24830
#&gt; 55   0  0.00  23710
#&gt; 56   0  0.00  22640
#&gt; 57   0  0.00  21610
#&gt; 58   0  0.00  20640
#&gt; 59   0  0.00  19700
#&gt; 60   0  0.00  18810
#&gt; 61   0  0.00  17960
#&gt; 62   0  0.00  17150
#&gt; 63   0  0.00  16370
#&gt; 64   0  0.00  15630
#&gt; 65   0  0.00  14930
#&gt; 66   0  0.00  14250
#&gt; 67   0  0.00  13610
#&gt; 68   0  0.00  12990
#&gt; 69   0  0.00  12400
#&gt; 70   0  0.00  11840
#&gt; 71   0  0.00  11310
#&gt; 72   0  0.00  10790
#&gt; 73   0  0.00  10310
#&gt; 74   0  0.00   9841
#&gt; 75   0  0.00   9395
#&gt; 76   0  0.00   8971
#&gt; 77   0  0.00   8565
#&gt; 78   0  0.00   8177
#&gt; 79   0  0.00   7808
#&gt; 80   0  0.00   7454
#&gt; 81   0  0.00   7117
#&gt; 82   0  0.00   6795
#&gt; 83   0  0.00   6488
#&gt; 84   0  0.00   6195
#&gt; 85   0  0.00   5914
#&gt; 86   0  0.00   5647
#&gt; 87   0  0.00   5392
#&gt; 88   0  0.00   5148
#&gt; 89   0  0.00   4915
#&gt; 90   0  0.00   4693
#&gt; 91   0  0.00   4480
#&gt; 92   0  0.00   4278
#&gt; 93   0  0.00   4084
#&gt; 94   0  0.00   3899
#&gt; 95   0  0.00   3723
#&gt; 96   0  0.00   3555
#&gt; 97   0  0.00   3394
#&gt; 98   0  0.00   3240
#&gt; 99   0  0.00   3094
#&gt; 100  0  0.00   2954
#&gt; 101  0  0.00   2820
#&gt; 102  0  0.00   2693
#&gt; 103  0  0.00   2571
#&gt; 104  0  0.00   2455
#&gt; 105  0  0.00   2344
#&gt; 106  0  0.00   2238
#&gt; 107  0  0.00   2136
#&gt; 108  0  0.00   2040
#&gt; 109  0  0.00   1948
#&gt; 110  0  0.00   1860
#&gt; 111  0  0.00   1775
#&gt; 112  0  0.00   1695
#&gt; 113  0  0.00   1618
#&gt; 114  0  0.00   1545
#&gt; 115  0  0.00   1475
#&gt; 116  0  0.00   1409
#&gt; 117  0  0.00   1345
#&gt; 118  0  0.00   1284
#&gt; 119  0  0.00   1226
#&gt; 120  0  0.00   1171
#&gt; 121  0  0.00   1118
#&gt; 122  0  0.00   1067
#&gt; 123  0  0.00   1019
#&gt; 124  0  0.00    973
#&gt; 125  1  0.81    929
#&gt; 126  1  2.82    887
#&gt; 127  1  4.73    847
#&gt; 128  1  6.55    808
#&gt; 129  1  8.30    772
#&gt; 130  1  9.98    737
#&gt; 131  1 11.61    704
#&gt; 132  1 13.20    672
#&gt; 133  1 14.74    641
#&gt; 134  1 16.25    612
#&gt; 135  1 17.74    585
#&gt; 136  1 19.21    558
#&gt; 137  1 20.65    533
#&gt; 138  1 22.08    509
#&gt; 139  1 23.50    486
#&gt; 140  1 24.91    464
#&gt; 141  1 26.31    443
#&gt; 142  1 27.70    423
#&gt; 143  1 29.08    404
#&gt; 144  1 30.46    386
#&gt; 145  1 31.83    368
#&gt; 146  1 33.19    351
#&gt; 147  1 34.54    336
#&gt; 148  1 35.89    320
#&gt; 149  1 37.22    306
#&gt; 150  1 38.54    292
#&gt; 151  1 39.86    279
#&gt; 152  1 41.16    266
#&gt; 153  1 42.47    254
#&gt; 154  1 43.75    243
#&gt; 155  1 45.04    232
#&gt; 156  1 46.31    221
#&gt; 157  1 47.57    211
#&gt; 158  1 48.82    202
#&gt; 159  1 50.06    192
#&gt; 160  1 51.28    184
#&gt; 161  1 52.48    176
#&gt; 162  1 53.65    168
#&gt; 163  1 54.80    160
#&gt; 164  1 55.91    153
#&gt; 165  1 56.99    146
#&gt; 166  1 58.03    139
#&gt; 167  1 59.04    133
#&gt; 168  1 60.02    127
#&gt; 169  1 60.95    121
#&gt; 170  1 61.85    116
#&gt; 171  1 62.72    110
#&gt; 172  1 63.55    106
#&gt; 173  1 64.36    101
#&gt; 174  1 65.14     96
#&gt; 175  1 65.90     92
#&gt; 176  1 66.63     88
#&gt; 177  1 67.34     84
#&gt; 178  1 68.04     80
#&gt; 179  2 69.24     76
#&gt; 180  2 70.55     73
#&gt; 181  2 71.70     70
#&gt; 182  2 72.73     66
#&gt; 183  2 73.67     63
#&gt; 184  2 74.55     61
#&gt; 185  2 75.37     58
#&gt; 186  2 76.15     55
#&gt; 187  2 76.89     53
#&gt; 188  2 77.61     50
#&gt; 189  2 78.31     48
#&gt; 190  2 78.98     46
#&gt; 191  2 79.64     44
#&gt; 192  2 80.28     42
#&gt; 193  2 80.91     40
#&gt; 194  2 81.53     38
#&gt; 195  2 82.14     36
#&gt; 196  2 82.73     35
#&gt; 197  2 83.31     33
#&gt; 198  2 83.87     32
#&gt; 199  2 84.42     30
#&gt; 200  2 84.95     29
#&gt; 
#&gt; $`0.92`
#&gt; 
#&gt; Call:  glmnet::glmnet(x = mm, y = as.factor(p), family = "binomial",      weights = weights, lambda = 10^(seq(4, 0, length.out = 200)) *          sum(reg)/length(reg) * sum(p)/sum(weights), standardize = F,      penalty.factor = reg) 
#&gt; 
#&gt;     Df  %Dev  Lambda
#&gt; 1    0  0.00 1281.00
#&gt; 2    0  0.00 1224.00
#&gt; 3    0  0.00 1168.00
#&gt; 4    0  0.00 1115.00
#&gt; 5    0  0.00 1065.00
#&gt; 6    0  0.00 1017.00
#&gt; 7    0  0.00  970.70
#&gt; 8    0  0.00  926.80
#&gt; 9    0  0.00  884.90
#&gt; 10   0  0.00  844.90
#&gt; 11   0  0.00  806.70
#&gt; 12   0  0.00  770.20
#&gt; 13   0  0.00  735.40
#&gt; 14   0  0.00  702.10
#&gt; 15   0  0.00  670.40
#&gt; 16   0  0.00  640.00
#&gt; 17   0  0.00  611.10
#&gt; 18   0  0.00  583.40
#&gt; 19   0  0.00  557.10
#&gt; 20   0  0.00  531.90
#&gt; 21   0  0.00  507.80
#&gt; 22   0  0.00  484.80
#&gt; 23   0  0.00  462.90
#&gt; 24   0  0.00  442.00
#&gt; 25   0  0.00  422.00
#&gt; 26   0  0.00  402.90
#&gt; 27   0  0.00  384.70
#&gt; 28   0  0.00  367.30
#&gt; 29   0  0.00  350.70
#&gt; 30   0  0.00  334.80
#&gt; 31   0  0.00  319.70
#&gt; 32   0  0.00  305.20
#&gt; 33   0  0.00  291.40
#&gt; 34   0  0.00  278.20
#&gt; 35   0  0.00  265.60
#&gt; 36   0  0.00  253.60
#&gt; 37   0  0.00  242.20
#&gt; 38   0  0.00  231.20
#&gt; 39   0  0.00  220.70
#&gt; 40   0  0.00  210.80
#&gt; 41   0  0.00  201.20
#&gt; 42   0  0.00  192.10
#&gt; 43   0  0.00  183.40
#&gt; 44   0  0.00  175.10
#&gt; 45   0  0.00  167.20
#&gt; 46   0  0.00  159.70
#&gt; 47   0  0.00  152.40
#&gt; 48   0  0.00  145.50
#&gt; 49   0  0.00  139.00
#&gt; 50   0  0.00  132.70
#&gt; 51   0  0.00  126.70
#&gt; 52   0  0.00  120.90
#&gt; 53   0  0.00  115.50
#&gt; 54   0  0.00  110.30
#&gt; 55   0  0.00  105.30
#&gt; 56   0  0.00  100.50
#&gt; 57   0  0.00   95.96
#&gt; 58   0  0.00   91.62
#&gt; 59   0  0.00   87.47
#&gt; 60   0  0.00   83.52
#&gt; 61   0  0.00   79.74
#&gt; 62   0  0.00   76.13
#&gt; 63   0  0.00   72.69
#&gt; 64   0  0.00   69.40
#&gt; 65   0  0.00   66.26
#&gt; 66   0  0.00   63.27
#&gt; 67   0  0.00   60.41
#&gt; 68   0  0.00   57.67
#&gt; 69   0  0.00   55.06
#&gt; 70   0  0.00   52.57
#&gt; 71   0  0.00   50.20
#&gt; 72   0  0.00   47.93
#&gt; 73   0  0.00   45.76
#&gt; 74   0  0.00   43.69
#&gt; 75   0  0.00   41.71
#&gt; 76   0  0.00   39.83
#&gt; 77   0  0.00   38.03
#&gt; 78   0  0.00   36.31
#&gt; 79   0  0.00   34.66
#&gt; 80   0  0.00   33.10
#&gt; 81   0  0.00   31.60
#&gt; 82   0  0.00   30.17
#&gt; 83   0  0.00   28.81
#&gt; 84   0  0.00   27.50
#&gt; 85   0  0.00   26.26
#&gt; 86   0  0.00   25.07
#&gt; 87   0  0.00   23.94
#&gt; 88   0  0.00   22.85
#&gt; 89   0  0.00   21.82
#&gt; 90   0  0.00   20.83
#&gt; 91   0  0.00   19.89
#&gt; 92   0  0.00   18.99
#&gt; 93   0  0.00   18.13
#&gt; 94   0  0.00   17.31
#&gt; 95   0  0.00   16.53
#&gt; 96   0  0.00   15.78
#&gt; 97   0  0.00   15.07
#&gt; 98   0  0.00   14.39
#&gt; 99   0  0.00   13.74
#&gt; 100  0  0.00   13.11
#&gt; 101  0  0.00   12.52
#&gt; 102  0  0.00   11.96
#&gt; 103  0  0.00   11.41
#&gt; 104  0  0.00   10.90
#&gt; 105  0  0.00   10.41
#&gt; 106  0  0.00    9.94
#&gt; 107  0  0.00    9.48
#&gt; 108  0  0.00    9.06
#&gt; 109  0  0.00    8.65
#&gt; 110  0  0.00    8.26
#&gt; 111  0  0.00    7.88
#&gt; 112  0  0.00    7.53
#&gt; 113  0  0.00    7.18
#&gt; 114  0  0.00    6.86
#&gt; 115  0  0.00    6.55
#&gt; 116  0  0.00    6.25
#&gt; 117  0  0.00    5.97
#&gt; 118  0  0.00    5.70
#&gt; 119  0  0.00    5.44
#&gt; 120  0  0.00    5.20
#&gt; 121  0  0.00    4.96
#&gt; 122  0  0.00    4.74
#&gt; 123  0  0.00    4.52
#&gt; 124  0  0.00    4.32
#&gt; 125  1  0.81    4.12
#&gt; 126  1  2.82    3.94
#&gt; 127  1  4.73    3.76
#&gt; 128  1  6.55    3.59
#&gt; 129  1  8.30    3.43
#&gt; 130  1  9.98    3.27
#&gt; 131  1 11.61    3.12
#&gt; 132  1 13.20    2.98
#&gt; 133  1 14.74    2.85
#&gt; 134  1 16.25    2.72
#&gt; 135  1 17.74    2.60
#&gt; 136  1 19.21    2.48
#&gt; 137  1 20.65    2.37
#&gt; 138  1 22.08    2.26
#&gt; 139  1 23.50    2.16
#&gt; 140  1 24.91    2.06
#&gt; 141  1 26.31    1.97
#&gt; 142  1 27.70    1.88
#&gt; 143  1 29.08    1.79
#&gt; 144  1 30.46    1.71
#&gt; 145  1 31.83    1.63
#&gt; 146  1 33.19    1.56
#&gt; 147  1 34.54    1.49
#&gt; 148  1 35.89    1.42
#&gt; 149  1 37.22    1.36
#&gt; 150  1 38.54    1.30
#&gt; 151  1 39.86    1.24
#&gt; 152  1 41.16    1.18
#&gt; 153  1 42.47    1.13
#&gt; 154  1 43.75    1.08
#&gt; 155  1 45.04    1.03
#&gt; 156  1 46.31    0.98
#&gt; 157  1 47.57    0.94
#&gt; 158  1 48.82    0.90
#&gt; 159  1 50.06    0.85
#&gt; 160  1 51.28    0.82
#&gt; 161  1 52.48    0.78
#&gt; 162  1 53.65    0.74
#&gt; 163  1 54.80    0.71
#&gt; 164  1 55.91    0.68
#&gt; 165  1 56.99    0.65
#&gt; 166  1 58.03    0.62
#&gt; 167  1 59.04    0.59
#&gt; 168  1 60.02    0.56
#&gt; 169  1 60.95    0.54
#&gt; 170  1 61.85    0.51
#&gt; 171  1 62.72    0.49
#&gt; 172  1 63.55    0.47
#&gt; 173  1 64.36    0.45
#&gt; 174  1 65.14    0.43
#&gt; 175  1 65.90    0.41
#&gt; 176  1 66.63    0.39
#&gt; 177  1 67.34    0.37
#&gt; 178  1 68.04    0.35
#&gt; 179  2 69.24    0.34
#&gt; 180  2 70.55    0.32
#&gt; 181  2 71.70    0.31
#&gt; 182  2 72.73    0.29
#&gt; 183  2 73.67    0.28
#&gt; 184  2 74.55    0.27
#&gt; 185  2 75.37    0.26
#&gt; 186  2 76.15    0.24
#&gt; 187  2 76.89    0.23
#&gt; 188  2 77.61    0.22
#&gt; 189  2 78.31    0.21
#&gt; 190  2 78.98    0.20
#&gt; 191  2 79.64    0.19
#&gt; 192  2 80.28    0.19
#&gt; 193  2 80.91    0.18
#&gt; 194  2 81.53    0.17
#&gt; 195  2 82.14    0.16
#&gt; 196  2 82.73    0.15
#&gt; 197  2 83.31    0.15
#&gt; 198  2 83.87    0.14
#&gt; 199  2 84.42    0.13
#&gt; 200  2 84.95    0.13
#&gt; 
#&gt; $`0.88`
#&gt; 
#&gt; Call:  glmnet::glmnet(x = mm, y = as.factor(p), family = "binomial",      weights = weights, lambda = 10^(seq(4, 0, length.out = 200)) *          sum(reg)/length(reg) * sum(p)/sum(weights), standardize = F,      penalty.factor = reg) 
#&gt; 
#&gt;     Df  %Dev  Lambda
#&gt; 1    0  0.00 122.300
#&gt; 2    0  0.00 116.700
#&gt; 3    0  0.00 111.500
#&gt; 4    0  0.00 106.400
#&gt; 5    0  0.00 101.600
#&gt; 6    0  0.00  97.010
#&gt; 7    0  0.00  92.630
#&gt; 8    0  0.00  88.440
#&gt; 9    0  0.00  84.440
#&gt; 10   0  0.00  80.620
#&gt; 11   0  0.00  76.970
#&gt; 12   0  0.00  73.490
#&gt; 13   0  0.00  70.170
#&gt; 14   0  0.00  66.990
#&gt; 15   0  0.00  63.960
#&gt; 16   0  0.00  61.070
#&gt; 17   0  0.00  58.310
#&gt; 18   0  0.00  55.670
#&gt; 19   0  0.00  53.150
#&gt; 20   0  0.00  50.750
#&gt; 21   0  0.00  48.450
#&gt; 22   0  0.00  46.260
#&gt; 23   0  0.00  44.170
#&gt; 24   0  0.00  42.170
#&gt; 25   0  0.00  40.260
#&gt; 26   0  0.00  38.440
#&gt; 27   0  0.00  36.700
#&gt; 28   0  0.00  35.040
#&gt; 29   0  0.00  33.460
#&gt; 30   0  0.00  31.950
#&gt; 31   0  0.00  30.500
#&gt; 32   0  0.00  29.120
#&gt; 33   0  0.00  27.800
#&gt; 34   0  0.00  26.550
#&gt; 35   0  0.00  25.350
#&gt; 36   0  0.00  24.200
#&gt; 37   0  0.00  23.110
#&gt; 38   0  0.00  22.060
#&gt; 39   0  0.00  21.060
#&gt; 40   0  0.00  20.110
#&gt; 41   0  0.00  19.200
#&gt; 42   0  0.00  18.330
#&gt; 43   0  0.00  17.500
#&gt; 44   0  0.00  16.710
#&gt; 45   0  0.00  15.960
#&gt; 46   0  0.00  15.230
#&gt; 47   0  0.00  14.540
#&gt; 48   0  0.00  13.890
#&gt; 49   0  0.00  13.260
#&gt; 50   0  0.00  12.660
#&gt; 51   0  0.00  12.090
#&gt; 52   0  0.00  11.540
#&gt; 53   0  0.00  11.020
#&gt; 54   0  0.00  10.520
#&gt; 55   0  0.00  10.040
#&gt; 56   0  0.00   9.590
#&gt; 57   0  0.00   9.156
#&gt; 58   0  0.00   8.742
#&gt; 59   0  0.00   8.346
#&gt; 60   0  0.00   7.969
#&gt; 61   0  0.00   7.609
#&gt; 62   0  0.00   7.264
#&gt; 63   0  0.00   6.936
#&gt; 64   0  0.00   6.622
#&gt; 65   0  0.00   6.323
#&gt; 66   0  0.00   6.037
#&gt; 67   0  0.00   5.764
#&gt; 68   0  0.00   5.503
#&gt; 69   0  0.00   5.254
#&gt; 70   0  0.00   5.016
#&gt; 71   0  0.00   4.790
#&gt; 72   0  0.00   4.573
#&gt; 73   0  0.00   4.366
#&gt; 74   0  0.00   4.169
#&gt; 75   0  0.00   3.980
#&gt; 76   0  0.00   3.800
#&gt; 77   0  0.00   3.628
#&gt; 78   0  0.00   3.464
#&gt; 79   0  0.00   3.307
#&gt; 80   0  0.00   3.158
#&gt; 81   0  0.00   3.015
#&gt; 82   0  0.00   2.879
#&gt; 83   0  0.00   2.748
#&gt; 84   0  0.00   2.624
#&gt; 85   0  0.00   2.505
#&gt; 86   0  0.00   2.392
#&gt; 87   0  0.00   2.284
#&gt; 88   0  0.00   2.181
#&gt; 89   0  0.00   2.082
#&gt; 90   0  0.00   1.988
#&gt; 91   0  0.00   1.898
#&gt; 92   0  0.00   1.812
#&gt; 93   0  0.00   1.730
#&gt; 94   0  0.00   1.652
#&gt; 95   0  0.00   1.577
#&gt; 96   0  0.00   1.506
#&gt; 97   0  0.00   1.438
#&gt; 98   0  0.00   1.373
#&gt; 99   0  0.00   1.311
#&gt; 100  0  0.00   1.251
#&gt; 101  0  0.00   1.195
#&gt; 102  0  0.00   1.141
#&gt; 103  0  0.00   1.089
#&gt; 104  0  0.00   1.040
#&gt; 105  0  0.00   0.993
#&gt; 106  0  0.00   0.948
#&gt; 107  0  0.00   0.905
#&gt; 108  0  0.00   0.864
#&gt; 109  0  0.00   0.825
#&gt; 110  0  0.00   0.788
#&gt; 111  0  0.00   0.752
#&gt; 112  0  0.00   0.718
#&gt; 113  0  0.00   0.686
#&gt; 114  0  0.00   0.655
#&gt; 115  0  0.00   0.625
#&gt; 116  0  0.00   0.597
#&gt; 117  0  0.00   0.570
#&gt; 118  0  0.00   0.544
#&gt; 119  0  0.00   0.519
#&gt; 120  0  0.00   0.496
#&gt; 121  0  0.00   0.473
#&gt; 122  0  0.00   0.452
#&gt; 123  0  0.00   0.432
#&gt; 124  0  0.00   0.412
#&gt; 125  1  0.81   0.393
#&gt; 126  1  2.82   0.376
#&gt; 127  1  4.73   0.359
#&gt; 128  1  6.55   0.342
#&gt; 129  1  8.30   0.327
#&gt; 130  1  9.98   0.312
#&gt; 131  1 11.61   0.298
#&gt; 132  1 13.20   0.285
#&gt; 133  1 14.74   0.272
#&gt; 134  1 16.25   0.259
#&gt; 135  1 17.74   0.248
#&gt; 136  1 19.21   0.236
#&gt; 137  1 20.65   0.226
#&gt; 138  1 22.08   0.216
#&gt; 139  1 23.50   0.206
#&gt; 140  1 24.91   0.196
#&gt; 141  1 26.31   0.188
#&gt; 142  1 27.70   0.179
#&gt; 143  1 29.08   0.171
#&gt; 144  1 30.46   0.163
#&gt; 145  1 31.83   0.156
#&gt; 146  1 33.19   0.149
#&gt; 147  1 34.54   0.142
#&gt; 148  1 35.89   0.136
#&gt; 149  1 37.22   0.130
#&gt; 150  1 38.54   0.124
#&gt; 151  1 39.86   0.118
#&gt; 152  1 41.16   0.113
#&gt; 153  1 42.47   0.108
#&gt; 154  1 43.75   0.103
#&gt; 155  1 45.04   0.098
#&gt; 156  1 46.31   0.094
#&gt; 157  1 47.57   0.089
#&gt; 158  1 48.82   0.085
#&gt; 159  1 50.06   0.082
#&gt; 160  1 51.28   0.078
#&gt; 161  1 52.48   0.074
#&gt; 162  1 53.65   0.071
#&gt; 163  1 54.80   0.068
#&gt; 164  1 55.91   0.065
#&gt; 165  1 56.99   0.062
#&gt; 166  1 58.03   0.059
#&gt; 167  1 59.04   0.056
#&gt; 168  1 60.02   0.054
#&gt; 169  1 60.95   0.051
#&gt; 170  1 61.85   0.049
#&gt; 171  1 62.72   0.047
#&gt; 172  1 63.55   0.045
#&gt; 173  1 64.36   0.043
#&gt; 174  1 65.14   0.041
#&gt; 175  1 65.90   0.039
#&gt; 176  1 66.63   0.037
#&gt; 177  1 67.34   0.035
#&gt; 178  1 68.04   0.034
#&gt; 179  2 69.24   0.032
#&gt; 180  2 70.55   0.031
#&gt; 181  2 71.70   0.029
#&gt; 182  2 72.73   0.028
#&gt; 183  2 73.67   0.027
#&gt; 184  2 74.55   0.026
#&gt; 185  2 75.37   0.024
#&gt; 186  2 76.15   0.023
#&gt; 187  2 76.89   0.022
#&gt; 188  2 77.61   0.021
#&gt; 189  2 78.31   0.020
#&gt; 190  2 78.98   0.019
#&gt; 191  2 79.64   0.019
#&gt; 192  2 80.28   0.018
#&gt; 193  2 80.91   0.017
#&gt; 194  2 81.53   0.016
#&gt; 195  2 82.14   0.015
#&gt; 196  2 82.73   0.015
#&gt; 197  2 83.31   0.014
#&gt; 198  2 83.87   0.013
#&gt; 199  2 84.42   0.013
#&gt; 200  2 84.95   0.012
#&gt; 
#&gt; $`0.94`
#&gt; 
#&gt; Call:  glmnet::glmnet(x = mm, y = as.factor(p), family = "binomial",      weights = weights, lambda = 10^(seq(4, 0, length.out = 200)) *          sum(reg)/length(reg) * sum(p)/sum(weights), standardize = F,      penalty.factor = reg) 
#&gt; 
#&gt;     Df  %Dev  Lambda
#&gt; 1    0  0.00 26.2800
#&gt; 2    0  0.00 25.1000
#&gt; 3    0  0.00 23.9600
#&gt; 4    0  0.00 22.8800
#&gt; 5    0  0.00 21.8400
#&gt; 6    0  0.00 20.8500
#&gt; 7    0  0.00 19.9100
#&gt; 8    0  0.00 19.0100
#&gt; 9    0  0.00 18.1500
#&gt; 10   0  0.00 17.3300
#&gt; 11   0  0.00 16.5500
#&gt; 12   0  0.00 15.8000
#&gt; 13   0  0.00 15.0800
#&gt; 14   0  0.00 14.4000
#&gt; 15   0  0.00 13.7500
#&gt; 16   0  0.00 13.1300
#&gt; 17   0  0.00 12.5300
#&gt; 18   0  0.00 11.9700
#&gt; 19   0  0.00 11.4300
#&gt; 20   0  0.00 10.9100
#&gt; 21   0  0.00 10.4200
#&gt; 22   0  0.00  9.9450
#&gt; 23   0  0.00  9.4950
#&gt; 24   0  0.00  9.0650
#&gt; 25   0  0.00  8.6550
#&gt; 26   0  0.00  8.2640
#&gt; 27   0  0.00  7.8900
#&gt; 28   0  0.00  7.5330
#&gt; 29   0  0.00  7.1930
#&gt; 30   0  0.00  6.8670
#&gt; 31   0  0.00  6.5570
#&gt; 32   0  0.00  6.2600
#&gt; 33   0  0.00  5.9770
#&gt; 34   0  0.00  5.7070
#&gt; 35   0  0.00  5.4490
#&gt; 36   0  0.00  5.2020
#&gt; 37   0  0.00  4.9670
#&gt; 38   0  0.00  4.7420
#&gt; 39   0  0.00  4.5280
#&gt; 40   0  0.00  4.3230
#&gt; 41   0  0.00  4.1270
#&gt; 42   0  0.00  3.9410
#&gt; 43   0  0.00  3.7620
#&gt; 44   0  0.00  3.5920
#&gt; 45   0  0.00  3.4300
#&gt; 46   0  0.00  3.2750
#&gt; 47   0  0.00  3.1270
#&gt; 48   0  0.00  2.9850
#&gt; 49   0  0.00  2.8500
#&gt; 50   0  0.00  2.7210
#&gt; 51   0  0.00  2.5980
#&gt; 52   0  0.00  2.4810
#&gt; 53   0  0.00  2.3680
#&gt; 54   0  0.00  2.2610
#&gt; 55   0  0.00  2.1590
#&gt; 56   0  0.00  2.0610
#&gt; 57   0  0.00  1.9680
#&gt; 58   0  0.00  1.8790
#&gt; 59   0  0.00  1.7940
#&gt; 60   0  0.00  1.7130
#&gt; 61   0  0.00  1.6360
#&gt; 62   0  0.00  1.5620
#&gt; 63   0  0.00  1.4910
#&gt; 64   0  0.00  1.4240
#&gt; 65   0  0.00  1.3590
#&gt; 66   0  0.00  1.2980
#&gt; 67   0  0.00  1.2390
#&gt; 68   0  0.00  1.1830
#&gt; 69   0  0.00  1.1290
#&gt; 70   0  0.00  1.0780
#&gt; 71   0  0.00  1.0300
#&gt; 72   0  0.00  0.9830
#&gt; 73   0  0.00  0.9386
#&gt; 74   0  0.00  0.8961
#&gt; 75   0  0.00  0.8556
#&gt; 76   0  0.00  0.8169
#&gt; 77   0  0.00  0.7799
#&gt; 78   0  0.00  0.7447
#&gt; 79   0  0.00  0.7110
#&gt; 80   0  0.00  0.6788
#&gt; 81   0  0.00  0.6481
#&gt; 82   0  0.00  0.6188
#&gt; 83   0  0.00  0.5908
#&gt; 84   0  0.00  0.5641
#&gt; 85   0  0.00  0.5386
#&gt; 86   0  0.00  0.5142
#&gt; 87   0  0.00  0.4910
#&gt; 88   0  0.00  0.4688
#&gt; 89   0  0.00  0.4476
#&gt; 90   0  0.00  0.4273
#&gt; 91   0  0.00  0.4080
#&gt; 92   0  0.00  0.3895
#&gt; 93   0  0.00  0.3719
#&gt; 94   0  0.00  0.3551
#&gt; 95   0  0.00  0.3390
#&gt; 96   0  0.00  0.3237
#&gt; 97   0  0.00  0.3091
#&gt; 98   0  0.00  0.2951
#&gt; 99   0  0.00  0.2817
#&gt; 100  0  0.00  0.2690
#&gt; 101  0  0.00  0.2568
#&gt; 102  0  0.00  0.2452
#&gt; 103  0  0.00  0.2341
#&gt; 104  0  0.00  0.2235
#&gt; 105  0  0.00  0.2134
#&gt; 106  0  0.00  0.2038
#&gt; 107  0  0.00  0.1946
#&gt; 108  0  0.00  0.1858
#&gt; 109  0  0.00  0.1774
#&gt; 110  0  0.00  0.1693
#&gt; 111  0  0.00  0.1617
#&gt; 112  0  0.00  0.1544
#&gt; 113  0  0.00  0.1474
#&gt; 114  0  0.00  0.1407
#&gt; 115  0  0.00  0.1343
#&gt; 116  0  0.00  0.1283
#&gt; 117  0  0.00  0.1225
#&gt; 118  0  0.00  0.1169
#&gt; 119  0  0.00  0.1116
#&gt; 120  0  0.00  0.1066
#&gt; 121  0  0.00  0.1018
#&gt; 122  0  0.00  0.0972
#&gt; 123  0  0.00  0.0928
#&gt; 124  0  0.00  0.0886
#&gt; 125  1  0.81  0.0846
#&gt; 126  1  2.82  0.0808
#&gt; 127  1  4.73  0.0771
#&gt; 128  1  6.55  0.0736
#&gt; 129  1  8.30  0.0703
#&gt; 130  1  9.98  0.0671
#&gt; 131  1 11.61  0.0641
#&gt; 132  1 13.20  0.0612
#&gt; 133  1 14.74  0.0584
#&gt; 134  1 16.25  0.0558
#&gt; 135  1 17.74  0.0532
#&gt; 136  1 19.21  0.0508
#&gt; 137  1 20.65  0.0485
#&gt; 138  1 22.08  0.0463
#&gt; 139  1 23.50  0.0442
#&gt; 140  1 24.91  0.0422
#&gt; 141  1 26.31  0.0403
#&gt; 142  1 27.70  0.0385
#&gt; 143  1 29.08  0.0368
#&gt; 144  1 30.46  0.0351
#&gt; 145  1 31.83  0.0335
#&gt; 146  1 33.19  0.0320
#&gt; 147  1 34.54  0.0306
#&gt; 148  1 35.89  0.0292
#&gt; 149  1 37.22  0.0278
#&gt; 150  1 38.54  0.0266
#&gt; 151  1 39.86  0.0254
#&gt; 152  1 41.16  0.0242
#&gt; 153  1 42.47  0.0231
#&gt; 154  1 43.75  0.0221
#&gt; 155  1 45.04  0.0211
#&gt; 156  1 46.31  0.0201
#&gt; 157  1 47.57  0.0192
#&gt; 158  1 48.82  0.0184
#&gt; 159  1 50.06  0.0175
#&gt; 160  1 51.28  0.0167
#&gt; 161  1 52.48  0.0160
#&gt; 162  1 53.65  0.0153
#&gt; 163  1 54.80  0.0146
#&gt; 164  1 55.91  0.0139
#&gt; 165  1 56.99  0.0133
#&gt; 166  1 58.03  0.0127
#&gt; 167  1 59.04  0.0121
#&gt; 168  1 60.02  0.0116
#&gt; 169  1 60.95  0.0110
#&gt; 170  1 61.85  0.0105
#&gt; 171  1 62.72  0.0101
#&gt; 172  1 63.55  0.0096
#&gt; 173  1 64.36  0.0092
#&gt; 174  1 65.14  0.0088
#&gt; 175  1 65.90  0.0084
#&gt; 176  1 66.63  0.0080
#&gt; 177  1 67.34  0.0076
#&gt; 178  1 68.04  0.0073
#&gt; 179  2 69.24  0.0069
#&gt; 180  2 70.55  0.0066
#&gt; 181  2 71.70  0.0063
#&gt; 182  2 72.73  0.0060
#&gt; 183  2 73.67  0.0058
#&gt; 184  2 74.55  0.0055
#&gt; 185  2 75.37  0.0053
#&gt; 186  2 76.15  0.0050
#&gt; 187  2 76.89  0.0048
#&gt; 188  2 77.61  0.0046
#&gt; 189  2 78.31  0.0044
#&gt; 190  3 79.00  0.0042
#&gt; 191  3 79.90  0.0040
#&gt; 192  3 80.75  0.0038
#&gt; 193  3 81.55  0.0036
#&gt; 194  3 82.30  0.0035
#&gt; 195  3 83.00  0.0033
#&gt; 196  3 83.67  0.0032
#&gt; 197  3 84.30  0.0030
#&gt; 198  3 84.91  0.0029
#&gt; 199  3 85.48  0.0028
#&gt; 200  3 86.03  0.0026
#&gt; 
#&gt; $`0.94`
#&gt; 
#&gt; Call:  glmnet::glmnet(x = mm, y = as.factor(p), family = "binomial",      weights = weights, lambda = 10^(seq(4, 0, length.out = 200)) *          sum(reg)/length(reg) * sum(p)/sum(weights), standardize = F,      penalty.factor = reg) 
#&gt; 
#&gt;     Df  %Dev Lambda
#&gt; 1    0  0.00  94830
#&gt; 2    0  0.00  90540
#&gt; 3    0  0.00  86440
#&gt; 4    0  0.00  82530
#&gt; 5    0  0.00  78800
#&gt; 6    0  0.00  75240
#&gt; 7    0  0.00  71830
#&gt; 8    0  0.00  68590
#&gt; 9    0  0.00  65480
#&gt; 10   0  0.00  62520
#&gt; 11   0  0.00  59690
#&gt; 12   0  0.00  56990
#&gt; 13   0  0.00  54420
#&gt; 14   0  0.00  51960
#&gt; 15   0  0.00  49610
#&gt; 16   0  0.00  47360
#&gt; 17   0  0.00  45220
#&gt; 18   0  0.00  43170
#&gt; 19   0  0.00  41220
#&gt; 20   0  0.00  39360
#&gt; 21   0  0.00  37580
#&gt; 22   0  0.00  35880
#&gt; 23   0  0.00  34260
#&gt; 24   0  0.00  32710
#&gt; 25   0  0.00  31230
#&gt; 26   0  0.00  29810
#&gt; 27   0  0.00  28470
#&gt; 28   0  0.00  27180
#&gt; 29   0  0.00  25950
#&gt; 30   0  0.00  24780
#&gt; 31   0  0.00  23650
#&gt; 32   0  0.00  22590
#&gt; 33   0  0.00  21560
#&gt; 34   0  0.00  20590
#&gt; 35   0  0.00  19660
#&gt; 36   0  0.00  18770
#&gt; 37   0  0.00  17920
#&gt; 38   0  0.00  17110
#&gt; 39   0  0.00  16330
#&gt; 40   0  0.00  15600
#&gt; 41   0  0.00  14890
#&gt; 42   0  0.00  14220
#&gt; 43   0  0.00  13570
#&gt; 44   0  0.00  12960
#&gt; 45   0  0.00  12370
#&gt; 46   0  0.00  11810
#&gt; 47   0  0.00  11280
#&gt; 48   0  0.00  10770
#&gt; 49   0  0.00  10280
#&gt; 50   0  0.00   9818
#&gt; 51   0  0.00   9374
#&gt; 52   0  0.00   8950
#&gt; 53   0  0.00   8545
#&gt; 54   0  0.00   8158
#&gt; 55   0  0.00   7789
#&gt; 56   0  0.00   7437
#&gt; 57   0  0.00   7101
#&gt; 58   0  0.00   6780
#&gt; 59   0  0.00   6473
#&gt; 60   0  0.00   6180
#&gt; 61   0  0.00   5901
#&gt; 62   0  0.00   5634
#&gt; 63   0  0.00   5379
#&gt; 64   0  0.00   5136
#&gt; 65   0  0.00   4903
#&gt; 66   0  0.00   4682
#&gt; 67   0  0.00   4470
#&gt; 68   0  0.00   4268
#&gt; 69   0  0.00   4075
#&gt; 70   0  0.00   3890
#&gt; 71   0  0.00   3715
#&gt; 72   0  0.00   3547
#&gt; 73   0  0.00   3386
#&gt; 74   0  0.00   3233
#&gt; 75   0  0.00   3087
#&gt; 76   0  0.00   2947
#&gt; 77   0  0.00   2814
#&gt; 78   0  0.00   2687
#&gt; 79   0  0.00   2565
#&gt; 80   0  0.00   2449
#&gt; 81   0  0.00   2338
#&gt; 82   0  0.00   2233
#&gt; 83   0  0.00   2132
#&gt; 84   0  0.00   2035
#&gt; 85   0  0.00   1943
#&gt; 86   0  0.00   1855
#&gt; 87   0  0.00   1771
#&gt; 88   0  0.00   1691
#&gt; 89   0  0.00   1615
#&gt; 90   0  0.00   1542
#&gt; 91   0  0.00   1472
#&gt; 92   0  0.00   1405
#&gt; 93   0  0.00   1342
#&gt; 94   0  0.00   1281
#&gt; 95   0  0.00   1223
#&gt; 96   0  0.00   1168
#&gt; 97   0  0.00   1115
#&gt; 98   0  0.00   1065
#&gt; 99   0  0.00   1016
#&gt; 100  0  0.00    970
#&gt; 101  0  0.00    927
#&gt; 102  0  0.00    885
#&gt; 103  0  0.00    845
#&gt; 104  0  0.00    806
#&gt; 105  0  0.00    770
#&gt; 106  0  0.00    735
#&gt; 107  0  0.00    702
#&gt; 108  0  0.00    670
#&gt; 109  0  0.00    640
#&gt; 110  0  0.00    611
#&gt; 111  0  0.00    583
#&gt; 112  0  0.00    557
#&gt; 113  0  0.00    532
#&gt; 114  0  0.00    508
#&gt; 115  0  0.00    485
#&gt; 116  0  0.00    463
#&gt; 117  0  0.00    442
#&gt; 118  0  0.00    422
#&gt; 119  0  0.00    403
#&gt; 120  0  0.00    385
#&gt; 121  0  0.00    367
#&gt; 122  0  0.00    351
#&gt; 123  0  0.00    335
#&gt; 124  0  0.00    320
#&gt; 125  1  0.81    305
#&gt; 126  1  2.82    291
#&gt; 127  1  4.73    278
#&gt; 128  1  6.55    266
#&gt; 129  1  8.30    254
#&gt; 130  1  9.98    242
#&gt; 131  1 11.61    231
#&gt; 132  1 13.20    221
#&gt; 133  1 14.74    211
#&gt; 134  1 16.25    201
#&gt; 135  1 17.74    192
#&gt; 136  1 19.21    183
#&gt; 137  1 20.65    175
#&gt; 138  1 22.08    167
#&gt; 139  1 23.50    160
#&gt; 140  1 24.91    152
#&gt; 141  1 26.31    146
#&gt; 142  1 27.70    139
#&gt; 143  1 29.08    133
#&gt; 144  1 30.46    127
#&gt; 145  1 31.83    121
#&gt; 146  1 33.19    115
#&gt; 147  1 34.54    110
#&gt; 148  1 35.89    105
#&gt; 149  1 37.22    100
#&gt; 150  1 38.54     96
#&gt; 151  1 39.86     92
#&gt; 152  1 41.16     87
#&gt; 153  1 42.47     83
#&gt; 154  1 43.75     80
#&gt; 155  1 45.04     76
#&gt; 156  1 46.31     73
#&gt; 157  1 47.57     69
#&gt; 158  1 48.82     66
#&gt; 159  1 50.06     63
#&gt; 160  1 51.28     60
#&gt; 161  1 52.48     58
#&gt; 162  1 53.65     55
#&gt; 163  1 54.80     53
#&gt; 164  1 55.91     50
#&gt; 165  1 56.99     48
#&gt; 166  1 58.03     46
#&gt; 167  1 59.04     44
#&gt; 168  1 60.02     42
#&gt; 169  1 60.95     40
#&gt; 170  1 61.85     38
#&gt; 171  1 62.72     36
#&gt; 172  2 64.44     35
#&gt; 173  2 66.48     33
#&gt; 174  2 68.30     32
#&gt; 175  2 69.93     30
#&gt; 176  2 71.42     29
#&gt; 177  2 72.77     27
#&gt; 178  2 74.02     26
#&gt; 179  2 75.18     25
#&gt; 180  2 76.25     24
#&gt; 181  2 77.25     23
#&gt; 182  2 78.20     22
#&gt; 183  2 79.08     21
#&gt; 184  2 79.91     20
#&gt; 185  2 80.70     19
#&gt; 186  2 81.44     18
#&gt; 187  2 82.15     17
#&gt; 188  2 82.82     17
#&gt; 189  2 83.45     16
#&gt; 190  2 84.06     15
#&gt; 191  2 84.63     14
#&gt; 192  2 85.18     14
#&gt; 193  2 85.71     13
#&gt; 194  2 86.21     13
#&gt; 195  2 86.69     12
#&gt; 196  2 87.15     11
#&gt; 197  2 87.59     11
#&gt; 198  2 88.02     10
#&gt; 199  2 88.42     10
#&gt; 200  2 88.81      9
#&gt; 
#&gt; $`-0.24`
#&gt; 
#&gt; Call:  glmnet::glmnet(x = mm, y = as.factor(p), family = "binomial",      weights = weights, lambda = 10^(seq(4, 0, length.out = 200)) *          sum(reg)/length(reg) * sum(p)/sum(weights), standardize = F,      penalty.factor = reg) 
#&gt; 
#&gt;     Df %Dev Lambda
#&gt; 1    0 0.00 289900
#&gt; 2    0 0.00 276800
#&gt; 3    0 0.00 264200
#&gt; 4    0 0.00 252300
#&gt; 5    0 0.00 240900
#&gt; 6    0 0.00 230000
#&gt; 7    0 0.00 219600
#&gt; 8    0 0.00 209700
#&gt; 9    0 0.00 200200
#&gt; 10   0 0.00 191100
#&gt; 11   0 0.00 182500
#&gt; 12   0 0.00 174200
#&gt; 13   0 0.00 166300
#&gt; 14   0 0.00 158800
#&gt; 15   0 0.00 151600
#&gt; 16   0 0.00 144800
#&gt; 17   0 0.00 138200
#&gt; 18   0 0.00 132000
#&gt; 19   0 0.00 126000
#&gt; 20   0 0.00 120300
#&gt; 21   0 0.00 114900
#&gt; 22   0 0.00 109700
#&gt; 23   0 0.00 104700
#&gt; 24   0 0.00  99970
#&gt; 25   0 0.00  95450
#&gt; 26   0 0.00  91140
#&gt; 27   0 0.00  87010
#&gt; 28   0 0.00  83080
#&gt; 29   0 0.00  79320
#&gt; 30   0 0.00  75730
#&gt; 31   0 0.00  72310
#&gt; 32   0 0.00  69040
#&gt; 33   0 0.00  65920
#&gt; 34   0 0.00  62930
#&gt; 35   0 0.00  60090
#&gt; 36   0 0.00  57370
#&gt; 37   0 0.00  54780
#&gt; 38   0 0.00  52300
#&gt; 39   0 0.00  49930
#&gt; 40   0 0.00  47670
#&gt; 41   0 0.00  45520
#&gt; 42   0 0.00  43460
#&gt; 43   0 0.00  41490
#&gt; 44   0 0.00  39620
#&gt; 45   0 0.00  37830
#&gt; 46   0 0.00  36110
#&gt; 47   0 0.00  34480
#&gt; 48   0 0.00  32920
#&gt; 49   0 0.00  31430
#&gt; 50   0 0.00  30010
#&gt; 51   0 0.00  28650
#&gt; 52   0 0.00  27360
#&gt; 53   0 0.00  26120
#&gt; 54   0 0.00  24940
#&gt; 55   0 0.00  23810
#&gt; 56   0 0.00  22730
#&gt; 57   0 0.00  21710
#&gt; 58   0 0.00  20720
#&gt; 59   0 0.00  19790
#&gt; 60   0 0.00  18890
#&gt; 61   0 0.00  18040
#&gt; 62   0 0.00  17220
#&gt; 63   0 0.00  16440
#&gt; 64   0 0.00  15700
#&gt; 65   0 0.00  14990
#&gt; 66   0 0.00  14310
#&gt; 67   0 0.00  13660
#&gt; 68   0 0.00  13050
#&gt; 69   0 0.00  12460
#&gt; 70   0 0.00  11890
#&gt; 71   0 0.00  11350
#&gt; 72   0 0.00  10840
#&gt; 73   0 0.00  10350
#&gt; 74   0 0.00   9882
#&gt; 75   0 0.00   9435
#&gt; 76   0 0.00   9009
#&gt; 77   0 0.00   8601
#&gt; 78   0 0.00   8212
#&gt; 79   0 0.00   7841
#&gt; 80   0 0.00   7486
#&gt; 81   0 0.00   7148
#&gt; 82   0 0.00   6824
#&gt; 83   0 0.00   6516
#&gt; 84   0 0.00   6221
#&gt; 85   0 0.00   5940
#&gt; 86   0 0.00   5671
#&gt; 87   0 0.00   5414
#&gt; 88   0 0.00   5170
#&gt; 89   0 0.00   4936
#&gt; 90   0 0.00   4713
#&gt; 91   0 0.00   4499
#&gt; 92   0 0.00   4296
#&gt; 93   0 0.00   4102
#&gt; 94   0 0.00   3916
#&gt; 95   0 0.00   3739
#&gt; 96   0 0.00   3570
#&gt; 97   0 0.00   3408
#&gt; 98   0 0.00   3254
#&gt; 99   0 0.00   3107
#&gt; 100  0 0.00   2967
#&gt; 101  0 0.00   2832
#&gt; 102  0 0.00   2704
#&gt; 103  0 0.00   2582
#&gt; 104  0 0.00   2465
#&gt; 105  0 0.00   2354
#&gt; 106  0 0.00   2247
#&gt; 107  0 0.00   2146
#&gt; 108  0 0.00   2049
#&gt; 109  0 0.00   1956
#&gt; 110  0 0.00   1867
#&gt; 111  0 0.00   1783
#&gt; 112  0 0.00   1702
#&gt; 113  0 0.00   1625
#&gt; 114  0 0.00   1552
#&gt; 115  0 0.00   1482
#&gt; 116  0 0.00   1415
#&gt; 117  0 0.00   1351
#&gt; 118  0 0.00   1290
#&gt; 119  0 0.00   1231
#&gt; 120  0 0.00   1176
#&gt; 121  0 0.00   1122
#&gt; 122  0 0.00   1072
#&gt; 123  0 0.00   1023
#&gt; 124  0 0.00    977
#&gt; 125  0 0.00    933
#&gt; 126  0 0.00    890
#&gt; 127  0 0.00    850
#&gt; 128  0 0.00    812
#&gt; 129  0 0.00    775
#&gt; 130  0 0.00    740
#&gt; 131  0 0.00    706
#&gt; 132  0 0.00    675
#&gt; 133  0 0.00    644
#&gt; 134  0 0.00    615
#&gt; 135  0 0.00    587
#&gt; 136  0 0.00    561
#&gt; 137  0 0.00    535
#&gt; 138  0 0.00    511
#&gt; 139  0 0.00    488
#&gt; 140  0 0.00    466
#&gt; 141  0 0.00    445
#&gt; 142  0 0.00    425
#&gt; 143  0 0.00    405
#&gt; 144  0 0.00    387
#&gt; 145  0 0.00    370
#&gt; 146  0 0.00    353
#&gt; 147  0 0.00    337
#&gt; 148  0 0.00    322
#&gt; 149  0 0.00    307
#&gt; 150  0 0.00    293
#&gt; 151  0 0.00    280
#&gt; 152  0 0.00    267
#&gt; 153  0 0.00    255
#&gt; 154  0 0.00    244
#&gt; 155  0 0.00    233
#&gt; 156  0 0.00    222
#&gt; 157  0 0.00    212
#&gt; 158  0 0.00    202
#&gt; 159  0 0.00    193
#&gt; 160  0 0.00    185
#&gt; 161  1 0.05    176
#&gt; 162  1 0.22    168
#&gt; 163  1 0.38    161
#&gt; 164  1 0.54    153
#&gt; 165  1 0.69    146
#&gt; 166  1 0.83    140
#&gt; 167  1 0.97    134
#&gt; 168  1 1.10    128
#&gt; 169  1 1.23    122
#&gt; 170  1 1.35    116
#&gt; 171  1 1.47    111
#&gt; 172  1 1.58    106
#&gt; 173  1 1.69    101
#&gt; 174  1 1.80     97
#&gt; 175  1 1.90     92
#&gt; 176  1 1.99     88
#&gt; 177  1 2.09     84
#&gt; 178  1 2.18     80
#&gt; 179  1 2.26     77
#&gt; 180  1 2.35     73
#&gt; 181  1 2.43     70
#&gt; 182  1 2.50     67
#&gt; 183  1 2.58     64
#&gt; 184  1 2.65     61
#&gt; 185  1 2.72     58
#&gt; 186  1 2.78     55
#&gt; 187  1 2.85     53
#&gt; 188  1 2.91     51
#&gt; 189  1 2.97     48
#&gt; 190  1 3.03     46
#&gt; 191  1 3.08     44
#&gt; 192  1 3.13     42
#&gt; 193  1 3.18     40
#&gt; 194  2 3.26     38
#&gt; 195  2 3.38     37
#&gt; 196  2 3.49     35
#&gt; 197  2 3.58     33
#&gt; 198  2 3.67     32
#&gt; 199  2 3.75     30
#&gt; 200  2 3.83     29
#&gt; 
#&gt; $`-0.3`
#&gt; 
#&gt; Call:  glmnet::glmnet(x = mm, y = as.factor(p), family = "binomial",      weights = weights, lambda = 10^(seq(4, 0, length.out = 200)) *          sum(reg)/length(reg) * sum(p)/sum(weights), standardize = F,      penalty.factor = reg) 
#&gt; 
#&gt;     Df %Dev Lambda
#&gt; 1    0 0.00 288700
#&gt; 2    0 0.00 275700
#&gt; 3    0 0.00 263200
#&gt; 4    0 0.00 251300
#&gt; 5    0 0.00 239900
#&gt; 6    0 0.00 229100
#&gt; 7    0 0.00 218700
#&gt; 8    0 0.00 208800
#&gt; 9    0 0.00 199400
#&gt; 10   0 0.00 190400
#&gt; 11   0 0.00 181700
#&gt; 12   0 0.00 173500
#&gt; 13   0 0.00 165700
#&gt; 14   0 0.00 158200
#&gt; 15   0 0.00 151000
#&gt; 16   0 0.00 144200
#&gt; 17   0 0.00 137700
#&gt; 18   0 0.00 131400
#&gt; 19   0 0.00 125500
#&gt; 20   0 0.00 119800
#&gt; 21   0 0.00 114400
#&gt; 22   0 0.00 109200
#&gt; 23   0 0.00 104300
#&gt; 24   0 0.00  99570
#&gt; 25   0 0.00  95070
#&gt; 26   0 0.00  90770
#&gt; 27   0 0.00  86670
#&gt; 28   0 0.00  82750
#&gt; 29   0 0.00  79000
#&gt; 30   0 0.00  75430
#&gt; 31   0 0.00  72020
#&gt; 32   0 0.00  68760
#&gt; 33   0 0.00  65650
#&gt; 34   0 0.00  62680
#&gt; 35   0 0.00  59850
#&gt; 36   0 0.00  57140
#&gt; 37   0 0.00  54560
#&gt; 38   0 0.00  52090
#&gt; 39   0 0.00  49730
#&gt; 40   0 0.00  47480
#&gt; 41   0 0.00  45340
#&gt; 42   0 0.00  43290
#&gt; 43   0 0.00  41330
#&gt; 44   0 0.00  39460
#&gt; 45   0 0.00  37670
#&gt; 46   0 0.00  35970
#&gt; 47   0 0.00  34340
#&gt; 48   0 0.00  32790
#&gt; 49   0 0.00  31310
#&gt; 50   0 0.00  29890
#&gt; 51   0 0.00  28540
#&gt; 52   0 0.00  27250
#&gt; 53   0 0.00  26020
#&gt; 54   0 0.00  24840
#&gt; 55   0 0.00  23720
#&gt; 56   0 0.00  22640
#&gt; 57   0 0.00  21620
#&gt; 58   0 0.00  20640
#&gt; 59   0 0.00  19710
#&gt; 60   0 0.00  18820
#&gt; 61   0 0.00  17970
#&gt; 62   0 0.00  17150
#&gt; 63   0 0.00  16380
#&gt; 64   0 0.00  15640
#&gt; 65   0 0.00  14930
#&gt; 66   0 0.00  14250
#&gt; 67   0 0.00  13610
#&gt; 68   0 0.00  12990
#&gt; 69   0 0.00  12410
#&gt; 70   0 0.00  11840
#&gt; 71   0 0.00  11310
#&gt; 72   0 0.00  10800
#&gt; 73   0 0.00  10310
#&gt; 74   0 0.00   9843
#&gt; 75   0 0.00   9398
#&gt; 76   0 0.00   8973
#&gt; 77   0 0.00   8567
#&gt; 78   0 0.00   8179
#&gt; 79   0 0.00   7809
#&gt; 80   0 0.00   7456
#&gt; 81   0 0.00   7119
#&gt; 82   0 0.00   6797
#&gt; 83   0 0.00   6490
#&gt; 84   0 0.00   6196
#&gt; 85   0 0.00   5916
#&gt; 86   0 0.00   5648
#&gt; 87   0 0.00   5393
#&gt; 88   0 0.00   5149
#&gt; 89   0 0.00   4916
#&gt; 90   0 0.00   4694
#&gt; 91   0 0.00   4481
#&gt; 92   0 0.00   4279
#&gt; 93   0 0.00   4085
#&gt; 94   0 0.00   3900
#&gt; 95   0 0.00   3724
#&gt; 96   0 0.00   3556
#&gt; 97   0 0.00   3395
#&gt; 98   0 0.00   3241
#&gt; 99   0 0.00   3095
#&gt; 100  0 0.00   2955
#&gt; 101  0 0.00   2821
#&gt; 102  0 0.00   2693
#&gt; 103  0 0.00   2572
#&gt; 104  0 0.00   2455
#&gt; 105  0 0.00   2344
#&gt; 106  0 0.00   2238
#&gt; 107  0 0.00   2137
#&gt; 108  0 0.00   2040
#&gt; 109  0 0.00   1948
#&gt; 110  0 0.00   1860
#&gt; 111  0 0.00   1776
#&gt; 112  0 0.00   1696
#&gt; 113  0 0.00   1619
#&gt; 114  0 0.00   1546
#&gt; 115  0 0.00   1476
#&gt; 116  0 0.00   1409
#&gt; 117  0 0.00   1345
#&gt; 118  0 0.00   1284
#&gt; 119  0 0.00   1226
#&gt; 120  0 0.00   1171
#&gt; 121  0 0.00   1118
#&gt; 122  0 0.00   1067
#&gt; 123  0 0.00   1019
#&gt; 124  0 0.00    973
#&gt; 125  0 0.00    929
#&gt; 126  0 0.00    887
#&gt; 127  0 0.00    847
#&gt; 128  0 0.00    808
#&gt; 129  0 0.00    772
#&gt; 130  0 0.00    737
#&gt; 131  0 0.00    704
#&gt; 132  0 0.00    672
#&gt; 133  0 0.00    642
#&gt; 134  0 0.00    612
#&gt; 135  0 0.00    585
#&gt; 136  0 0.00    558
#&gt; 137  0 0.00    533
#&gt; 138  0 0.00    509
#&gt; 139  0 0.00    486
#&gt; 140  0 0.00    464
#&gt; 141  0 0.00    443
#&gt; 142  0 0.00    423
#&gt; 143  0 0.00    404
#&gt; 144  0 0.00    386
#&gt; 145  0 0.00    368
#&gt; 146  0 0.00    352
#&gt; 147  0 0.00    336
#&gt; 148  0 0.00    320
#&gt; 149  0 0.00    306
#&gt; 150  0 0.00    292
#&gt; 151  0 0.00    279
#&gt; 152  0 0.00    266
#&gt; 153  0 0.00    254
#&gt; 154  0 0.00    243
#&gt; 155  0 0.00    232
#&gt; 156  0 0.00    221
#&gt; 157  0 0.00    211
#&gt; 158  0 0.00    202
#&gt; 159  0 0.00    193
#&gt; 160  0 0.00    184
#&gt; 161  0 0.00    176
#&gt; 162  0 0.00    168
#&gt; 163  0 0.00    160
#&gt; 164  0 0.00    153
#&gt; 165  0 0.00    146
#&gt; 166  0 0.00    139
#&gt; 167  0 0.00    133
#&gt; 168  0 0.00    127
#&gt; 169  0 0.00    121
#&gt; 170  0 0.00    116
#&gt; 171  0 0.00    110
#&gt; 172  0 0.00    106
#&gt; 173  0 0.00    101
#&gt; 174  0 0.00     96
#&gt; 175  0 0.00     92
#&gt; 176  0 0.00     88
#&gt; 177  0 0.00     84
#&gt; 178  0 0.00     80
#&gt; 179  1 0.12     76
#&gt; 180  1 0.28     73
#&gt; 181  1 0.42     70
#&gt; 182  1 0.55     66
#&gt; 183  1 0.68     63
#&gt; 184  1 0.79     61
#&gt; 185  1 0.90     58
#&gt; 186  1 1.00     55
#&gt; 187  1 1.10     53
#&gt; 188  1 1.18     50
#&gt; 189  1 1.26     48
#&gt; 190  1 1.34     46
#&gt; 191  1 1.41     44
#&gt; 192  1 1.48     42
#&gt; 193  1 1.54     40
#&gt; 194  1 1.59     38
#&gt; 195  1 1.64     36
#&gt; 196  1 1.69     35
#&gt; 197  1 1.74     33
#&gt; 198  1 1.78     32
#&gt; 199  1 1.82     30
#&gt; 200  1 1.85     29
#&gt; 
#&gt; $`-0.0600000000000001`
#&gt; 
#&gt; Call:  glmnet::glmnet(x = mm, y = as.factor(p), family = "binomial",      weights = weights, lambda = 10^(seq(4, 0, length.out = 200)) *          sum(reg)/length(reg) * sum(p)/sum(weights), standardize = F,      penalty.factor = reg) 
#&gt; 
#&gt;     Df %Dev Lambda
#&gt; 1    0 0.00 288600
#&gt; 2    0 0.00 275600
#&gt; 3    0 0.00 263100
#&gt; 4    0 0.00 251200
#&gt; 5    0 0.00 239800
#&gt; 6    0 0.00 229000
#&gt; 7    0 0.00 218600
#&gt; 8    0 0.00 208700
#&gt; 9    0 0.00 199300
#&gt; 10   0 0.00 190300
#&gt; 11   0 0.00 181700
#&gt; 12   0 0.00 173500
#&gt; 13   0 0.00 165600
#&gt; 14   0 0.00 158100
#&gt; 15   0 0.00 151000
#&gt; 16   0 0.00 144100
#&gt; 17   0 0.00 137600
#&gt; 18   0 0.00 131400
#&gt; 19   0 0.00 125500
#&gt; 20   0 0.00 119800
#&gt; 21   0 0.00 114400
#&gt; 22   0 0.00 109200
#&gt; 23   0 0.00 104300
#&gt; 24   0 0.00  99540
#&gt; 25   0 0.00  95040
#&gt; 26   0 0.00  90740
#&gt; 27   0 0.00  86640
#&gt; 28   0 0.00  82720
#&gt; 29   0 0.00  78980
#&gt; 30   0 0.00  75410
#&gt; 31   0 0.00  71990
#&gt; 32   0 0.00  68740
#&gt; 33   0 0.00  65630
#&gt; 34   0 0.00  62660
#&gt; 35   0 0.00  59830
#&gt; 36   0 0.00  57120
#&gt; 37   0 0.00  54540
#&gt; 38   0 0.00  52070
#&gt; 39   0 0.00  49720
#&gt; 40   0 0.00  47470
#&gt; 41   0 0.00  45320
#&gt; 42   0 0.00  43270
#&gt; 43   0 0.00  41310
#&gt; 44   0 0.00  39450
#&gt; 45   0 0.00  37660
#&gt; 46   0 0.00  35960
#&gt; 47   0 0.00  34330
#&gt; 48   0 0.00  32780
#&gt; 49   0 0.00  31300
#&gt; 50   0 0.00  29880
#&gt; 51   0 0.00  28530
#&gt; 52   0 0.00  27240
#&gt; 53   0 0.00  26010
#&gt; 54   0 0.00  24830
#&gt; 55   0 0.00  23710
#&gt; 56   0 0.00  22640
#&gt; 57   0 0.00  21610
#&gt; 58   0 0.00  20630
#&gt; 59   0 0.00  19700
#&gt; 60   0 0.00  18810
#&gt; 61   0 0.00  17960
#&gt; 62   0 0.00  17150
#&gt; 63   0 0.00  16370
#&gt; 64   0 0.00  15630
#&gt; 65   0 0.00  14920
#&gt; 66   0 0.00  14250
#&gt; 67   0 0.00  13600
#&gt; 68   0 0.00  12990
#&gt; 69   0 0.00  12400
#&gt; 70   0 0.00  11840
#&gt; 71   0 0.00  11310
#&gt; 72   0 0.00  10790
#&gt; 73   0 0.00  10310
#&gt; 74   0 0.00   9840
#&gt; 75   0 0.00   9395
#&gt; 76   0 0.00   8970
#&gt; 77   0 0.00   8564
#&gt; 78   0 0.00   8177
#&gt; 79   0 0.00   7807
#&gt; 80   0 0.00   7454
#&gt; 81   0 0.00   7117
#&gt; 82   0 0.00   6795
#&gt; 83   0 0.00   6487
#&gt; 84   0 0.00   6194
#&gt; 85   0 0.00   5914
#&gt; 86   0 0.00   5646
#&gt; 87   0 0.00   5391
#&gt; 88   0 0.00   5147
#&gt; 89   0 0.00   4914
#&gt; 90   0 0.00   4692
#&gt; 91   0 0.00   4480
#&gt; 92   0 0.00   4277
#&gt; 93   0 0.00   4084
#&gt; 94   0 0.00   3899
#&gt; 95   0 0.00   3723
#&gt; 96   0 0.00   3554
#&gt; 97   0 0.00   3394
#&gt; 98   0 0.00   3240
#&gt; 99   0 0.00   3094
#&gt; 100  0 0.00   2954
#&gt; 101  0 0.00   2820
#&gt; 102  0 0.00   2693
#&gt; 103  0 0.00   2571
#&gt; 104  0 0.00   2455
#&gt; 105  0 0.00   2343
#&gt; 106  0 0.00   2238
#&gt; 107  0 0.00   2136
#&gt; 108  0 0.00   2040
#&gt; 109  0 0.00   1947
#&gt; 110  0 0.00   1859
#&gt; 111  0 0.00   1775
#&gt; 112  0 0.00   1695
#&gt; 113  0 0.00   1618
#&gt; 114  0 0.00   1545
#&gt; 115  0 0.00   1475
#&gt; 116  0 0.00   1409
#&gt; 117  0 0.00   1345
#&gt; 118  0 0.00   1284
#&gt; 119  0 0.00   1226
#&gt; 120  0 0.00   1170
#&gt; 121  0 0.00   1118
#&gt; 122  0 0.00   1067
#&gt; 123  0 0.00   1019
#&gt; 124  0 0.00    973
#&gt; 125  0 0.00    929
#&gt; 126  0 0.00    887
#&gt; 127  0 0.00    847
#&gt; 128  0 0.00    808
#&gt; 129  0 0.00    772
#&gt; 130  0 0.00    737
#&gt; 131  0 0.00    704
#&gt; 132  0 0.00    672
#&gt; 133  0 0.00    641
#&gt; 134  0 0.00    612
#&gt; 135  0 0.00    585
#&gt; 136  0 0.00    558
#&gt; 137  0 0.00    533
#&gt; 138  0 0.00    509
#&gt; 139  0 0.00    486
#&gt; 140  0 0.00    464
#&gt; 141  0 0.00    443
#&gt; 142  0 0.00    423
#&gt; 143  0 0.00    404
#&gt; 144  0 0.00    385
#&gt; 145  0 0.00    368
#&gt; 146  0 0.00    351
#&gt; 147  0 0.00    336
#&gt; 148  0 0.00    320
#&gt; 149  0 0.00    306
#&gt; 150  0 0.00    292
#&gt; 151  0 0.00    279
#&gt; 152  0 0.00    266
#&gt; 153  0 0.00    254
#&gt; 154  0 0.00    243
#&gt; 155  0 0.00    232
#&gt; 156  0 0.00    221
#&gt; 157  0 0.00    211
#&gt; 158  0 0.00    202
#&gt; 159  0 0.00    192
#&gt; 160  0 0.00    184
#&gt; 161  0 0.00    176
#&gt; 162  0 0.00    168
#&gt; 163  0 0.00    160
#&gt; 164  0 0.00    153
#&gt; 165  0 0.00    146
#&gt; 166  0 0.00    139
#&gt; 167  0 0.00    133
#&gt; 168  0 0.00    127
#&gt; 169  0 0.00    121
#&gt; 170  0 0.00    116
#&gt; 171  0 0.00    110
#&gt; 172  0 0.00    106
#&gt; 173  0 0.00    101
#&gt; 174  0 0.00     96
#&gt; 175  1 0.12     92
#&gt; 176  1 0.34     88
#&gt; 177  1 0.54     84
#&gt; 178  1 0.73     80
#&gt; 179  1 0.91     76
#&gt; 180  1 1.07     73
#&gt; 181  2 1.25     70
#&gt; 182  2 1.46     66
#&gt; 183  2 1.64     63
#&gt; 184  2 1.82     61
#&gt; 185  2 1.98     58
#&gt; 186  2 2.13     55
#&gt; 187  2 2.27     53
#&gt; 188  2 2.40     50
#&gt; 189  2 2.51     48
#&gt; 190  2 2.63     46
#&gt; 191  2 2.73     44
#&gt; 192  2 2.82     42
#&gt; 193  2 2.91     40
#&gt; 194  2 2.99     38
#&gt; 195  2 3.07     36
#&gt; 196  2 3.14     35
#&gt; 197  2 3.20     33
#&gt; 198  2 3.26     32
#&gt; 199  2 3.32     30
#&gt; 200  2 3.37     29
#&gt; 
#&gt; $`-0.04`
#&gt; 
#&gt; Call:  glmnet::glmnet(x = mm, y = as.factor(p), family = "binomial",      weights = weights, lambda = 10^(seq(4, 0, length.out = 200)) *          sum(reg)/length(reg) * sum(p)/sum(weights), standardize = F,      penalty.factor = reg) 
#&gt; 
#&gt;     Df  %Dev Lambda
#&gt; 1    0  0.00 383400
#&gt; 2    0  0.00 366100
#&gt; 3    0  0.00 349500
#&gt; 4    0  0.00 333700
#&gt; 5    0  0.00 318600
#&gt; 6    0  0.00 304200
#&gt; 7    0  0.00 290400
#&gt; 8    0  0.00 277300
#&gt; 9    0  0.00 264800
#&gt; 10   0  0.00 252800
#&gt; 11   0  0.00 241400
#&gt; 12   0  0.00 230400
#&gt; 13   0  0.00 220000
#&gt; 14   0  0.00 210100
#&gt; 15   0  0.00 200600
#&gt; 16   0  0.00 191500
#&gt; 17   0  0.00 182800
#&gt; 18   0  0.00 174600
#&gt; 19   0  0.00 166700
#&gt; 20   0  0.00 159100
#&gt; 21   0  0.00 151900
#&gt; 22   0  0.00 145100
#&gt; 23   0  0.00 138500
#&gt; 24   0  0.00 132200
#&gt; 25   0  0.00 126300
#&gt; 26   0  0.00 120500
#&gt; 27   0  0.00 115100
#&gt; 28   0  0.00 109900
#&gt; 29   0  0.00 104900
#&gt; 30   0  0.00 100200
#&gt; 31   0  0.00  95640
#&gt; 32   0  0.00  91320
#&gt; 33   0  0.00  87190
#&gt; 34   0  0.00  83240
#&gt; 35   0  0.00  79480
#&gt; 36   0  0.00  75880
#&gt; 37   0  0.00  72450
#&gt; 38   0  0.00  69180
#&gt; 39   0  0.00  66050
#&gt; 40   0  0.00  63060
#&gt; 41   0  0.00  60210
#&gt; 42   0  0.00  57480
#&gt; 43   0  0.00  54880
#&gt; 44   0  0.00  52400
#&gt; 45   0  0.00  50030
#&gt; 46   0  0.00  47770
#&gt; 47   0  0.00  45610
#&gt; 48   0  0.00  43550
#&gt; 49   0  0.00  41580
#&gt; 50   0  0.00  39700
#&gt; 51   0  0.00  37900
#&gt; 52   0  0.00  36190
#&gt; 53   0  0.00  34550
#&gt; 54   0  0.00  32990
#&gt; 55   0  0.00  31500
#&gt; 56   0  0.00  30070
#&gt; 57   0  0.00  28710
#&gt; 58   0  0.00  27410
#&gt; 59   0  0.00  26170
#&gt; 60   0  0.00  24990
#&gt; 61   0  0.00  23860
#&gt; 62   0  0.00  22780
#&gt; 63   0  0.00  21750
#&gt; 64   0  0.00  20770
#&gt; 65   0  0.00  19830
#&gt; 66   0  0.00  18930
#&gt; 67   0  0.00  18070
#&gt; 68   0  0.00  17260
#&gt; 69   0  0.00  16480
#&gt; 70   0  0.00  15730
#&gt; 71   0  0.00  15020
#&gt; 72   0  0.00  14340
#&gt; 73   0  0.00  13690
#&gt; 74   0  0.00  13070
#&gt; 75   0  0.00  12480
#&gt; 76   0  0.00  11920
#&gt; 77   0  0.00  11380
#&gt; 78   0  0.00  10860
#&gt; 79   0  0.00  10370
#&gt; 80   0  0.00   9902
#&gt; 81   0  0.00   9454
#&gt; 82   0  0.00   9027
#&gt; 83   0  0.00   8618
#&gt; 84   0  0.00   8229
#&gt; 85   0  0.00   7856
#&gt; 86   0  0.00   7501
#&gt; 87   0  0.00   7162
#&gt; 88   0  0.00   6838
#&gt; 89   0  0.00   6529
#&gt; 90   0  0.00   6233
#&gt; 91   0  0.00   5951
#&gt; 92   0  0.00   5682
#&gt; 93   0  0.00   5425
#&gt; 94   0  0.00   5180
#&gt; 95   0  0.00   4946
#&gt; 96   0  0.00   4722
#&gt; 97   0  0.00   4508
#&gt; 98   0  0.00   4304
#&gt; 99   0  0.00   4110
#&gt; 100  0  0.00   3924
#&gt; 101  0  0.00   3746
#&gt; 102  0  0.00   3577
#&gt; 103  0  0.00   3415
#&gt; 104  0  0.00   3261
#&gt; 105  0  0.00   3113
#&gt; 106  0  0.00   2972
#&gt; 107  0  0.00   2838
#&gt; 108  0  0.00   2710
#&gt; 109  0  0.00   2587
#&gt; 110  0  0.00   2470
#&gt; 111  0  0.00   2358
#&gt; 112  0  0.00   2252
#&gt; 113  0  0.00   2150
#&gt; 114  0  0.00   2053
#&gt; 115  0  0.00   1960
#&gt; 116  0  0.00   1871
#&gt; 117  0  0.00   1787
#&gt; 118  0  0.00   1706
#&gt; 119  0  0.00   1629
#&gt; 120  0  0.00   1555
#&gt; 121  0  0.00   1485
#&gt; 122  0  0.00   1417
#&gt; 123  0  0.00   1353
#&gt; 124  0  0.00   1292
#&gt; 125  0  0.00   1234
#&gt; 126  0  0.00   1178
#&gt; 127  0  0.00   1125
#&gt; 128  0  0.00   1074
#&gt; 129  0  0.00   1025
#&gt; 130  0  0.00    979
#&gt; 131  0  0.00    935
#&gt; 132  0  0.00    892
#&gt; 133  0  0.00    852
#&gt; 134  0  0.00    813
#&gt; 135  0  0.00    777
#&gt; 136  0  0.00    742
#&gt; 137  0  0.00    708
#&gt; 138  0  0.00    676
#&gt; 139  0  0.00    645
#&gt; 140  0  0.00    616
#&gt; 141  0  0.00    588
#&gt; 142  0  0.00    562
#&gt; 143  0  0.00    536
#&gt; 144  0  0.00    512
#&gt; 145  0  0.00    489
#&gt; 146  0  0.00    467
#&gt; 147  0  0.00    446
#&gt; 148  0  0.00    426
#&gt; 149  0  0.00    406
#&gt; 150  0  0.00    388
#&gt; 151  0  0.00    370
#&gt; 152  0  0.00    354
#&gt; 153  0  0.00    338
#&gt; 154  0  0.00    322
#&gt; 155  0  0.00    308
#&gt; 156  0  0.00    294
#&gt; 157  0  0.00    280
#&gt; 158  0  0.00    268
#&gt; 159  0  0.00    256
#&gt; 160  0  0.00    244
#&gt; 161  0  0.00    233
#&gt; 162  0  0.00    223
#&gt; 163  0  0.00    212
#&gt; 164  0  0.00    203
#&gt; 165  0  0.00    194
#&gt; 166  0  0.00    185
#&gt; 167  0  0.00    177
#&gt; 168  0  0.00    169
#&gt; 169  1  2.01    161
#&gt; 170  1  4.27    154
#&gt; 171  1  6.12    147
#&gt; 172  1  7.65    140
#&gt; 173  1  8.95    134
#&gt; 174  1 10.07    128
#&gt; 175  1 11.04    122
#&gt; 176  2 11.97    116
#&gt; 177  2 13.12    111
#&gt; 178  2 14.19    106
#&gt; 179  2 15.19    101
#&gt; 180  3 16.19     97
#&gt; 181  3 17.39     92
#&gt; 182  2 18.37     88
#&gt; 183  2 19.29     84
#&gt; 184  2 20.15     80
#&gt; 185  2 20.98     77
#&gt; 186  2 21.76     73
#&gt; 187  2 22.49     70
#&gt; 188  2 23.19     67
#&gt; 189  2 23.85     64
#&gt; 190  2 24.46     61
#&gt; 191  2 25.04     58
#&gt; 192  2 25.59     56
#&gt; 193  2 26.11     53
#&gt; 194  2 26.59     51
#&gt; 195  2 27.04     48
#&gt; 196  2 27.46     46
#&gt; 197  2 27.85     44
#&gt; 198  2 28.22     42
#&gt; 199  2 28.56     40
#&gt; 200  2 28.88     38
#&gt; 
#&gt; $`-0.52`
#&gt; 
#&gt; Call:  glmnet::glmnet(x = mm, y = as.factor(p), family = "binomial",      weights = weights, lambda = 10^(seq(4, 0, length.out = 200)) *          sum(reg)/length(reg) * sum(p)/sum(weights), standardize = F,      penalty.factor = reg) 
#&gt; 
#&gt;     Df %Dev  Lambda
#&gt; 1    0 0.00 1352.00
#&gt; 2    0 0.00 1291.00
#&gt; 3    0 0.00 1232.00
#&gt; 4    0 0.00 1176.00
#&gt; 5    0 0.00 1123.00
#&gt; 6    0 0.00 1072.00
#&gt; 7    0 0.00 1024.00
#&gt; 8    0 0.00  977.60
#&gt; 9    0 0.00  933.40
#&gt; 10   0 0.00  891.20
#&gt; 11   0 0.00  850.90
#&gt; 12   0 0.00  812.40
#&gt; 13   0 0.00  775.70
#&gt; 14   0 0.00  740.60
#&gt; 15   0 0.00  707.10
#&gt; 16   0 0.00  675.10
#&gt; 17   0 0.00  644.60
#&gt; 18   0 0.00  615.40
#&gt; 19   0 0.00  587.60
#&gt; 20   0 0.00  561.00
#&gt; 21   0 0.00  535.60
#&gt; 22   0 0.00  511.40
#&gt; 23   0 0.00  488.30
#&gt; 24   0 0.00  466.20
#&gt; 25   0 0.00  445.10
#&gt; 26   0 0.00  425.00
#&gt; 27   0 0.00  405.80
#&gt; 28   0 0.00  387.40
#&gt; 29   0 0.00  369.90
#&gt; 30   0 0.00  353.20
#&gt; 31   0 0.00  337.20
#&gt; 32   0 0.00  321.90
#&gt; 33   0 0.00  307.40
#&gt; 34   0 0.00  293.50
#&gt; 35   0 0.00  280.20
#&gt; 36   0 0.00  267.50
#&gt; 37   0 0.00  255.40
#&gt; 38   0 0.00  243.90
#&gt; 39   0 0.00  232.80
#&gt; 40   0 0.00  222.30
#&gt; 41   0 0.00  212.30
#&gt; 42   0 0.00  202.70
#&gt; 43   0 0.00  193.50
#&gt; 44   0 0.00  184.70
#&gt; 45   0 0.00  176.40
#&gt; 46   0 0.00  168.40
#&gt; 47   0 0.00  160.80
#&gt; 48   0 0.00  153.50
#&gt; 49   0 0.00  146.60
#&gt; 50   0 0.00  139.90
#&gt; 51   0 0.00  133.60
#&gt; 52   0 0.00  127.60
#&gt; 53   0 0.00  121.80
#&gt; 54   0 0.00  116.30
#&gt; 55   0 0.00  111.00
#&gt; 56   0 0.00  106.00
#&gt; 57   0 0.00  101.20
#&gt; 58   0 0.00   96.64
#&gt; 59   0 0.00   92.27
#&gt; 60   0 0.00   88.09
#&gt; 61   0 0.00   84.11
#&gt; 62   0 0.00   80.31
#&gt; 63   0 0.00   76.67
#&gt; 64   0 0.00   73.21
#&gt; 65   0 0.00   69.89
#&gt; 66   0 0.00   66.73
#&gt; 67   0 0.00   63.72
#&gt; 68   0 0.00   60.83
#&gt; 69   0 0.00   58.08
#&gt; 70   0 0.00   55.46
#&gt; 71   0 0.00   52.95
#&gt; 72   0 0.00   50.55
#&gt; 73   0 0.00   48.27
#&gt; 74   0 0.00   46.08
#&gt; 75   0 0.00   44.00
#&gt; 76   0 0.00   42.01
#&gt; 77   0 0.00   40.11
#&gt; 78   0 0.00   38.29
#&gt; 79   0 0.00   36.56
#&gt; 80   0 0.00   34.91
#&gt; 81   0 0.00   33.33
#&gt; 82   0 0.00   31.82
#&gt; 83   0 0.00   30.38
#&gt; 84   0 0.00   29.01
#&gt; 85   0 0.00   27.70
#&gt; 86   0 0.00   26.44
#&gt; 87   0 0.00   25.25
#&gt; 88   0 0.00   24.11
#&gt; 89   0 0.00   23.02
#&gt; 90   0 0.00   21.98
#&gt; 91   0 0.00   20.98
#&gt; 92   0 0.00   20.03
#&gt; 93   0 0.00   19.13
#&gt; 94   0 0.00   18.26
#&gt; 95   0 0.00   17.44
#&gt; 96   0 0.00   16.65
#&gt; 97   0 0.00   15.89
#&gt; 98   0 0.00   15.17
#&gt; 99   0 0.00   14.49
#&gt; 100  0 0.00   13.83
#&gt; 101  0 0.00   13.21
#&gt; 102  0 0.00   12.61
#&gt; 103  0 0.00   12.04
#&gt; 104  0 0.00   11.50
#&gt; 105  0 0.00   10.98
#&gt; 106  0 0.00   10.48
#&gt; 107  0 0.00   10.01
#&gt; 108  0 0.00    9.55
#&gt; 109  0 0.00    9.12
#&gt; 110  0 0.00    8.71
#&gt; 111  0 0.00    8.31
#&gt; 112  0 0.00    7.94
#&gt; 113  0 0.00    7.58
#&gt; 114  0 0.00    7.24
#&gt; 115  0 0.00    6.91
#&gt; 116  0 0.00    6.60
#&gt; 117  0 0.00    6.30
#&gt; 118  0 0.00    6.01
#&gt; 119  0 0.00    5.74
#&gt; 120  0 0.00    5.48
#&gt; 121  0 0.00    5.23
#&gt; 122  0 0.00    5.00
#&gt; 123  0 0.00    4.77
#&gt; 124  0 0.00    4.56
#&gt; 125  0 0.00    4.35
#&gt; 126  0 0.00    4.15
#&gt; 127  0 0.00    3.96
#&gt; 128  0 0.00    3.79
#&gt; 129  0 0.00    3.61
#&gt; 130  0 0.00    3.45
#&gt; 131  0 0.00    3.30
#&gt; 132  0 0.00    3.15
#&gt; 133  0 0.00    3.00
#&gt; 134  0 0.00    2.87
#&gt; 135  0 0.00    2.74
#&gt; 136  0 0.00    2.61
#&gt; 137  0 0.00    2.50
#&gt; 138  0 0.00    2.38
#&gt; 139  0 0.00    2.28
#&gt; 140  0 0.00    2.17
#&gt; 141  0 0.00    2.07
#&gt; 142  0 0.00    1.98
#&gt; 143  0 0.00    1.89
#&gt; 144  0 0.00    1.80
#&gt; 145  0 0.00    1.72
#&gt; 146  0 0.00    1.65
#&gt; 147  0 0.00    1.57
#&gt; 148  0 0.00    1.50
#&gt; 149  0 0.00    1.43
#&gt; 150  0 0.00    1.37
#&gt; 151  0 0.00    1.31
#&gt; 152  0 0.00    1.25
#&gt; 153  0 0.00    1.19
#&gt; 154  0 0.00    1.14
#&gt; 155  0 0.00    1.08
#&gt; 156  0 0.00    1.04
#&gt; 157  0 0.00    0.99
#&gt; 158  0 0.00    0.94
#&gt; 159  0 0.00    0.90
#&gt; 160  0 0.00    0.86
#&gt; 161  1 0.05    0.82
#&gt; 162  1 0.22    0.78
#&gt; 163  1 0.38    0.75
#&gt; 164  1 0.54    0.72
#&gt; 165  1 0.69    0.68
#&gt; 166  1 0.83    0.65
#&gt; 167  1 0.97    0.62
#&gt; 168  1 1.10    0.59
#&gt; 169  1 1.23    0.57
#&gt; 170  1 1.35    0.54
#&gt; 171  1 1.47    0.52
#&gt; 172  1 1.58    0.49
#&gt; 173  1 1.69    0.47
#&gt; 174  1 1.80    0.45
#&gt; 175  1 1.90    0.43
#&gt; 176  1 1.99    0.41
#&gt; 177  1 2.09    0.39
#&gt; 178  1 2.18    0.37
#&gt; 179  1 2.26    0.36
#&gt; 180  1 2.35    0.34
#&gt; 181  1 2.43    0.33
#&gt; 182  1 2.50    0.31
#&gt; 183  1 2.58    0.30
#&gt; 184  1 2.65    0.28
#&gt; 185  1 2.72    0.27
#&gt; 186  1 2.78    0.26
#&gt; 187  1 2.85    0.25
#&gt; 188  1 2.91    0.24
#&gt; 189  1 2.97    0.22
#&gt; 190  1 3.03    0.21
#&gt; 191  1 3.08    0.20
#&gt; 192  1 3.13    0.20
#&gt; 193  1 3.18    0.19
#&gt; 194  2 3.26    0.18
#&gt; 195  2 3.38    0.17
#&gt; 196  2 3.49    0.16
#&gt; 197  2 3.58    0.16
#&gt; 198  2 3.67    0.15
#&gt; 199  2 3.75    0.14
#&gt; 200  2 3.83    0.14
#&gt; 
#&gt; $`-0.2`
#&gt; 
#&gt; Call:  glmnet::glmnet(x = mm, y = as.factor(p), family = "binomial",      weights = weights, lambda = 10^(seq(4, 0, length.out = 200)) *          sum(reg)/length(reg) * sum(p)/sum(weights), standardize = F,      penalty.factor = reg) 
#&gt; 
#&gt;     Df %Dev  Lambda
#&gt; 1    0 0.00 1256.00
#&gt; 2    0 0.00 1199.00
#&gt; 3    0 0.00 1145.00
#&gt; 4    0 0.00 1093.00
#&gt; 5    0 0.00 1043.00
#&gt; 6    0 0.00  996.30
#&gt; 7    0 0.00  951.20
#&gt; 8    0 0.00  908.20
#&gt; 9    0 0.00  867.10
#&gt; 10   0 0.00  827.90
#&gt; 11   0 0.00  790.50
#&gt; 12   0 0.00  754.70
#&gt; 13   0 0.00  720.60
#&gt; 14   0 0.00  688.00
#&gt; 15   0 0.00  656.90
#&gt; 16   0 0.00  627.20
#&gt; 17   0 0.00  598.80
#&gt; 18   0 0.00  571.70
#&gt; 19   0 0.00  545.90
#&gt; 20   0 0.00  521.20
#&gt; 21   0 0.00  497.60
#&gt; 22   0 0.00  475.10
#&gt; 23   0 0.00  453.60
#&gt; 24   0 0.00  433.10
#&gt; 25   0 0.00  413.50
#&gt; 26   0 0.00  394.80
#&gt; 27   0 0.00  376.90
#&gt; 28   0 0.00  359.90
#&gt; 29   0 0.00  343.60
#&gt; 30   0 0.00  328.10
#&gt; 31   0 0.00  313.20
#&gt; 32   0 0.00  299.10
#&gt; 33   0 0.00  285.50
#&gt; 34   0 0.00  272.60
#&gt; 35   0 0.00  260.30
#&gt; 36   0 0.00  248.50
#&gt; 37   0 0.00  237.30
#&gt; 38   0 0.00  226.60
#&gt; 39   0 0.00  216.30
#&gt; 40   0 0.00  206.50
#&gt; 41   0 0.00  197.20
#&gt; 42   0 0.00  188.30
#&gt; 43   0 0.00  179.70
#&gt; 44   0 0.00  171.60
#&gt; 45   0 0.00  163.90
#&gt; 46   0 0.00  156.40
#&gt; 47   0 0.00  149.40
#&gt; 48   0 0.00  142.60
#&gt; 49   0 0.00  136.20
#&gt; 50   0 0.00  130.00
#&gt; 51   0 0.00  124.10
#&gt; 52   0 0.00  118.50
#&gt; 53   0 0.00  113.20
#&gt; 54   0 0.00  108.00
#&gt; 55   0 0.00  103.10
#&gt; 56   0 0.00   98.48
#&gt; 57   0 0.00   94.03
#&gt; 58   0 0.00   89.78
#&gt; 59   0 0.00   85.71
#&gt; 60   0 0.00   81.84
#&gt; 61   0 0.00   78.14
#&gt; 62   0 0.00   74.60
#&gt; 63   0 0.00   71.23
#&gt; 64   0 0.00   68.01
#&gt; 65   0 0.00   64.93
#&gt; 66   0 0.00   61.99
#&gt; 67   0 0.00   59.19
#&gt; 68   0 0.00   56.51
#&gt; 69   0 0.00   53.96
#&gt; 70   0 0.00   51.52
#&gt; 71   0 0.00   49.19
#&gt; 72   0 0.00   46.96
#&gt; 73   0 0.00   44.84
#&gt; 74   0 0.00   42.81
#&gt; 75   0 0.00   40.87
#&gt; 76   0 0.00   39.03
#&gt; 77   0 0.00   37.26
#&gt; 78   0 0.00   35.58
#&gt; 79   0 0.00   33.97
#&gt; 80   0 0.00   32.43
#&gt; 81   0 0.00   30.96
#&gt; 82   0 0.00   29.56
#&gt; 83   0 0.00   28.23
#&gt; 84   0 0.00   26.95
#&gt; 85   0 0.00   25.73
#&gt; 86   0 0.00   24.57
#&gt; 87   0 0.00   23.46
#&gt; 88   0 0.00   22.39
#&gt; 89   0 0.00   21.38
#&gt; 90   0 0.00   20.41
#&gt; 91   0 0.00   19.49
#&gt; 92   0 0.00   18.61
#&gt; 93   0 0.00   17.77
#&gt; 94   0 0.00   16.96
#&gt; 95   0 0.00   16.20
#&gt; 96   0 0.00   15.46
#&gt; 97   0 0.00   14.77
#&gt; 98   0 0.00   14.10
#&gt; 99   0 0.00   13.46
#&gt; 100  0 0.00   12.85
#&gt; 101  0 0.00   12.27
#&gt; 102  0 0.00   11.71
#&gt; 103  0 0.00   11.18
#&gt; 104  0 0.00   10.68
#&gt; 105  0 0.00   10.20
#&gt; 106  0 0.00    9.73
#&gt; 107  0 0.00    9.30
#&gt; 108  0 0.00    8.87
#&gt; 109  0 0.00    8.47
#&gt; 110  0 0.00    8.09
#&gt; 111  0 0.00    7.72
#&gt; 112  0 0.00    7.37
#&gt; 113  0 0.00    7.04
#&gt; 114  0 0.00    6.72
#&gt; 115  0 0.00    6.42
#&gt; 116  0 0.00    6.13
#&gt; 117  0 0.00    5.85
#&gt; 118  0 0.00    5.59
#&gt; 119  0 0.00    5.33
#&gt; 120  0 0.00    5.09
#&gt; 121  0 0.00    4.86
#&gt; 122  0 0.00    4.64
#&gt; 123  0 0.00    4.43
#&gt; 124  0 0.00    4.23
#&gt; 125  0 0.00    4.04
#&gt; 126  0 0.00    3.86
#&gt; 127  0 0.00    3.68
#&gt; 128  0 0.00    3.52
#&gt; 129  0 0.00    3.36
#&gt; 130  0 0.00    3.21
#&gt; 131  0 0.00    3.06
#&gt; 132  0 0.00    2.92
#&gt; 133  0 0.00    2.79
#&gt; 134  0 0.00    2.66
#&gt; 135  0 0.00    2.54
#&gt; 136  0 0.00    2.43
#&gt; 137  0 0.00    2.32
#&gt; 138  0 0.00    2.21
#&gt; 139  0 0.00    2.11
#&gt; 140  0 0.00    2.02
#&gt; 141  0 0.00    1.93
#&gt; 142  0 0.00    1.84
#&gt; 143  0 0.00    1.76
#&gt; 144  0 0.00    1.68
#&gt; 145  0 0.00    1.60
#&gt; 146  0 0.00    1.53
#&gt; 147  0 0.00    1.46
#&gt; 148  0 0.00    1.39
#&gt; 149  0 0.00    1.33
#&gt; 150  0 0.00    1.27
#&gt; 151  0 0.00    1.21
#&gt; 152  0 0.00    1.16
#&gt; 153  0 0.00    1.11
#&gt; 154  0 0.00    1.06
#&gt; 155  0 0.00    1.01
#&gt; 156  0 0.00    0.96
#&gt; 157  0 0.00    0.92
#&gt; 158  0 0.00    0.88
#&gt; 159  0 0.00    0.84
#&gt; 160  0 0.00    0.80
#&gt; 161  1 0.05    0.76
#&gt; 162  1 0.22    0.73
#&gt; 163  1 0.38    0.70
#&gt; 164  1 0.54    0.66
#&gt; 165  1 0.69    0.63
#&gt; 166  1 0.83    0.61
#&gt; 167  1 0.97    0.58
#&gt; 168  1 1.10    0.55
#&gt; 169  1 1.23    0.53
#&gt; 170  1 1.35    0.50
#&gt; 171  1 1.47    0.48
#&gt; 172  1 1.58    0.46
#&gt; 173  1 1.69    0.44
#&gt; 174  1 1.80    0.42
#&gt; 175  1 1.90    0.40
#&gt; 176  1 1.99    0.38
#&gt; 177  2 2.22    0.36
#&gt; 178  2 2.45    0.35
#&gt; 179  2 2.66    0.33
#&gt; 180  2 2.86    0.32
#&gt; 181  2 3.05    0.30
#&gt; 182  2 3.23    0.29
#&gt; 183  2 3.39    0.28
#&gt; 184  2 3.54    0.26
#&gt; 185  2 3.69    0.25
#&gt; 186  2 3.82    0.24
#&gt; 187  2 3.95    0.23
#&gt; 188  2 4.07    0.22
#&gt; 189  2 4.18    0.21
#&gt; 190  2 4.28    0.20
#&gt; 191  2 4.38    0.19
#&gt; 192  2 4.47    0.18
#&gt; 193  2 4.55    0.17
#&gt; 194  2 4.63    0.17
#&gt; 195  2 4.70    0.16
#&gt; 196  2 4.77    0.15
#&gt; 197  2 4.84    0.14
#&gt; 198  2 4.89    0.14
#&gt; 199  2 4.95    0.13
#&gt; 200  3 5.01    0.13
#&gt; 
#&gt; $`-0.08`
#&gt; 
#&gt; Call:  glmnet::glmnet(x = mm, y = as.factor(p), family = "binomial",      weights = weights, lambda = 10^(seq(4, 0, length.out = 200)) *          sum(reg)/length(reg) * sum(p)/sum(weights), standardize = F,      penalty.factor = reg) 
#&gt; 
#&gt;     Df  %Dev Lambda
#&gt; 1    0  0.00  96060
#&gt; 2    0  0.00  91710
#&gt; 3    0  0.00  87560
#&gt; 4    0  0.00  83600
#&gt; 5    0  0.00  79820
#&gt; 6    0  0.00  76210
#&gt; 7    0  0.00  72770
#&gt; 8    0  0.00  69470
#&gt; 9    0  0.00  66330
#&gt; 10   0  0.00  63330
#&gt; 11   0  0.00  60470
#&gt; 12   0  0.00  57730
#&gt; 13   0  0.00  55120
#&gt; 14   0  0.00  52630
#&gt; 15   0  0.00  50250
#&gt; 16   0  0.00  47980
#&gt; 17   0  0.00  45810
#&gt; 18   0  0.00  43730
#&gt; 19   0  0.00  41760
#&gt; 20   0  0.00  39870
#&gt; 21   0  0.00  38060
#&gt; 22   0  0.00  36340
#&gt; 23   0  0.00  34700
#&gt; 24   0  0.00  33130
#&gt; 25   0  0.00  31630
#&gt; 26   0  0.00  30200
#&gt; 27   0  0.00  28830
#&gt; 28   0  0.00  27530
#&gt; 29   0  0.00  26290
#&gt; 30   0  0.00  25100
#&gt; 31   0  0.00  23960
#&gt; 32   0  0.00  22880
#&gt; 33   0  0.00  21840
#&gt; 34   0  0.00  20860
#&gt; 35   0  0.00  19910
#&gt; 36   0  0.00  19010
#&gt; 37   0  0.00  18150
#&gt; 38   0  0.00  17330
#&gt; 39   0  0.00  16550
#&gt; 40   0  0.00  15800
#&gt; 41   0  0.00  15080
#&gt; 42   0  0.00  14400
#&gt; 43   0  0.00  13750
#&gt; 44   0  0.00  13130
#&gt; 45   0  0.00  12530
#&gt; 46   0  0.00  11970
#&gt; 47   0  0.00  11430
#&gt; 48   0  0.00  10910
#&gt; 49   0  0.00  10420
#&gt; 50   0  0.00   9945
#&gt; 51   0  0.00   9495
#&gt; 52   0  0.00   9066
#&gt; 53   0  0.00   8656
#&gt; 54   0  0.00   8264
#&gt; 55   0  0.00   7890
#&gt; 56   0  0.00   7534
#&gt; 57   0  0.00   7193
#&gt; 58   0  0.00   6868
#&gt; 59   0  0.00   6557
#&gt; 60   0  0.00   6260
#&gt; 61   0  0.00   5977
#&gt; 62   0  0.00   5707
#&gt; 63   0  0.00   5449
#&gt; 64   0  0.00   5202
#&gt; 65   0  0.00   4967
#&gt; 66   0  0.00   4742
#&gt; 67   0  0.00   4528
#&gt; 68   0  0.00   4323
#&gt; 69   0  0.00   4128
#&gt; 70   0  0.00   3941
#&gt; 71   0  0.00   3763
#&gt; 72   0  0.00   3592
#&gt; 73   0  0.00   3430
#&gt; 74   0  0.00   3275
#&gt; 75   0  0.00   3127
#&gt; 76   0  0.00   2985
#&gt; 77   0  0.00   2850
#&gt; 78   0  0.00   2721
#&gt; 79   0  0.00   2598
#&gt; 80   0  0.00   2481
#&gt; 81   0  0.00   2369
#&gt; 82   0  0.00   2261
#&gt; 83   0  0.00   2159
#&gt; 84   0  0.00   2062
#&gt; 85   0  0.00   1968
#&gt; 86   0  0.00   1879
#&gt; 87   0  0.00   1794
#&gt; 88   0  0.00   1713
#&gt; 89   0  0.00   1636
#&gt; 90   0  0.00   1562
#&gt; 91   0  0.00   1491
#&gt; 92   0  0.00   1424
#&gt; 93   0  0.00   1359
#&gt; 94   0  0.00   1298
#&gt; 95   0  0.00   1239
#&gt; 96   0  0.00   1183
#&gt; 97   0  0.00   1129
#&gt; 98   0  0.00   1078
#&gt; 99   0  0.00   1030
#&gt; 100  0  0.00    983
#&gt; 101  0  0.00    939
#&gt; 102  0  0.00    896
#&gt; 103  0  0.00    856
#&gt; 104  0  0.00    817
#&gt; 105  0  0.00    780
#&gt; 106  0  0.00    745
#&gt; 107  0  0.00    711
#&gt; 108  0  0.00    679
#&gt; 109  0  0.00    648
#&gt; 110  0  0.00    619
#&gt; 111  0  0.00    591
#&gt; 112  0  0.00    564
#&gt; 113  0  0.00    539
#&gt; 114  0  0.00    514
#&gt; 115  0  0.00    491
#&gt; 116  0  0.00    469
#&gt; 117  0  0.00    448
#&gt; 118  0  0.00    427
#&gt; 119  0  0.00    408
#&gt; 120  0  0.00    390
#&gt; 121  0  0.00    372
#&gt; 122  0  0.00    355
#&gt; 123  0  0.00    339
#&gt; 124  0  0.00    324
#&gt; 125  0  0.00    309
#&gt; 126  0  0.00    295
#&gt; 127  0  0.00    282
#&gt; 128  0  0.00    269
#&gt; 129  0  0.00    257
#&gt; 130  0  0.00    245
#&gt; 131  0  0.00    234
#&gt; 132  0  0.00    224
#&gt; 133  0  0.00    213
#&gt; 134  0  0.00    204
#&gt; 135  0  0.00    195
#&gt; 136  0  0.00    186
#&gt; 137  0  0.00    177
#&gt; 138  0  0.00    169
#&gt; 139  0  0.00    162
#&gt; 140  0  0.00    154
#&gt; 141  0  0.00    147
#&gt; 142  0  0.00    141
#&gt; 143  0  0.00    134
#&gt; 144  0  0.00    128
#&gt; 145  0  0.00    122
#&gt; 146  0  0.00    117
#&gt; 147  0  0.00    112
#&gt; 148  0  0.00    107
#&gt; 149  0  0.00    102
#&gt; 150  0  0.00     97
#&gt; 151  0  0.00     93
#&gt; 152  0  0.00     89
#&gt; 153  0  0.00     85
#&gt; 154  0  0.00     81
#&gt; 155  0  0.00     77
#&gt; 156  0  0.00     74
#&gt; 157  0  0.00     70
#&gt; 158  0  0.00     67
#&gt; 159  0  0.00     64
#&gt; 160  0  0.00     61
#&gt; 161  1  0.05     58
#&gt; 162  1  0.22     56
#&gt; 163  1  0.38     53
#&gt; 164  1  0.54     51
#&gt; 165  1  0.69     49
#&gt; 166  1  0.83     46
#&gt; 167  1  0.97     44
#&gt; 168  1  1.10     42
#&gt; 169  2  2.65     40
#&gt; 170  2  4.94     39
#&gt; 171  2  6.82     37
#&gt; 172  2  8.39     35
#&gt; 173  2  9.73     34
#&gt; 174  2 10.89     32
#&gt; 175  2 11.90     31
#&gt; 176  2 12.79     29
#&gt; 177  2 13.58     28
#&gt; 178  2 14.29     27
#&gt; 179  2 14.93     25
#&gt; 180  2 15.50     24
#&gt; 181  2 16.03     23
#&gt; 182  2 16.51     22
#&gt; 183  3 17.48     21
#&gt; 184  2 18.54     20
#&gt; 185  2 19.06     19
#&gt; 186  2 19.57     18
#&gt; 187  2 20.05     18
#&gt; 188  2 20.52     17
#&gt; 189  2 20.97     16
#&gt; 190  2 21.40     15
#&gt; 191  2 21.83     15
#&gt; 192  2 22.24     14
#&gt; 193  2 22.65     13
#&gt; 194  2 23.04     13
#&gt; 195  2 23.43     12
#&gt; 196  2 23.81     12
#&gt; 197  2 24.18     11
#&gt; 198  2 24.54     11
#&gt; 199  2 24.89     10
#&gt; 200  2 25.23     10
#&gt; 
#&gt; $`0.2`
#&gt; 
#&gt; Call:  glmnet::glmnet(x = mm, y = as.factor(p), family = "binomial",      weights = weights, lambda = 10^(seq(4, 0, length.out = 200)) *          sum(reg)/length(reg) * sum(p)/sum(weights), standardize = F,      penalty.factor = reg) 
#&gt; 
#&gt;     Df %Dev Lambda
#&gt; 1    0 0.00 96.500
#&gt; 2    0 0.00 92.130
#&gt; 3    0 0.00 87.970
#&gt; 4    0 0.00 83.990
#&gt; 5    0 0.00 80.190
#&gt; 6    0 0.00 76.560
#&gt; 7    0 0.00 73.100
#&gt; 8    0 0.00 69.790
#&gt; 9    0 0.00 66.640
#&gt; 10   0 0.00 63.620
#&gt; 11   0 0.00 60.750
#&gt; 12   0 0.00 58.000
#&gt; 13   0 0.00 55.380
#&gt; 14   0 0.00 52.870
#&gt; 15   0 0.00 50.480
#&gt; 16   0 0.00 48.200
#&gt; 17   0 0.00 46.020
#&gt; 18   0 0.00 43.940
#&gt; 19   0 0.00 41.950
#&gt; 20   0 0.00 40.050
#&gt; 21   0 0.00 38.240
#&gt; 22   0 0.00 36.510
#&gt; 23   0 0.00 34.860
#&gt; 24   0 0.00 33.280
#&gt; 25   0 0.00 31.780
#&gt; 26   0 0.00 30.340
#&gt; 27   0 0.00 28.970
#&gt; 28   0 0.00 27.660
#&gt; 29   0 0.00 26.410
#&gt; 30   0 0.00 25.210
#&gt; 31   0 0.00 24.070
#&gt; 32   0 0.00 22.980
#&gt; 33   0 0.00 21.940
#&gt; 34   0 0.00 20.950
#&gt; 35   0 0.00 20.000
#&gt; 36   0 0.00 19.100
#&gt; 37   0 0.00 18.230
#&gt; 38   0 0.00 17.410
#&gt; 39   0 0.00 16.620
#&gt; 40   0 0.00 15.870
#&gt; 41   0 0.00 15.150
#&gt; 42   0 0.00 14.470
#&gt; 43   0 0.00 13.810
#&gt; 44   0 0.00 13.190
#&gt; 45   0 0.00 12.590
#&gt; 46   0 0.00 12.020
#&gt; 47   0 0.00 11.480
#&gt; 48   0 0.00 10.960
#&gt; 49   0 0.00 10.460
#&gt; 50   0 0.00  9.991
#&gt; 51   0 0.00  9.539
#&gt; 52   0 0.00  9.107
#&gt; 53   0 0.00  8.696
#&gt; 54   0 0.00  8.302
#&gt; 55   0 0.00  7.927
#&gt; 56   0 0.00  7.568
#&gt; 57   0 0.00  7.226
#&gt; 58   0 0.00  6.899
#&gt; 59   0 0.00  6.587
#&gt; 60   0 0.00  6.289
#&gt; 61   0 0.00  6.005
#&gt; 62   0 0.00  5.733
#&gt; 63   0 0.00  5.474
#&gt; 64   0 0.00  5.226
#&gt; 65   0 0.00  4.990
#&gt; 66   0 0.00  4.764
#&gt; 67   0 0.00  4.549
#&gt; 68   0 0.00  4.343
#&gt; 69   0 0.00  4.147
#&gt; 70   0 0.00  3.959
#&gt; 71   0 0.00  3.780
#&gt; 72   0 0.00  3.609
#&gt; 73   0 0.00  3.446
#&gt; 74   0 0.00  3.290
#&gt; 75   0 0.00  3.141
#&gt; 76   0 0.00  2.999
#&gt; 77   0 0.00  2.863
#&gt; 78   0 0.00  2.734
#&gt; 79   0 0.00  2.610
#&gt; 80   0 0.00  2.492
#&gt; 81   0 0.00  2.379
#&gt; 82   0 0.00  2.272
#&gt; 83   0 0.00  2.169
#&gt; 84   0 0.00  2.071
#&gt; 85   0 0.00  1.977
#&gt; 86   0 0.00  1.888
#&gt; 87   0 0.00  1.803
#&gt; 88   0 0.00  1.721
#&gt; 89   0 0.00  1.643
#&gt; 90   0 0.00  1.569
#&gt; 91   0 0.00  1.498
#&gt; 92   0 0.00  1.430
#&gt; 93   0 0.00  1.365
#&gt; 94   0 0.00  1.304
#&gt; 95   0 0.00  1.245
#&gt; 96   0 0.00  1.188
#&gt; 97   0 0.00  1.135
#&gt; 98   0 0.00  1.083
#&gt; 99   0 0.00  1.034
#&gt; 100  0 0.00  0.988
#&gt; 101  0 0.00  0.943
#&gt; 102  0 0.00  0.900
#&gt; 103  0 0.00  0.860
#&gt; 104  0 0.00  0.821
#&gt; 105  0 0.00  0.784
#&gt; 106  0 0.00  0.748
#&gt; 107  0 0.00  0.714
#&gt; 108  0 0.00  0.682
#&gt; 109  0 0.00  0.651
#&gt; 110  0 0.00  0.622
#&gt; 111  0 0.00  0.594
#&gt; 112  0 0.00  0.567
#&gt; 113  0 0.00  0.541
#&gt; 114  0 0.00  0.517
#&gt; 115  0 0.00  0.493
#&gt; 116  0 0.00  0.471
#&gt; 117  0 0.00  0.450
#&gt; 118  0 0.00  0.429
#&gt; 119  0 0.00  0.410
#&gt; 120  0 0.00  0.391
#&gt; 121  0 0.00  0.374
#&gt; 122  0 0.00  0.357
#&gt; 123  0 0.00  0.341
#&gt; 124  0 0.00  0.325
#&gt; 125  0 0.00  0.310
#&gt; 126  0 0.00  0.296
#&gt; 127  0 0.00  0.283
#&gt; 128  0 0.00  0.270
#&gt; 129  0 0.00  0.258
#&gt; 130  0 0.00  0.246
#&gt; 131  0 0.00  0.235
#&gt; 132  0 0.00  0.225
#&gt; 133  0 0.00  0.214
#&gt; 134  0 0.00  0.205
#&gt; 135  0 0.00  0.196
#&gt; 136  0 0.00  0.187
#&gt; 137  0 0.00  0.178
#&gt; 138  0 0.00  0.170
#&gt; 139  0 0.00  0.162
#&gt; 140  0 0.00  0.155
#&gt; 141  0 0.00  0.148
#&gt; 142  0 0.00  0.141
#&gt; 143  0 0.00  0.135
#&gt; 144  0 0.00  0.129
#&gt; 145  0 0.00  0.123
#&gt; 146  0 0.00  0.117
#&gt; 147  0 0.00  0.112
#&gt; 148  0 0.00  0.107
#&gt; 149  0 0.00  0.102
#&gt; 150  0 0.00  0.098
#&gt; 151  0 0.00  0.093
#&gt; 152  0 0.00  0.089
#&gt; 153  0 0.00  0.085
#&gt; 154  0 0.00  0.081
#&gt; 155  0 0.00  0.077
#&gt; 156  0 0.00  0.074
#&gt; 157  0 0.00  0.071
#&gt; 158  0 0.00  0.067
#&gt; 159  0 0.00  0.064
#&gt; 160  0 0.00  0.061
#&gt; 161  0 0.00  0.059
#&gt; 162  0 0.00  0.056
#&gt; 163  0 0.00  0.053
#&gt; 164  0 0.00  0.051
#&gt; 165  0 0.00  0.049
#&gt; 166  0 0.00  0.047
#&gt; 167  0 0.00  0.044
#&gt; 168  0 0.00  0.042
#&gt; 169  0 0.00  0.041
#&gt; 170  0 0.00  0.039
#&gt; 171  0 0.00  0.037
#&gt; 172  0 0.00  0.035
#&gt; 173  0 0.00  0.034
#&gt; 174  0 0.00  0.032
#&gt; 175  1 0.12  0.031
#&gt; 176  1 0.34  0.029
#&gt; 177  1 0.54  0.028
#&gt; 178  1 0.73  0.027
#&gt; 179  1 0.91  0.026
#&gt; 180  1 1.07  0.024
#&gt; 181  1 1.22  0.023
#&gt; 182  1 1.37  0.022
#&gt; 183  1 1.50  0.021
#&gt; 184  1 1.62  0.020
#&gt; 185  1 1.74  0.019
#&gt; 186  1 1.85  0.018
#&gt; 187  1 1.94  0.018
#&gt; 188  1 2.04  0.017
#&gt; 189  1 2.12  0.016
#&gt; 190  1 2.20  0.015
#&gt; 191  1 2.28  0.015
#&gt; 192  1 2.34  0.014
#&gt; 193  1 2.41  0.013
#&gt; 194  1 2.47  0.013
#&gt; 195  1 2.52  0.012
#&gt; 196  1 2.57  0.012
#&gt; 197  1 2.62  0.011
#&gt; 198  1 2.66  0.011
#&gt; 199  1 2.70  0.010
#&gt; 200  1 2.73  0.010
#&gt; 
#&gt; $`0.4`
#&gt; 
#&gt; Call:  glmnet::glmnet(x = mm, y = as.factor(p), family = "binomial",      weights = weights, lambda = 10^(seq(4, 0, length.out = 200)) *          sum(reg)/length(reg) * sum(p)/sum(weights), standardize = F,      penalty.factor = reg) 
#&gt; 
#&gt;     Df  %Dev Lambda
#&gt; 1    0  0.00  94900
#&gt; 2    0  0.00  90610
#&gt; 3    0  0.00  86510
#&gt; 4    0  0.00  82600
#&gt; 5    0  0.00  78860
#&gt; 6    0  0.00  75290
#&gt; 7    0  0.00  71890
#&gt; 8    0  0.00  68640
#&gt; 9    0  0.00  65530
#&gt; 10   0  0.00  62570
#&gt; 11   0  0.00  59740
#&gt; 12   0  0.00  57040
#&gt; 13   0  0.00  54460
#&gt; 14   0  0.00  51990
#&gt; 15   0  0.00  49640
#&gt; 16   0  0.00  47400
#&gt; 17   0  0.00  45250
#&gt; 18   0  0.00  43210
#&gt; 19   0  0.00  41250
#&gt; 20   0  0.00  39390
#&gt; 21   0  0.00  37610
#&gt; 22   0  0.00  35900
#&gt; 23   0  0.00  34280
#&gt; 24   0  0.00  32730
#&gt; 25   0  0.00  31250
#&gt; 26   0  0.00  29840
#&gt; 27   0  0.00  28490
#&gt; 28   0  0.00  27200
#&gt; 29   0  0.00  25970
#&gt; 30   0  0.00  24790
#&gt; 31   0  0.00  23670
#&gt; 32   0  0.00  22600
#&gt; 33   0  0.00  21580
#&gt; 34   0  0.00  20600
#&gt; 35   0  0.00  19670
#&gt; 36   0  0.00  18780
#&gt; 37   0  0.00  17930
#&gt; 38   0  0.00  17120
#&gt; 39   0  0.00  16350
#&gt; 40   0  0.00  15610
#&gt; 41   0  0.00  14900
#&gt; 42   0  0.00  14230
#&gt; 43   0  0.00  13580
#&gt; 44   0  0.00  12970
#&gt; 45   0  0.00  12380
#&gt; 46   0  0.00  11820
#&gt; 47   0  0.00  11290
#&gt; 48   0  0.00  10780
#&gt; 49   0  0.00  10290
#&gt; 50   0  0.00   9825
#&gt; 51   0  0.00   9381
#&gt; 52   0  0.00   8956
#&gt; 53   0  0.00   8551
#&gt; 54   0  0.00   8165
#&gt; 55   0  0.00   7795
#&gt; 56   0  0.00   7443
#&gt; 57   0  0.00   7106
#&gt; 58   0  0.00   6785
#&gt; 59   0  0.00   6478
#&gt; 60   0  0.00   6185
#&gt; 61   0  0.00   5905
#&gt; 62   0  0.00   5638
#&gt; 63   0  0.00   5383
#&gt; 64   0  0.00   5140
#&gt; 65   0  0.00   4907
#&gt; 66   0  0.00   4685
#&gt; 67   0  0.00   4473
#&gt; 68   0  0.00   4271
#&gt; 69   0  0.00   4078
#&gt; 70   0  0.00   3893
#&gt; 71   0  0.00   3717
#&gt; 72   0  0.00   3549
#&gt; 73   0  0.00   3389
#&gt; 74   0  0.00   3235
#&gt; 75   0  0.00   3089
#&gt; 76   0  0.00   2949
#&gt; 77   0  0.00   2816
#&gt; 78   0  0.00   2689
#&gt; 79   0  0.00   2567
#&gt; 80   0  0.00   2451
#&gt; 81   0  0.00   2340
#&gt; 82   0  0.00   2234
#&gt; 83   0  0.00   2133
#&gt; 84   0  0.00   2037
#&gt; 85   0  0.00   1945
#&gt; 86   0  0.00   1857
#&gt; 87   0  0.00   1773
#&gt; 88   0  0.00   1692
#&gt; 89   0  0.00   1616
#&gt; 90   0  0.00   1543
#&gt; 91   0  0.00   1473
#&gt; 92   0  0.00   1406
#&gt; 93   0  0.00   1343
#&gt; 94   0  0.00   1282
#&gt; 95   0  0.00   1224
#&gt; 96   0  0.00   1169
#&gt; 97   0  0.00   1116
#&gt; 98   0  0.00   1065
#&gt; 99   0  0.00   1017
#&gt; 100  0  0.00    971
#&gt; 101  0  0.00    927
#&gt; 102  0  0.00    885
#&gt; 103  0  0.00    845
#&gt; 104  0  0.00    807
#&gt; 105  0  0.00    771
#&gt; 106  0  0.00    736
#&gt; 107  0  0.00    702
#&gt; 108  0  0.00    671
#&gt; 109  0  0.00    640
#&gt; 110  0  0.00    611
#&gt; 111  0  0.00    584
#&gt; 112  0  0.00    557
#&gt; 113  0  0.00    532
#&gt; 114  0  0.00    508
#&gt; 115  0  0.00    485
#&gt; 116  0  0.00    463
#&gt; 117  0  0.00    442
#&gt; 118  0  0.00    422
#&gt; 119  0  0.00    403
#&gt; 120  0  0.00    385
#&gt; 121  0  0.00    367
#&gt; 122  0  0.00    351
#&gt; 123  0  0.00    335
#&gt; 124  0  0.00    320
#&gt; 125  0  0.00    305
#&gt; 126  0  0.00    292
#&gt; 127  0  0.00    278
#&gt; 128  0  0.00    266
#&gt; 129  0  0.00    254
#&gt; 130  0  0.00    242
#&gt; 131  0  0.00    231
#&gt; 132  0  0.00    221
#&gt; 133  0  0.00    211
#&gt; 134  0  0.00    201
#&gt; 135  0  0.00    192
#&gt; 136  0  0.00    184
#&gt; 137  0  0.00    175
#&gt; 138  0  0.00    167
#&gt; 139  0  0.00    160
#&gt; 140  0  0.00    152
#&gt; 141  0  0.00    146
#&gt; 142  0  0.00    139
#&gt; 143  0  0.00    133
#&gt; 144  0  0.00    127
#&gt; 145  0  0.00    121
#&gt; 146  0  0.00    116
#&gt; 147  0  0.00    110
#&gt; 148  0  0.00    105
#&gt; 149  0  0.00    101
#&gt; 150  0  0.00     96
#&gt; 151  0  0.00     92
#&gt; 152  0  0.00     88
#&gt; 153  0  0.00     84
#&gt; 154  0  0.00     80
#&gt; 155  0  0.00     76
#&gt; 156  0  0.00     73
#&gt; 157  0  0.00     69
#&gt; 158  0  0.00     66
#&gt; 159  0  0.00     63
#&gt; 160  0  0.00     60
#&gt; 161  0  0.00     58
#&gt; 162  0  0.00     55
#&gt; 163  0  0.00     53
#&gt; 164  0  0.00     50
#&gt; 165  0  0.00     48
#&gt; 166  0  0.00     46
#&gt; 167  0  0.00     44
#&gt; 168  0  0.00     42
#&gt; 169  1  2.01     40
#&gt; 170  1  4.27     38
#&gt; 171  1  6.12     36
#&gt; 172  1  7.65     35
#&gt; 173  1  8.95     33
#&gt; 174  1 10.07     32
#&gt; 175  1 11.04     30
#&gt; 176  1 11.88     29
#&gt; 177  1 12.63     28
#&gt; 178  1 13.28     26
#&gt; 179  1 13.87     25
#&gt; 180  1 14.39     24
#&gt; 181  1 14.86     23
#&gt; 182  1 15.28     22
#&gt; 183  1 15.66     21
#&gt; 184  1 16.00     20
#&gt; 185  1 16.31     19
#&gt; 186  1 16.59     18
#&gt; 187  1 16.85     17
#&gt; 188  1 17.08     17
#&gt; 189  1 17.29     16
#&gt; 190  1 17.48     15
#&gt; 191  1 17.66     14
#&gt; 192  1 17.82     14
#&gt; 193  1 17.97     13
#&gt; 194  1 18.10     13
#&gt; 195  1 18.22     12
#&gt; 196  1 18.33     11
#&gt; 197  1 18.44     11
#&gt; 198  1 18.53     10
#&gt; 199  1 18.61     10
#&gt; 200  1 18.69      9
#&gt; 
#&gt; $`0.68`
#&gt; 
#&gt; Call:  glmnet::glmnet(x = mm, y = as.factor(p), family = "binomial",      weights = weights, lambda = 10^(seq(4, 0, length.out = 200)) *          sum(reg)/length(reg) * sum(p)/sum(weights), standardize = F,      penalty.factor = reg) 
#&gt; 
#&gt;     Df  %Dev Lambda
#&gt; 1    0  0.00  94800
#&gt; 2    0  0.00  90510
#&gt; 3    0  0.00  86420
#&gt; 4    0  0.00  82510
#&gt; 5    0  0.00  78780
#&gt; 6    0  0.00  75220
#&gt; 7    0  0.00  71820
#&gt; 8    0  0.00  68570
#&gt; 9    0  0.00  65470
#&gt; 10   0  0.00  62500
#&gt; 11   0  0.00  59680
#&gt; 12   0  0.00  56980
#&gt; 13   0  0.00  54400
#&gt; 14   0  0.00  51940
#&gt; 15   0  0.00  49590
#&gt; 16   0  0.00  47350
#&gt; 17   0  0.00  45210
#&gt; 18   0  0.00  43160
#&gt; 19   0  0.00  41210
#&gt; 20   0  0.00  39350
#&gt; 21   0  0.00  37570
#&gt; 22   0  0.00  35870
#&gt; 23   0  0.00  34250
#&gt; 24   0  0.00  32700
#&gt; 25   0  0.00  31220
#&gt; 26   0  0.00  29810
#&gt; 27   0  0.00  28460
#&gt; 28   0  0.00  27170
#&gt; 29   0  0.00  25940
#&gt; 30   0  0.00  24770
#&gt; 31   0  0.00  23650
#&gt; 32   0  0.00  22580
#&gt; 33   0  0.00  21560
#&gt; 34   0  0.00  20580
#&gt; 35   0  0.00  19650
#&gt; 36   0  0.00  18760
#&gt; 37   0  0.00  17910
#&gt; 38   0  0.00  17100
#&gt; 39   0  0.00  16330
#&gt; 40   0  0.00  15590
#&gt; 41   0  0.00  14890
#&gt; 42   0  0.00  14210
#&gt; 43   0  0.00  13570
#&gt; 44   0  0.00  12960
#&gt; 45   0  0.00  12370
#&gt; 46   0  0.00  11810
#&gt; 47   0  0.00  11280
#&gt; 48   0  0.00  10770
#&gt; 49   0  0.00  10280
#&gt; 50   0  0.00   9815
#&gt; 51   0  0.00   9371
#&gt; 52   0  0.00   8947
#&gt; 53   0  0.00   8543
#&gt; 54   0  0.00   8156
#&gt; 55   0  0.00   7787
#&gt; 56   0  0.00   7435
#&gt; 57   0  0.00   7099
#&gt; 58   0  0.00   6778
#&gt; 59   0  0.00   6471
#&gt; 60   0  0.00   6179
#&gt; 61   0  0.00   5899
#&gt; 62   0  0.00   5632
#&gt; 63   0  0.00   5378
#&gt; 64   0  0.00   5134
#&gt; 65   0  0.00   4902
#&gt; 66   0  0.00   4680
#&gt; 67   0  0.00   4469
#&gt; 68   0  0.00   4267
#&gt; 69   0  0.00   4074
#&gt; 70   0  0.00   3889
#&gt; 71   0  0.00   3713
#&gt; 72   0  0.00   3546
#&gt; 73   0  0.00   3385
#&gt; 74   0  0.00   3232
#&gt; 75   0  0.00   3086
#&gt; 76   0  0.00   2946
#&gt; 77   0  0.00   2813
#&gt; 78   0  0.00   2686
#&gt; 79   0  0.00   2564
#&gt; 80   0  0.00   2448
#&gt; 81   0  0.00   2338
#&gt; 82   0  0.00   2232
#&gt; 83   0  0.00   2131
#&gt; 84   0  0.00   2035
#&gt; 85   0  0.00   1943
#&gt; 86   0  0.00   1855
#&gt; 87   0  0.00   1771
#&gt; 88   0  0.00   1691
#&gt; 89   0  0.00   1614
#&gt; 90   0  0.00   1541
#&gt; 91   0  0.00   1472
#&gt; 92   0  0.00   1405
#&gt; 93   0  0.00   1341
#&gt; 94   0  0.00   1281
#&gt; 95   0  0.00   1223
#&gt; 96   0  0.00   1168
#&gt; 97   0  0.00   1115
#&gt; 98   0  0.00   1064
#&gt; 99   0  0.00   1016
#&gt; 100  0  0.00    970
#&gt; 101  0  0.00    926
#&gt; 102  0  0.00    884
#&gt; 103  0  0.00    844
#&gt; 104  0  0.00    806
#&gt; 105  0  0.00    770
#&gt; 106  0  0.00    735
#&gt; 107  0  0.00    702
#&gt; 108  0  0.00    670
#&gt; 109  0  0.00    640
#&gt; 110  0  0.00    611
#&gt; 111  0  0.00    583
#&gt; 112  0  0.00    557
#&gt; 113  0  0.00    532
#&gt; 114  0  0.00    508
#&gt; 115  0  0.00    485
#&gt; 116  0  0.00    463
#&gt; 117  0  0.00    442
#&gt; 118  0  0.00    422
#&gt; 119  0  0.00    403
#&gt; 120  0  0.00    384
#&gt; 121  0  0.00    367
#&gt; 122  0  0.00    350
#&gt; 123  0  0.00    335
#&gt; 124  0  0.00    320
#&gt; 125  0  0.00    305
#&gt; 126  0  0.00    291
#&gt; 127  0  0.00    278
#&gt; 128  0  0.00    266
#&gt; 129  0  0.00    254
#&gt; 130  0  0.00    242
#&gt; 131  0  0.00    231
#&gt; 132  0  0.00    221
#&gt; 133  0  0.00    211
#&gt; 134  0  0.00    201
#&gt; 135  0  0.00    192
#&gt; 136  0  0.00    183
#&gt; 137  0  0.00    175
#&gt; 138  0  0.00    167
#&gt; 139  0  0.00    160
#&gt; 140  0  0.00    152
#&gt; 141  0  0.00    146
#&gt; 142  0  0.00    139
#&gt; 143  0  0.00    133
#&gt; 144  0  0.00    127
#&gt; 145  0  0.00    121
#&gt; 146  0  0.00    115
#&gt; 147  0  0.00    110
#&gt; 148  0  0.00    105
#&gt; 149  0  0.00    100
#&gt; 150  0  0.00     96
#&gt; 151  0  0.00     92
#&gt; 152  0  0.00     87
#&gt; 153  0  0.00     83
#&gt; 154  0  0.00     80
#&gt; 155  0  0.00     76
#&gt; 156  0  0.00     73
#&gt; 157  0  0.00     69
#&gt; 158  0  0.00     66
#&gt; 159  0  0.00     63
#&gt; 160  0  0.00     60
#&gt; 161  0  0.00     58
#&gt; 162  0  0.00     55
#&gt; 163  0  0.00     53
#&gt; 164  0  0.00     50
#&gt; 165  0  0.00     48
#&gt; 166  0  0.00     46
#&gt; 167  0  0.00     44
#&gt; 168  0  0.00     42
#&gt; 169  1  2.01     40
#&gt; 170  1  4.27     38
#&gt; 171  2 13.73     36
#&gt; 172  2 20.08     35
#&gt; 173  2 24.51     33
#&gt; 174  2 27.82     32
#&gt; 175  2 30.44     30
#&gt; 176  2 32.60     29
#&gt; 177  2 34.44     27
#&gt; 178  2 36.05     26
#&gt; 179  2 37.46     25
#&gt; 180  2 38.74     24
#&gt; 181  2 39.89     23
#&gt; 182  2 40.95     22
#&gt; 183  2 41.92     21
#&gt; 184  2 42.83     20
#&gt; 185  2 43.67     19
#&gt; 186  2 44.46     18
#&gt; 187  2 45.20     17
#&gt; 188  2 45.90     17
#&gt; 189  2 46.56     16
#&gt; 190  2 47.18     15
#&gt; 191  2 47.77     14
#&gt; 192  2 48.33     14
#&gt; 193  2 48.86     13
#&gt; 194  2 49.36     13
#&gt; 195  2 49.84     12
#&gt; 196  2 50.29     11
#&gt; 197  2 50.73     11
#&gt; 198  2 51.14     10
#&gt; 199  2 51.53     10
#&gt; 200  2 51.90      9
#&gt; </div><div class='input'><span class='va'>esm_max_t1</span><span class='op'>$</span><span class='va'>predictors</span>
</div><div class='output co'>#&gt; <span style='color: #949494;'># A tibble: 1 x 8</span>
#&gt;   c1    c2    c3    c4      c5      c6    c7    c8   
#&gt;   <span style='color: #949494; font-style: italic;'>&lt;chr&gt;</span> <span style='color: #949494; font-style: italic;'>&lt;chr&gt;</span> <span style='color: #949494; font-style: italic;'>&lt;chr&gt;</span> <span style='color: #949494; font-style: italic;'>&lt;chr&gt;</span>   <span style='color: #949494; font-style: italic;'>&lt;chr&gt;</span>   <span style='color: #949494; font-style: italic;'>&lt;chr&gt;</span> <span style='color: #949494; font-style: italic;'>&lt;chr&gt;</span> <span style='color: #949494; font-style: italic;'>&lt;chr&gt;</span>
#&gt; <span style='color: #BCBCBC;'>1</span> aet   cwd   tmin  ppt_djf ppt_jja pH    awc   depth</div><div class='input'><span class='va'>esm_max_t1</span><span class='op'>$</span><span class='va'>performance</span>
</div><div class='output co'>#&gt; <span style='color: #949494;'># A tibble: 7 x 25</span>
#&gt;   model   threshold    thr_value n_presences n_absences TPR_mean TPR_sd TNR_mean
#&gt;   <span style='color: #949494; font-style: italic;'>&lt;chr&gt;</span>   <span style='color: #949494; font-style: italic;'>&lt;chr&gt;</span>            <span style='color: #949494; font-style: italic;'>&lt;dbl&gt;</span>       <span style='color: #949494; font-style: italic;'>&lt;int&gt;</span>      <span style='color: #949494; font-style: italic;'>&lt;int&gt;</span>    <span style='color: #949494; font-style: italic;'>&lt;dbl&gt;</span>  <span style='color: #949494; font-style: italic;'>&lt;dbl&gt;</span>    <span style='color: #949494; font-style: italic;'>&lt;dbl&gt;</span>
#&gt; <span style='color: #BCBCBC;'>1</span> esm_max equal_sens_~   0.397            50         50     0.82  0.284     0.98
#&gt; <span style='color: #BCBCBC;'>2</span> esm_max lpt           -<span style='color: #BB0000;'>0.001</span><span style='color: #BB0000; text-decoration: underline;'>26</span>          50         50    <span style='color: #BB0000;'>NA</span>    <span style='color: #BB0000;'>NA</span>        <span style='color: #BB0000;'>NA</span>   
#&gt; <span style='color: #BCBCBC;'>3</span> esm_max max_fpb        0.416            50         50     0.98  0.1       0.82
#&gt; <span style='color: #BCBCBC;'>4</span> esm_max max_jaccard    0.416            50         50     0.98  0.1       0.82
#&gt; <span style='color: #BCBCBC;'>5</span> esm_max max_sens_sp~   0.416            50         50     0.82  0.284     1   
#&gt; <span style='color: #BCBCBC;'>6</span> esm_max max_sorensen   0.416            50         50     0.98  0.1       0.82
#&gt; <span style='color: #BCBCBC;'>7</span> esm_max sensitivity    0.547            50         50     0.98  0.1       0.82
#&gt; <span style='color: #949494;'># ... with 17 more variables: TNR_sd &lt;dbl&gt;, SORENSEN_mean &lt;dbl&gt;,</span>
#&gt; <span style='color: #949494;'>#   SORENSEN_sd &lt;dbl&gt;, JACCARD_mean &lt;dbl&gt;, JACCARD_sd &lt;dbl&gt;, FPB_mean &lt;dbl&gt;,</span>
#&gt; <span style='color: #949494;'>#   FPB_sd &lt;dbl&gt;, OR_mean &lt;dbl&gt;, OR_sd &lt;dbl&gt;, TSS_mean &lt;dbl&gt;, TSS_sd &lt;dbl&gt;,</span>
#&gt; <span style='color: #949494;'>#   AUC_mean &lt;dbl&gt;, AUC_sd &lt;dbl&gt;, BOYCE_mean &lt;dbl&gt;, BOYCE_sd &lt;dbl&gt;,</span>
#&gt; <span style='color: #949494;'>#   IMAE_mean &lt;dbl&gt;, IMAE_sd &lt;dbl&gt;</span></div><div class='input'><span class='co'># }</span>
</div></pre>
  </div>
  <div class="col-md-3 hidden-xs hidden-sm" id="pkgdown-sidebar">
    <nav id="toc" data-toggle="toc" class="sticky-top">
      <h2 data-toc-skip>Contents</h2>
    </nav>
  </div>
</div>


      <footer>
      <div class="copyright">
  <p>Developed by Santiago J.E. Velazco, André F.A. Andrade, Brooke Rose, Ignacio Minoli, Janet Franklin.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="https://pkgdown.r-lib.org/">pkgdown</a> 1.6.1.</p>
</div>

      </footer>
   </div>

  


  </body>
</html>


